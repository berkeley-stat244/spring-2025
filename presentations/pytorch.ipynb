{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch as a GPU Computation Library\n",
    "\n",
    "## Introduction\n",
    "\n",
    "PyTorch is widely known as a leading deep learning framework.  However, it's far more than that!  PyTorch is also a powerful and versatile library for general-purpose, high-performance numerical computation.  In this notebook, we'll explore how to leverage PyTorch for GPU-accelerated computing *beyond* deep learning. We'll cover fundamental tensor operations, object-oriented structuring with `nn.Module`, and the significant performance boosts offered by `torch.compile` (PyTorch's JIT compiler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Ensure consistent results (optional, but good for demonstrations)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set default matmul precision (good practice for modern GPUs)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch as a Linear Algebra Library: Tensors and GPU Offloading\n",
    "\n",
    "Let's begin with the foundation: tensors and how to harness the power of the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tensor Creation Examples ---\n",
      "Tensor a: tensor([1, 2, 3])\n",
      "Tensor b:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# --- Tensor Creation ---\n",
    "\n",
    "# Create tensors from lists\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b:\\n{b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor c: tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with specific data types\n",
    "c = torch.tensor([1, 2, 3], dtype=torch.float64)\n",
    "print(f\"Tensor c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros Tensor:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Ones Tensor:\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Uniform Random Tensor:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "Standard Normal Random Tensor:\n",
      "tensor([[ 2.2082, -0.6380],\n",
      "        [ 0.4617,  0.2674]])\n"
     ]
    }
   ],
   "source": [
    "# Create tensors filled with zeros, ones, or random numbers\n",
    "zeros_tensor = torch.zeros(2, 3)  # 2x3 tensor of zeros\n",
    "ones_tensor = torch.ones(5)      # 1D tensor of ones (length 5)\n",
    "rand_uniform_tensor = torch.rand(3, 4)  # 3x4, uniform random [0, 1)\n",
    "rand_normal_tensor = torch.randn(2, 2) # 2x2, standard normal\n",
    "print(f\"Zeros Tensor:\\n{zeros_tensor}\")\n",
    "print(f\"Ones Tensor:\\n{ones_tensor}\")\n",
    "print(f\"Uniform Random Tensor:\\n{rand_uniform_tensor}\")\n",
    "print(f\"Standard Normal Random Tensor:\\n{rand_normal_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from NumPy Array:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor from a NumPy array\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(f\"Tensor from NumPy Array:\\n{tensor_from_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Acceleration: Moving Tensors to the GPU\n",
    "\n",
    "The real power of PyTorch lies in its seamless GPU integration.  We can move tensors to the GPU using the `.to()` method.  Let's check for GPU availability and demonstrate moving tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GPU Check and Tensor Movement ---\n",
      "CUDA is available! Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# --- GPU Check and Tensor Movement ---\n",
    "\n",
    "# Check for CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is initially on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor on the CPU, this is a better practice than creating on CPU and moving to GPU\n",
    "x = torch.randn(5, 5)\n",
    "print(f\"x is initially on device: {x.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is now on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Move the tensor to the GPU (if available)\n",
    "x = x.to(device)\n",
    "print(f\"x is now on device: {x.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (result of x * 2) is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Perform a simple operation (element-wise multiplication)\n",
    "y = x * 2\n",
    "print(f\"y (result of x * 2) is on device: {y.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is now on device: cpu\n",
      "y:\n",
      "tensor([[-1.5163,  2.1566,  1.6016,  3.3612,  0.7117],\n",
      "        [-1.3732, -0.9867,  0.4830, -0.4632,  1.3743],\n",
      "        [-2.1784, -0.7106, -1.8276, -1.3163,  0.1560],\n",
      "        [ 1.0516, -0.9760, -0.8689, -2.7727, -2.5724],\n",
      "        [-2.8065,  0.0720, -0.1270,  1.3512, -0.1956]])\n"
     ]
    }
   ],
   "source": [
    "# Move the result back to the CPU (for printing, etc.)\n",
    "y = y.to('cpu')\n",
    "print(f\"y is now on device: {y.device}\")\n",
    "print(f\"y:\\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU vs. GPU: Matrix Multiplication Benchmark\n",
    "\n",
    "To truly appreciate the speedup offered by GPUs, let's compare the execution time of a matrix multiplication on both the CPU and GPU. We'll use large matrices to make the difference dramatic.  **Crucially**, we use `torch.cuda.synchronize()` to ensure the GPU operation completes before we stop the timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Matrix Multiplication Benchmark ---\n"
     ]
    }
   ],
   "source": [
    "# --- Matrix Multiplication Benchmark ---\n",
    "\n",
    "size = 10000  # Large matrix size\n",
    "\n",
    "# Create tensors on CPU and GPU\n",
    "a_cpu = torch.randn(size, size)\n",
    "b_cpu = torch.randn(size, size)\n",
    "a_gpu = a_cpu.to(device)  # Move to GPU if available\n",
    "b_gpu = b_cpu.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 6.1749 seconds\n"
     ]
    }
   ],
   "source": [
    "# CPU timing\n",
    "start_time = time.perf_counter()\n",
    "c_cpu = torch.matmul(a_cpu, b_cpu)\n",
    "end_time = time.perf_counter()\n",
    "cpu_time = end_time - start_time\n",
    "print(f\"CPU time: {cpu_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU time: 0.1948 seconds\n"
     ]
    }
   ],
   "source": [
    "# GPU timing (with synchronization)\n",
    "start_time = time.perf_counter()\n",
    "c_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "torch.cuda.synchronize()  # Ensure GPU operation completes!\n",
    "end_time = time.perf_counter()\n",
    "gpu_time = end_time - start_time\n",
    "print(f\"GPU time: {gpu_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup: 31.70x\n"
     ]
    }
   ],
   "source": [
    "print(f\"Speedup: {cpu_time / gpu_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Useful Linear Algebra Operations\n",
    "\n",
    "PyTorch has a rich set of functions beyond matrix multiplication. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Other Linear Algebra Operations ---\n",
      "Element-wise addition: tensor([5, 7, 9], device='cuda:0')\n",
      "Element-wise subtraction: tensor([-3, -3, -3], device='cuda:0')\n",
      "Element-wise multiplication: tensor([ 4, 10, 18], device='cuda:0')\n",
      "Element-wise division: tensor([0.2500, 0.4000, 0.5000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# --- Other Linear Algebra Operations ---\n",
    "\n",
    "# Element-wise operations\n",
    "a = torch.tensor([1, 2, 3], device=device)\n",
    "b = torch.tensor([4, 5, 6], device=device)\n",
    "\n",
    "print(f\"Element-wise addition: {a + b}\")\n",
    "print(f\"Element-wise subtraction: {a - b}\")\n",
    "print(f\"Element-wise multiplication: {a * b}\")\n",
    "print(f\"Element-wise division: {a / b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of all elements: -2.418802261352539\n",
      "Mean of all elements: -0.20156686007976532\n",
      "Max element: 2.1613736152648926\n",
      "Min element: -1.9243990182876587\n"
     ]
    }
   ],
   "source": [
    "# Reductions\n",
    "x = torch.randn(3, 4, device=device)\n",
    "print(f\"Sum of all elements: {x.sum()}\")\n",
    "print(f\"Mean of all elements: {x.mean()}\")\n",
    "print(f\"Max element: {x.max()}\")\n",
    "print(f\"Min element: {x.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped tensor (view): torch.Size([12])\n",
      "Reshaped tensor (reshape): torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "y = x.view(12)  # Reshape to a 1D tensor\n",
    "print(f\"Reshaped tensor (view): {y.shape}\")\n",
    "z = x.reshape(2, 6)  # Reshape to a 2x6 tensor\n",
    "print(f\"Reshaped tensor (reshape): {z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of x: tensor([ 0.1940,  2.1614, -0.1721,  0.8491], device='cuda:0')\n",
      "Second column of x: tensor([ 2.1614,  0.6530, -1.2753], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Slicing and indexing (similar to NumPy)\n",
    "print(f\"First row of x: {x[0, :]}\")\n",
    "print(f\"Second column of x: {x[:, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch as an OOP Library: Introduction to `nn.Module`\n",
    "\n",
    "While we won't be building neural networks, PyTorch's `nn.Module` class is incredibly useful for structuring *any* computation that involves parameters (values you want to manage or update).  Think of `nn.Module` as a container for your operations and their associated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Custom `nn.Module`\n",
    "\n",
    "Let's create a simple module that performs a linear transformation (y = Ax + b). This will demonstrate the basic structure of an `nn.Module`.  This example performs a projection onto the column space of a matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Custom nn.Module: Projection onto Column Space ---\n"
     ]
    }
   ],
   "source": [
    "# --- Custom nn.Module: Projection onto Column Space ---\n",
    "\n",
    "class ProjectIntoColumnSpace(nn.Module):\n",
    "    def __init__(self, A):\n",
    "        super().__init__()\n",
    "        # Use nn.Parameter to register A as a parameter of the module.\n",
    "        self.A = nn.Buffer(A)\n",
    "        # Lazily initialize P\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not hasattr(self, 'P'):\n",
    "            # Compute the projection matrix\n",
    "            self.P = nn.Buffer(self.A @ torch.linalg.pinv(self.A))\n",
    "        # Project x onto the column space of A\n",
    "        return self.P @ x\n",
    "\n",
    "# Create a random matrix A\n",
    "A = torch.randn(10000, 1000)\n",
    "\n",
    "# Initialize the module and move it to the GPU\n",
    "model = ProjectIntoColumnSpace(A).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('Tensor A is on device:', model.A.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection time (GPU): 0.2253 seconds\n",
      "Output shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor\n",
    "x = torch.randn(10000, device=device)\n",
    "\n",
    "# Perform the projection and time it\n",
    "start_time = time.perf_counter()\n",
    "y = model(x)\n",
    "torch.cuda.synchronize() # synchronize before and after operation.\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Projection time (GPU): {end_time - start_time:.4f} seconds\")\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection time (GPU): 0.0015 seconds\n",
      "Output shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "y = model(x)\n",
    "torch.cuda.synchronize() # synchronize before and after operation.\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Projection time (GPU): {end_time - start_time:.4f} seconds\")\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection time (CPU): 0.0101 seconds\n",
      "Output shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# Perform projection on CPU for comparison\n",
    "model_cpu = model.to('cpu')  # Move the model to the CPU\n",
    "x_cpu = x.to('cpu')        # Move the input to the CPU\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "y_cpu = model_cpu(x_cpu)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"Projection time (CPU): {end_time - start_time:.4f} seconds\")\n",
    "print(f\"Output shape: {y_cpu.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Benefits of `nn.Module`:**\n",
    "\n",
    "*   **Organization:** Keeps parameters and computation logic together.\n",
    "*   **Parameter Management:** Easy access to all parameters (e.g., `model.parameters()`).\n",
    "*   **Device Management:** Moving the module to the GPU (e.g., `.to(device)`) automatically moves all its parameters and buffers.\n",
    "* **Buffers**: Buffers are like parameters but are not optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch as a JIT Language: Leveraging `torch.compile`\n",
    "\n",
    "While GPU acceleration provides a significant boost, PyTorch's default \"eager\" execution mode can have overhead. Each operation is executed individually.  `torch.compile` addresses this by acting as a Just-In-Time (JIT) compiler, optimizing your code for even greater performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Python Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure Python Pi Estimate: 3.142174, Time: 5.2009 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Pure Python Implementation ---\n",
    "\n",
    "def monte_carlo_pi_python(n_samples):\n",
    "    inside_circle = 0\n",
    "    for _ in range(n_samples):\n",
    "        x = random.uniform(-1, 1)  # Random x coordinate\n",
    "        y = random.uniform(-1, 1)  # Random y coordinate\n",
    "        if x**2 + y**2 <= 1:     # Check if inside the circle\n",
    "            inside_circle += 1\n",
    "    return 4 * inside_circle / n_samples\n",
    "\n",
    "n_samples = 10000000\n",
    "\n",
    "# expect this to be slow\n",
    "start_time = time.perf_counter()\n",
    "pi_python = monte_carlo_pi_python(n_samples)\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Pure Python Pi Estimate: {pi_python:.6f}, Time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch (Uncompiled) Pi Estimate: 3.143118, Time: 0.0450 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- PyTorch Implementation (Vectorized) ---\n",
    "\n",
    "def monte_carlo_pi_pytorch(n_samples, device):\n",
    "    x = torch.rand(n_samples, device=device) * 2 - 1  # Range [-1, 1]\n",
    "    y = torch.rand(n_samples, device=device) * 2 - 1\n",
    "    inside_circle = (x**2 + y**2 <= 1).sum()  # Count points inside\n",
    "    return 4 * inside_circle.float() / n_samples # Return to float\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "pi_pytorch = monte_carlo_pi_pytorch(n_samples, device)\n",
    "torch.cuda.synchronize() # This is important for the CPU as well.\n",
    "end_time = time.perf_counter()\n",
    "print(f\"PyTorch (Uncompiled) Pi Estimate: {pi_pytorch:.6f}, Time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing `torch.compile`\n",
    "\n",
    "Vectorization reduces the overhead of Python, but you can go even further with `torch.compile`. Let's see how it works and the performance benefits it offers.\n",
    "\n",
    "`torch.compile` analyzes your PyTorch code and generates optimized code, often using the Triton kernel language for GPUs.  This can lead to:\n",
    "\n",
    "*   **Operator Fusion:** Combining multiple operations into a single kernel.\n",
    "*   **Kernel Specialization:** Generating code tailored to specific tensor shapes and data types.\n",
    "*   **Reduced Overhead:** Minimizing the communication between the CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic `torch.compile` Usage\n",
    "\n",
    "The simplest way to use `torch.compile` is to pass your function to `torch.compile`. Let's see how this works with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled Time: 2.1428 seconds\n"
     ]
    }
   ],
   "source": [
    "# PyTorch (Compiled) - Some compilation is done when the function is passed to torch.compile\n",
    "start_time = time.perf_counter()\n",
    "monte_carlo_pi_pytorch_compiled = torch.compile(monte_carlo_pi_pytorch)\n",
    "print(f'Compiled Time: {time.perf_counter() - start_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for warm-up run: 3.141584, Time: 3.0536 seconds\n"
     ]
    }
   ],
   "source": [
    "# PyTorch (Compiled) - The first run will also do some compilation\n",
    "start_time = time.perf_counter()\n",
    "pi_compiled = monte_carlo_pi_pytorch_compiled(n_samples, device)\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Time for warm-up run: {pi_compiled:.6f}, Time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch (Compiled) Pi Estimate: 3.141207, Time: 0.0022 seconds\n"
     ]
    }
   ],
   "source": [
    "# PyTorch (Compiled) - Subsequent runs will be faster\n",
    "start_time = time.perf_counter()\n",
    "pi_compiled = monte_carlo_pi_pytorch_compiled(n_samples, device)\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"PyTorch (Compiled) Pi Estimate: {pi_compiled:.6f}, Time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.compile` provides significant speedups in this Monte Carlo simulation primarily by **eliminating Python interpreter overhead and fusing operations**.  The pure Python version is slow because each iteration of the loop involves many individual Python operations. The uncompiled PyTorch version is faster due to vectorized operations, but *still* launches separate kernels for random number generation, squaring, addition, comparison, and summation. `torch.compile`, however, analyzes the entire function and generates a *single, optimized kernel* that performs all these steps in one go, drastically reducing the communication between CPU and GPU and minimizing kernel launch overhead. This is a classic example of **operator fusion**, a key optimization technique employed by `torch.compile`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, `torch.compile` is not a silver bullet. If the overhead of the Python interpreter is negligible compared to the computation, you may not see a significant speedup, and the compilation cost may outweigh the benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled function time: 0.0034 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- When compiling doesn't help ---\n",
    "\n",
    "def element_wise_mult(A, B):\n",
    "  return A * B\n",
    "\n",
    "A = torch.randn(10000, 10000, device=device)\n",
    "B = torch.randn(10000, 10000, device=device)\n",
    "\n",
    "# Uncompiled version\n",
    "start_time = time.perf_counter()\n",
    "result_uncompiled = element_wise_mult(A, B)\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Uncompiled function time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for warm-up run: 0.0514 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compiled version\n",
    "compiled_element_wise_mult = torch.compile(element_wise_mult)\n",
    "\n",
    "# Warm-up (important for JIT compilers)\n",
    "start_time = time.perf_counter()\n",
    "compiled_element_wise_mult(A, B)\n",
    "torch.cuda.synchronize()\n",
    "print(f'Time for warm-up run: {time.perf_counter() - start_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled function time: 0.0020 seconds\n"
     ]
    }
   ],
   "source": [
    "# After warm-up\n",
    "start_time = time.perf_counter()\n",
    "result_compiled = compiled_element_wise_mult(A, B)\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Compiled function time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompilation and Data Types\n",
    "\n",
    "Like in Julia, `torch.compile` recompiles functions when the input types change. This can introduce overhead, so it's important to be aware of when recompilation occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (int64) time: 0.0484 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Recompilation Examples ---\n",
    "\n",
    "# 1. Recompilation due to Data Type Change\n",
    "x_int = torch.ones(10000, 10000, device=device, dtype=torch.int64)\n",
    "y_int = torch.ones(10000, 10000, device=device, dtype=torch.int64)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "compiled_element_wise_mult(x_int, y_int)  # Compile for int64\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"First call (int64) time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second call (int64) time: 0.0040 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "compiled_element_wise_mult(x_int, y_int)  # Reuse compiled int64 version\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Second call (int64) time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call with CPU tensors time: 3.9983 seconds\n"
     ]
    }
   ],
   "source": [
    "# 2. Recompilation due to Device Change\n",
    "x_cpu = x_int.cpu()\n",
    "y_cpu = y_int.cpu()\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "compiled_element_wise_mult(x_cpu, y_cpu)  # Recompile for CPU\n",
    "# No need for torch.cuda.synchronize() on CPU\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Call with CPU tensors time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompilation and Dynamic Shapes\n",
    "\n",
    "Unlike in Julia, by default, `torch.compile` tries to be smart about input tensor sizes. The *first* time it compiles a function, it generates a *specialized* kernel that's optimized for the *specific* input sizes it encountered. However, if it sees inputs of *different* sizes later, it will attempt to recompile with a more *dynamic* kernel that can handle a range of sizes, avoiding further recompilations (within limits). This behavior can be controlled using the `dynamic` argument to `torch.compile` (though we won't dive into that here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (different size) time: 0.2496 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Recompilation and Dynamic Shapes ---\n",
    "\n",
    "# We'll use compiled_element_wise_mult from before:\n",
    "# @torch.compile\n",
    "# def compiled_element_wise_mult(A, B):\n",
    "#     return A * B\n",
    "# We have already compiled this function with tensors of size 10000x10000.\n",
    "\n",
    "# Different Size (Triggers Recompilation with Dynamic Kernel)\n",
    "x_different = torch.ones(2000, 2000, device=device, dtype=torch.int64)\n",
    "y_different = torch.ones(2000, 2000, device=device, dtype=torch.int64)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "compiled_element_wise_mult(x_different, y_different)  # Will try to recompilation for dynamic shapes\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"First call (different size) time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (smaller size) time: 0.0005 seconds\n"
     ]
    }
   ],
   "source": [
    "# Smaller Size (Uses Dynamic Kernel - No Recompilation)\n",
    "x_smaller = torch.ones(500, 500, device=device, dtype=torch.int64)\n",
    "y_smaller = torch.ones(500, 500, device=device, dtype=torch.int64)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "compiled_element_wise_mult(x_smaller, y_smaller)  # Uses dynamic kernel\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"First call (smaller size) time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (larger size) time: 0.0234 seconds\n"
     ]
    }
   ],
   "source": [
    "# Larger Size (Uses Dynamic Kernel - No Recompilation)\n",
    "\n",
    "x_larger = torch.ones(20000, 20000, device=device, dtype=torch.int64)\n",
    "y_larger = torch.ones(20000, 20000, device=device, dtype=torch.int64)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "compiled_element_wise_mult(x_larger, y_larger) # Uses dynamic kernel\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"First call (larger size) time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "*   `torch.compile` aims to balance specialization (for best performance) and dynamism (to avoid excessive recompilation).\n",
    "*   The `dynamic=True/False/None` argument to `torch.compile` gives you more control over this behavior, but the default (None) usually works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling Different `if-else` Branches\n",
    "\n",
    "Python is not a native JIT language, so `torch.compile` can't handle arbitrary Python control flow. However, it can handle `if-else` statements where the condition depends on *static* values (like function arguments), *not* on the tensor data itself.  It compiles a separate version for each branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (mode=0, first call): 0.0991 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Compiling Different if-else Branches ---\n",
    "\n",
    "# Another approach to using torch.compile is as a decorator\n",
    "@torch.compile\n",
    "def if_func(x, y, mode):\n",
    "    if mode == 0:\n",
    "        return x + y\n",
    "    else:\n",
    "        return x - y\n",
    "\n",
    "x = torch.randn(1000, 1000, device=device)\n",
    "y = torch.randn(1000, 1000, device=device)\n",
    "\n",
    "# Call with mode=0 (compiles the addition branch)\n",
    "start_time = time.perf_counter()\n",
    "result_0 = if_func(x, y, 0)\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Time (mode=0, first call): {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (mode=0, second call): 0.0013 seconds\n"
     ]
    }
   ],
   "source": [
    "# Call again with mode=0 (reuses the compiled addition branch)\n",
    "start_time = time.perf_counter()\n",
    "result_0_again = if_func(x, y, 0)\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Time (mode=0, second call): {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (mode=1, first call): 0.1276 seconds\n"
     ]
    }
   ],
   "source": [
    "# Call with mode=1 (compiles the subtraction branch)\n",
    "start_time = time.perf_counter()\n",
    "result_1 = if_func(x, y, 1)\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Time (mode=1, first call): {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (mode=1, second call): 0.0011 seconds\n"
     ]
    }
   ],
   "source": [
    "# Call again with mode=1 (reuses the compiled subtraction branch)\n",
    "start_time = time.perf_counter()\n",
    "result_1_again = if_func(x, y, 1)\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Time (mode=1, second call): {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompilation Summary\n",
    "\n",
    "`torch.compile` is a powerful tool for optimizing PyTorch code, but it's somewhat a \"hack\" on top of Python, so it has very complex behavior on recompilation. Some rules of thumb are:\n",
    "\n",
    "* Recompilation is triggered when the input types or devices change. (Same as in Julia)\n",
    "* A \"static\" kernel for a specific input size is generated by default, but a dynamic kernel that can handle a range of sizes is generated it sees inputs of different sizes later. (Different from Julia)\n",
    "* Each branch of an `if-else` statement is compiled separately. (Different from Julia)\n",
    "* `torch.compile` is designed to be user-friendly, but it has many complex behaviors that support this goal. See best practices at https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Breaks\n",
    "\n",
    "`torch.compile` aims to capture the whole function in a single computation graph for full optimization. However, it can't always do this. When it encounters code that can't be traced, a \"graph break\" occurs, and `torch.compile` compiles codes before and after the break in separate graphs. This prevents optimization through the entire function and should be avoided when possible.\n",
    "\n",
    "Graph breaks occur on things like:\n",
    "\n",
    "* Data-dependent if-statements\n",
    "* Many Python built-in functions\n",
    "* C functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UserError",
     "evalue": "Dynamic control flow is not supported at the moment. Please use functorch.experimental.control_flow.cond to explicitly capture the control flow. For more information about this error, see: https://pytorch.org/docs/main/generated/exportdb/index.html#cond-operands\n\nfrom user code:\n   File \"/tmp/ipykernel_3217622/3778475794.py\", line 5, in data_dependent_branch\n    if x.sum() > 0:\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUserError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m10000\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m10000\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 13\u001b[0m result_pos \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_data_dependent_branch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    470\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:1269\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1264\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1265\u001b[0m             )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:526\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    510\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    512\u001b[0m signpost_event(\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m     },\n\u001b[1;32m    524\u001b[0m )\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:924\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    922\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 924\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:666\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_compile.compile_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentire_frame_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord():\n\u001b[0;32m--> 666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_utils_internal.py:87\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m     90\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:699\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    697\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py:1322\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1319\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1320\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1322\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:219\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m    216\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39m_maybe_revert_all_patches()\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:634\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 634\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    636\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:2796\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2796\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:560\u001b[0m, in \u001b[0;36mgeneric_jump.<locals>.inner\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjump(inst)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# TODO link the torch.cond doc later\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUserError(\n\u001b[1;32m    561\u001b[0m         exc\u001b[38;5;241m.\u001b[39mUserErrorType\u001b[38;5;241m.\u001b[39mDYNAMIC_CONTROL_FLOW,\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDynamic control flow is not supported at the moment. Please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctorch.experimental.control_flow.cond to explicitly capture the control flow.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    564\u001b[0m         case_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcond_operands\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    565\u001b[0m     )\n",
      "\u001b[0;31mUserError\u001b[0m: Dynamic control flow is not supported at the moment. Please use functorch.experimental.control_flow.cond to explicitly capture the control flow. For more information about this error, see: https://pytorch.org/docs/main/generated/exportdb/index.html#cond-operands\n\nfrom user code:\n   File \"/tmp/ipykernel_3217622/3778475794.py\", line 5, in data_dependent_branch\n    if x.sum() > 0:\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "# --- Graph Break Example ---\n",
    "\n",
    "# Data-dependent branching is not supported\n",
    "def data_dependent_branch(x, y):\n",
    "    if x.sum() > 0:\n",
    "        x = x + y\n",
    "    return x\n",
    "\n",
    "compiled_data_dependent_branch = torch.compile(data_dependent_branch, fullgraph=True) # Force full graph compilation\n",
    "\n",
    "x = torch.ones(10000, device=device)\n",
    "y = torch.ones(10000, device=device)\n",
    "\n",
    "# Compilation will fail here\n",
    "result_pos = compiled_data_dependent_branch(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (first call): 0.0163 seconds\n"
     ]
    }
   ],
   "source": [
    "# We can compile this function with fullgraph=False, but it will lose the maximum performance benefit\n",
    "\n",
    "compiled_data_dependent_branch = torch.compile(data_dependent_branch, fullgraph=False) # Allow partial graph compilation\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "result_pos = compiled_data_dependent_branch(x, y)\n",
    "torch.cuda.synchronize()\n",
    "print(f\"Time (first call): {time.perf_counter() - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.3889 seconds\n"
     ]
    }
   ],
   "source": [
    "# not supported Python built-in functions\n",
    "\n",
    "# time.time() is not supported\n",
    "@torch.compile\n",
    "def compiled_unsupported_func(x):\n",
    "    x = x * 2\n",
    "    return x + time.time()\n",
    "\n",
    "# Will raise a warning, but still usable\n",
    "start_time = time.perf_counter()\n",
    "result = compiled_unsupported_func(x)\n",
    "torch.cuda.synchronize()\n",
    "print(f\"Time: {time.perf_counter() - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Compiled Code\n",
    "\n",
    "You can inspect the compiled code using the `torch._dynamo.explain`. This is helpful for understanding how `torch.compile` is transforming your code, but it can be quite verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Count: 2\n",
      "Graph Break Count: 1\n",
      "Op Count: 2\n",
      "Break Reasons:\n",
      "  Break Reason 1:\n",
      "    Reason: generic_jump TensorVariable()\n",
      "    User Stack:\n",
      "      <FrameSummary file /tmp/ipykernel_3217622/3778475794.py, line 5 in data_dependent_branch>\n",
      "Ops per Graph:\n",
      "  Ops 1:\n",
      "    <built-in function gt>\n",
      "  Ops 2:\n",
      "    <built-in function add>\n",
      "Out Guards:\n",
      "  Guard 1:\n",
      "    Name: \"L['x']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7fb8f407d990; to 'Tensor' at 0x7fb8f404e030>\n",
      "    Guarded Class Weakref: <weakref at 0x7fba2f7b6c00; to 'torch._C._TensorMeta' at 0x55b9c185cd10 (Tensor)>\n",
      "  Guard 2:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 3:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 4:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 5:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 6:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 7:\n",
      "    Name: \"L['y']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['y'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7fb89dc3e9d0; to 'Tensor' at 0x7fb8f419bed0>\n",
      "    Guarded Class Weakref: <weakref at 0x7fba2f7b6c00; to 'torch._C._TensorMeta' at 0x55b9c185cd10 (Tensor)>\n",
      "  Guard 8:\n",
      "    Name: \"L['x']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7fb8f407d990; to 'Tensor' at 0x7fb8f404e030>\n",
      "    Guarded Class Weakref: <weakref at 0x7fba2f7b6c00; to 'torch._C._TensorMeta' at 0x55b9c185cd10 (Tensor)>\n",
      "  Guard 9:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 10:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 11:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 12:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 13:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "Compile Times: TorchDynamo compilation metrics:\n",
      "Function, Runtimes (s)\n",
      "_compile.compile_inner, 0.0208, 0.0062\n",
      "OutputGraph.call_user_compiler, 0.0001, 0.0001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Inspecting the Compiled Graph ---\n",
    "\n",
    "'''\n",
    "def data_dependent_branch(x, y):\n",
    "    if x.sum() > 0:\n",
    "        x = x + y\n",
    "    return x\n",
    "'''\n",
    "\n",
    "# Print the compiled graph\n",
    "print(torch._dynamo.explain(data_dependent_branch)(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Count: 2\n",
      "Graph Break Count: 1\n",
      "Op Count: 2\n",
      "Break Reasons:\n",
      "  Break Reason 1:\n",
      "    Reason: Graph break due to unsupported builtin time.time. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
      "    User Stack:\n",
      "      <FrameSummary file /tmp/ipykernel_3217622/3545385762.py, line 7 in compiled_unsupported_func>\n",
      "Ops per Graph:\n",
      "  Ops 1:\n",
      "    <built-in function mul>\n",
      "  Ops 2:\n",
      "    <built-in function add>\n",
      "Out Guards:\n",
      "  Guard 1:\n",
      "    Name: \"G['time'].time\"\n",
      "    Source: global\n",
      "    Create Function: FUNCTION_MATCH\n",
      "    Guard Types: ['ID_MATCH']\n",
      "    Code List: [\"___check_obj_id(G['time'].time, 140440482436000)\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: <weakref at 0x7fbad90684a0; to 'type' at 0x55b9bbc23840 (builtin_function_or_method)>\n",
      "  Guard 2:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 3:\n",
      "    Name: \"L['x']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7fb8f407d990; to 'Tensor' at 0x7fb8f404e030>\n",
      "    Guarded Class Weakref: <weakref at 0x7fba2f7b6c00; to 'torch._C._TensorMeta' at 0x55b9c185cd10 (Tensor)>\n",
      "  Guard 4:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 5:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 6:\n",
      "    Name: \"G['time']\"\n",
      "    Source: global\n",
      "    Create Function: FUNCTION_MATCH\n",
      "    Guard Types: ['ID_MATCH']\n",
      "    Code List: [\"___check_obj_id(G['time'], 140440482435760)\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: <weakref at 0x7fbad909b010; to 'type' at 0x55b9bbc23300 (module)>\n",
      "  Guard 7:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 8:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 9:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 10:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 11:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 12:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 13:\n",
      "    Name: \"L['___stack0']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['___stack0'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7fb8f2009580; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7fba2f7b6c00; to 'torch._C._TensorMeta' at 0x55b9c185cd10 (Tensor)>\n",
      "  Guard 14:\n",
      "    Name: \"L['___stack1']\"\n",
      "    Source: local\n",
      "    Create Function: CONSTANT_MATCH\n",
      "    Guard Types: ['EQUALS_MATCH']\n",
      "    Code List: [\"L['___stack1'] == 1740125644.693142\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: <weakref at 0x7fbad907e390; to 'type' at 0x55b9bbc299e0 (float)>\n",
      "  Guard 15:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "Compile Times: TorchDynamo compilation metrics:\n",
      "Function, Runtimes (s)\n",
      "_compile.compile_inner, 0.0330, 0.0057\n",
      "OutputGraph.call_user_compiler, 0.0001, 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/zhangyunzhe2023/conda/envs/NODE/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:725: UserWarning: Graph break due to unsupported builtin time.time. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
      "  torch._dynamo.utils.warn_once(msg)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def compiled_unsupported_func(x):\n",
    "    x = x * 2\n",
    "    return x + time.time()\n",
    "'''\n",
    "\n",
    "# Print the compiled graph\n",
    "print(torch._dynamo.explain(compiled_unsupported_func)(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NODE",
   "language": "python",
   "name": "node"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
