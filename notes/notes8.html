<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chris Paciorek">
<meta name="dcterms.date" content="2025-02-13">

<title>Notes 8: GPUs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-e3c2a318547996e22cb8eadf618fd4f6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../assets/styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/notes1.html">Course Notes</a></li><li class="breadcrumb-item"><a href="../notes/notes8.html">Notes 8: GPUs</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../assets/stat_bear.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Stat 244</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/berkeley-stat244/spring-2025" title="" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../syllabus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Syllabus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../schedule.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Schedule</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../office_hours.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Office Hours</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Course Notes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/notes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes 1: Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/notes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes 2: Memory and scope</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/notes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes 3: Types and dispatch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/notes4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes 4: Managing Julia</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/notes5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes 5: Efficiency</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/notes6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes 6: JIT compilation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/notes7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes 7: Parallelization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/notes8.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Notes 8: GPUs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/notes9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes 9: Numerics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/notes10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes 10: Other topics</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../presentations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Presentations</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Problem Sets</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ps/ps1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Problem Set 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ps/ps2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Problem Set 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ps/ps3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Problem Set 3</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">How tos</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../howtos/accessJulia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Accessing Julia</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../howtos/accessSCF.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Accessing the SCF</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../howtos/useQuarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Quarto</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../howtos/installPython.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Installing Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../howtos/installGit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Installing Git</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../howtos/presentations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Presentation guidelines</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../license.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">License</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#gpu-overview" id="toc-gpu-overview" class="nav-link" data-scroll-target="#gpu-overview">GPU overview</a>
  <ul class="collapse">
  <li><a href="#accessing-a-gpu-on-the-scf" id="toc-accessing-a-gpu-on-the-scf" class="nav-link" data-scroll-target="#accessing-a-gpu-on-the-scf">Accessing a GPU on the SCF</a></li>
  <li><a href="#gpu-availability" id="toc-gpu-availability" class="nav-link" data-scroll-target="#gpu-availability">GPU availability</a></li>
  </ul></li>
  <li><a href="#basic-gpu-offloading" id="toc-basic-gpu-offloading" class="nav-link" data-scroll-target="#basic-gpu-offloading">Basic GPU offloading</a>
  <ul class="collapse">
  <li><a href="#matrix-multiplication-example" id="toc-matrix-multiplication-example" class="nav-link" data-scroll-target="#matrix-multiplication-example">Matrix multiplication example</a></li>
  <li><a href="#multiple-dispatch-with-gpu-operations" id="toc-multiple-dispatch-with-gpu-operations" class="nav-link" data-scroll-target="#multiple-dispatch-with-gpu-operations">Multiple dispatch with GPU operations</a></li>
  <li><a href="#copying-data-tofrom-the-gpu" id="toc-copying-data-tofrom-the-gpu" class="nav-link" data-scroll-target="#copying-data-tofrom-the-gpu">Copying data to/from the GPU</a></li>
  <li><a href="#vs-32-bit-numbers" id="toc-vs-32-bit-numbers" class="nav-link" data-scroll-target="#vs-32-bit-numbers">64- vs 32-bit numbers</a></li>
  <li><a href="#interacting-with-nvidias-cuda-libraries" id="toc-interacting-with-nvidias-cuda-libraries" class="nav-link" data-scroll-target="#interacting-with-nvidias-cuda-libraries">Interacting with NVIDIA’s CUDA libraries</a></li>
  <li><a href="#vectorized-example" id="toc-vectorized-example" class="nav-link" data-scroll-target="#vectorized-example">Vectorized example</a></li>
  </ul></li>
  <li><a href="#gpu-architecture" id="toc-gpu-architecture" class="nav-link" data-scroll-target="#gpu-architecture">GPU architecture</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#glossary" id="toc-glossary" class="nav-link" data-scroll-target="#glossary">Glossary</a></li>
  <li><a href="#threads-blocks-and-grids" id="toc-threads-blocks-and-grids" class="nav-link" data-scroll-target="#threads-blocks-and-grids">Threads, blocks and grids</a></li>
  <li><a href="#gpu-execution-and-efficiency" id="toc-gpu-execution-and-efficiency" class="nav-link" data-scroll-target="#gpu-execution-and-efficiency">GPU execution and efficiency</a></li>
  </ul></li>
  <li><a href="#gpu-monitoring---details" id="toc-gpu-monitoring---details" class="nav-link" data-scroll-target="#gpu-monitoring---details">GPU monitoring - details</a>
  <ul class="collapse">
  <li><a href="#using-nvidia-smi-for-hardware-and-activity" id="toc-using-nvidia-smi-for-hardware-and-activity" class="nav-link" data-scroll-target="#using-nvidia-smi-for-hardware-and-activity">Using <code>nvidia-smi</code> for hardware and activity</a></li>
  <li><a href="#devicequery-for-hardware-information" id="toc-devicequery-for-hardware-information" class="nav-link" data-scroll-target="#devicequery-for-hardware-information"><code>deviceQuery</code> for hardware information</a></li>
  </ul></li>
  <li><a href="#gpu-kernels" id="toc-gpu-kernels" class="nav-link" data-scroll-target="#gpu-kernels">GPU kernels</a>
  <ul class="collapse">
  <li><a href="#basic-example" id="toc-basic-example" class="nav-link" data-scroll-target="#basic-example">Basic example</a></li>
  <li><a href="#multiple-blocks" id="toc-multiple-blocks" class="nav-link" data-scroll-target="#multiple-blocks">Multiple blocks</a></li>
  <li><a href="#larger-computations" id="toc-larger-computations" class="nav-link" data-scroll-target="#larger-computations">Larger computations</a></li>
  </ul></li>
  <li><a href="#efficient-memory-access" id="toc-efficient-memory-access" class="nav-link" data-scroll-target="#efficient-memory-access">Efficient memory access</a>
  <ul class="collapse">
  <li><a href="#coalesced-access-to-global-memory" id="toc-coalesced-access-to-global-memory" class="nav-link" data-scroll-target="#coalesced-access-to-global-memory">Coalesced access to global memory</a></li>
  <li><a href="#using-shared-memory" id="toc-using-shared-memory" class="nav-link" data-scroll-target="#using-shared-memory">Using shared memory</a></li>
  </ul></li>
  <li><a href="#using-atomics-for-reduction-operations" id="toc-using-atomics-for-reduction-operations" class="nav-link" data-scroll-target="#using-atomics-for-reduction-operations">Using atomics for reduction operations</a>
  <ul class="collapse">
  <li><a href="#using-shared-memory-to-reduce-the-cost-of-atomic-operations" id="toc-using-shared-memory-to-reduce-the-cost-of-atomic-operations" class="nav-link" data-scroll-target="#using-shared-memory-to-reduce-the-cost-of-atomic-operations">Using shared memory to reduce the cost of atomic operations</a></li>
  <li><a href="#debugging-kernel-code" id="toc-debugging-kernel-code" class="nav-link" data-scroll-target="#debugging-kernel-code">Debugging kernel code</a></li>
  </ul></li>
  <li><a href="#final-comments---when-to-use-the-gpu" id="toc-final-comments---when-to-use-the-gpu" class="nav-link" data-scroll-target="#final-comments---when-to-use-the-gpu">Final comments - when to use the GPU</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="notes8.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/notes1.html">Course Notes</a></li><li class="breadcrumb-item"><a href="../notes/notes8.html">Notes 8: GPUs</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Notes 8: GPUs</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chris Paciorek </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 13, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This document is the eighth of a set of notes, this document focusing on the parallelization via GPUs. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.</p>
<p>Given that, the document heavily relies on demos, with interpretation in some cases left to the reader.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="GPU code run separately from rendering">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
GPU code run separately from rendering
</div>
</div>
<div class="callout-body-container callout-body">
<p>In most cases, I have set the code chunks to not execute when rendering this document, since that would require rendering to HTML/PDF on a machine with a GPU. In some cases I’ve copied in output from running the chunk manually outside of the rendering process, sometimes as part of code comments.</p>
</div>
</div>
</section>
<section id="gpu-overview" class="level2">
<h2 class="anchored" data-anchor-id="gpu-overview">GPU overview</h2>
<p>(This is repeated from the last set of notes.)</p>
<p>Some features of GPUs:</p>
<ul>
<li>Many processing units (thousands) that are slow individually compared to the CPU but provide massive parallelism.</li>
<li>They have somewhat more limited memory (though modern GPUs like the A100 can have 80 GB of GPU memory).</li>
<li>They can only use data in their own memory, not in the CPU’s memory, so one must transfer data back and forth between the CPU (the <em>host</em>) and the GPU (the <em>device</em>). This copying can, in some computations, constitute a very large fraction of the overall computation. So it is best to create the data and/or leave the data (for subsequent calculations) on the GPU when possible and to limit transfers.</li>
<li>For large-scale work, a GPU server will have multiple GPUs, e.g., 8 A100s or 8 H100s. We won’t discuss running code on multiple GPUs nor on GPUs distributed across multiple machines.</li>
<li>We’ll focus on NVIDIA GPUs and NVIDIA’s CUDA platform for running code on the GPU, but other manufacturers produce other kinds of GPUs accessed via different interfaces.</li>
</ul>
<section id="accessing-a-gpu-on-the-scf" class="level3">
<h3 class="anchored" data-anchor-id="accessing-a-gpu-on-the-scf">Accessing a GPU on the SCF</h3>
<p>We have one (somewhat old) GPU on the <code>gpu</code> partition that is available for all to use equally.</p>
<p>After logging into an SCF standalone/login machine such as <code>arwen</code>, <code>gandalf</code>, or <code>radagast</code>, you can ask for access to a GPU through the SLURM scheduler.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">srun</span> <span class="at">-p</span> gpu <span class="at">--gpus</span><span class="op">=</span>1 <span class="at">--pty</span> bash</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">nvidia-smi</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>A large variety of other, more powerful GPUs, are available through “partitions” that were purchased by faculty research grants. You’re welcome to use them, but your job could be preempted (killed) at any moment by higher-priority jobs.</p>
<p>The partitions with GPUs include <code>jsteinhardt</code>, <code>yss</code>, and <code>yugroup</code>. Most of the GPUs are in <code>jsteinhardt</code>.</p>
<p>Here’s how you’d request an A100 GPU in the <code>jsteinhardt</code> partition.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">srun</span> <span class="at">-p</span> gpu <span class="at">-jsteinhardt</span> <span class="at">--gpus</span><span class="op">=</span>A100:1 <span class="at">--pty</span> bash</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">nvidia-smi</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>One can also use <code>sbatch</code> to run a background job.</p>
<p>More details on using the SCF cluster and the GPUs in it are available in our <a href="https://statistics.berkeley.edu/computing/servers/cluster/quick-start">quick start guide</a>.</p>
</section>
<section id="gpu-availability" class="level3">
<h3 class="anchored" data-anchor-id="gpu-availability">GPU availability</h3>
<p>If you’re on a machine with a (NVIDIA) GPU, you should be able to run <code>nvidia-smi</code> from the command line to see what GPUs are available to you. Depending on how the system is set up, if you’re running through a scheduler such as Slurm, you may not see all the GPUs that are physically on the machine.</p>
<p>The environment variable <code>CUDA_VISIBLE_DEVICES</code> will show the ID(s) of the GPU(s) available to you. You generally won’t need to use this, but it can be useful to check this variable.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="va">$CUDA_VISIBLE_DEVICES</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can check in Julia. Here’s what we would see if a GPU is available (also requiring that CUDA.jl is set up to use a GPU even if one is available).</p>
<div id="68be079a" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">CUDA</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="fu">has_cuda_gpu</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>true</code></pre>
<div class="callout callout-style-default callout-important callout-titled" title="Invoke `using CUDA` on a machine with a GPU">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Invoke <code>using CUDA</code> on a machine with a GPU
</div>
</div>
<div class="callout-body-container callout-body">
<p>I invoked <code>using CUDA</code> on a machine without a GPU and it caused some pre-compilation that then prevented me from using the GPU on a machine with a GPU, giving messages like “Error: CUDA.jl could not find an appropriate CUDA runtime to use”. Untangling that took some effort (in particular manually removing CUDA-related Julia packages via <code>rm -r ~/.julia/compiled/v1.10/CUDA*</code>).</p>
<p>So if you’re on a cluster or other system with multiple machines (such as the SCF), some with and without GPUs, it’s best if you only use <code>CUDA</code> on a machine with a GPU.</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled" title="CUDA precompilation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
CUDA precompilation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’ve noticed that if I try to use the Julia CUDA package on different machines within a cluster (e.g., on the SCF), that when I move to a different machine, the CUDA package (usually) gets precompiled again. I think this is because the compilation is specific to the hardware (perhaps both GPU and CPU). It does mean there can be a delay when <code>using CUDA</code> is run.</p>
</div>
</div>
<p>There are various other Julia CUDA package functions that we can explore for checking details of the GPU, such as <code>CUDA.total_memory()</code>.</p>
</section>
</section>
<section id="basic-gpu-offloading" class="level2">
<h2 class="anchored" data-anchor-id="basic-gpu-offloading">Basic GPU offloading</h2>
<p>Before we get into the complexities of how GPUs work and what one has to do to write one’s own code to use the GPU, let’s see how easily one can offload standard calculations (in particular linear algebra) to the GPU.</p>
<p>The main package we’ll use is the <a href="https://cuda.juliagpu.org/stable/">CUDA package</a>. It provides:</p>
<ul>
<li>the <code>CuArray</code> type for working with arrays on the GPU,</li>
<li>wrappers for functions in NVIDIA’s CUDA libraries (the NVIDIA CUDA toolkit is distinct from the Julia CUDA package) that allow one to use Julia functions/operators with CuArrays, and</li>
<li>tooling for writing CUDA kernels in Julia.</li>
</ul>
<p>In this section we’ll see the first two, before looking at writing kernels in the last section of these notes.</p>
<p>Note that this kind of offloading of linear algebra and vectorized computations to the GPU <a href="https://computing.stat.berkeley.edu/tutorial-parallelization/parallel-python#6-using-the-gpu-via-pytorch">can be done similarly with PyTorch, JAX and CuPy in Python</a>.</p>
<section id="matrix-multiplication-example" class="level3">
<h3 class="anchored" data-anchor-id="matrix-multiplication-example">Matrix multiplication example</h3>
<p>Matrix multiplication is simple, but it’s at the heart of a lot of algorithms, including deep learning, so it’s a simple, but non-trivial example of a situation where we could offload a part of a computation to the GPU for big speedups.</p>
<p>First we’ll set up the matrices, generating them on the host/CPU (and they’ll therefore be in CPU memory) and transferring to GPU (device) memory.</p>
<div id="1b7ddcc0" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">BenchmarkTools</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">CUDA</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">LinearAlgebra</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">7000</span>;</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">randn</span>(n, n);</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fu">randn</span>(n, n);</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> <span class="fu">CuArray</span>(x);</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>y_gpu <span class="op">=</span> <span class="fu">CuArray</span>(y);</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">## These use 64-bit numbers:</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="fu">typeof</span>(x)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix{Float64} (alias for Array{Float64, 2})</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="fu">typeof</span>(x_gpu)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="fu">total_memory</span>()</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 85097971712         # This is using an A100 GPU with 85 GB GPU memory.</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="fu">used_memory</span>()</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 784000000</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>n<span class="op">*</span>n<span class="op">*</span><span class="fl">8</span><span class="op">*</span><span class="fl">2</span>               <span class="co"># "Theoretical" memory use.</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 784000000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s compare the speed of the multiplication on CPU (host) vs.&nbsp;GPU (device). As usual we’ll put our computation into functions to allow for JIT compilation and avoid use of global variables whose type can change.</p>
<div id="a0009e9a" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">LinearAlgebra</span>.BLAS.<span class="fu">set_num_threads</span>(<span class="fl">1</span>);  <span class="co"># To keep things simple but perhaps unfair to the CPU.</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">matmult</span>(x, y)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> x <span class="op">*</span> y;</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> out <span class="op">=</span> <span class="fu">matmult</span>(x, y);  </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 10.280 s (2 allocations: 373.84 MiB)</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> z_gpu <span class="op">=</span> <span class="fu">matmult</span>(x_gpu, y_gpu);</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 41.270 ms (51 allocations: 1.19 KiB)</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>z_gpu <span class="op">=</span> <span class="fu">matmult</span>(x_gpu, y_gpu);</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="fu">typeof</span>(z_gpu)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># CuArray{Float64, 2, CUDA.DeviceMemory}</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>z[<span class="fl">1</span>]</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># ERROR: Scalar indexing is disallowed.</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Invocation of getindex resulted in scalar indexing of a GPU array.</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Moving the result back to the host.</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> <span class="fu">Array</span>(z_gpu);</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>z[<span class="fl">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So the speedup was about 250-fold!</p>
<p>In class we’ll use <code>nvidia-smi</code> and <code>top</code> to monitor the GPU and CPU.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Timing GPU code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Timing GPU code
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some comments on timing GPU code:</p>
<ul>
<li>I believe it’s <a href="https://cuda.juliagpu.org/stable/development/profiling/#Time-measurements">reasonable to use <code>@btime</code> to time GPU code</a></li>
<li>We could also use <code>CUDA.@time</code>.</li>
<li>Since GPU code is executed asynchronously, we should use <code>CUDA.@sync</code> or <code>synchronize</code> to make sure the execution is finished.</li>
</ul>
</div>
</div>
<p>GPUs use a lot of electricity. It’s possible that an administrator of a computing cluster might throttle the power consumption of a GPU server and this could affect computational speed.</p>
</section>
<section id="multiple-dispatch-with-gpu-operations" class="level3">
<h3 class="anchored" data-anchor-id="multiple-dispatch-with-gpu-operations">Multiple dispatch with GPU operations</h3>
<p>Given that we use <code>*</code> with two <code>CuArray</code> matrices, Julia’s multiple dispatch system is clearly involved in determining what code to call to execute the matrix multiplication on the GPU.</p>
<p>Digging around a bit in the source code for the <code>LinearAlgebra</code> and <code>CUDA</code> packages, it looks like the result of the dispatch is to call <code>LinearAlgebra.generic_matmatmul!</code>, with the method that works for CuArrays in the <code>lib/cublas/linalg.jl</code> file in the CUDA package. That presumably invokes the cuBLAS versions of the BLAS functions found in <code>lib/cublas/libcublas.jl</code> and <code>lib/cublas/wrappers.jl</code> where we see calls out to native C-based cuBLAS functions. But I don’t see where the C code is.</p>
<p>At a high level that makes sense, though the details of all the wrapping and type inheritance/dispatch that is going on might be hard to trace through carefully.</p>
</section>
<section id="copying-data-tofrom-the-gpu" class="level3">
<h3 class="anchored" data-anchor-id="copying-data-tofrom-the-gpu">Copying data to/from the GPU</h3>
<p>If we copy a lot of data back and forth to the GPU, that can end up being a large fraction of the time of a GPU-based calculation. We’d like to:</p>
<ul>
<li>generate data on the GPU if we can,</li>
<li>keep data on the GPU for multiple steps in a calculation, and</li>
<li>avoid copying output back to the device if possible.</li>
</ul>
<p>Let’s see how the time of copying compares to the time of doing the matrix multiplication.</p>
<div id="fa40de2c" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">randn</span>(n, n);</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># To the GPU:</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> x_gpu <span class="op">=</span> <span class="fu">CuArray</span>(x);</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 61.138 ms (10 allocations: 288 bytes)</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@time</span> CUDA.<span class="pp">@sync</span> x_gpu <span class="op">=</span> <span class="fu">CuArray</span>(x);</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">#  0.071553 seconds (12 CPU allocations: 320 bytes) (1 GPU allocation: 373.840 MiB, 0.03% memmgmt time)</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># From the GPU:</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@time</span> CUDA.<span class="pp">@sync</span> out <span class="op">=</span> <span class="fu">Array</span>(z_gpu);</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.329900 seconds (7 CPU allocations: 373.840 MiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So the transfer back takes much longer than the actual calculation!</p>
<p>The time for the transfer back to the host includes the time to allocate space on the host. We’d have to do more work to disentangle the transfer time from the allocation time (e.g., timing <code>y = Vector{Float64}(undef, n);</code>).</p>
</section>
<section id="vs-32-bit-numbers" class="level3">
<h3 class="anchored" data-anchor-id="vs-32-bit-numbers">64- vs 32-bit numbers</h3>
<p>In most contexts, computation on a GPU will use 32-bit real numbers for efficiency. This reduces memory use and speeds up computation because half as many bytes are being allocated, copied, and computed with. Of course we want to keep in mind the reduced precision in case that could be a problem for certain computations.</p>
<p>Let’s first simply see if matrix multiplication on the CPU is faster with 32-bit numbers.</p>
<div id="68d8b320" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">BenchmarkTools</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">matmult</span>(x, y)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> x <span class="op">*</span> y</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">5000</span>;</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">randn</span>(n, n);</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fu">randn</span>(n, n);</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> <span class="fu">matmult</span>(x, y);</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>x32 <span class="op">=</span> <span class="fu">convert</span>(<span class="dt">Array</span>{<span class="dt">Float32</span>}, x);</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>y32 <span class="op">=</span> <span class="fu">convert</span>(<span class="dt">Array</span>{<span class="dt">Float32</span>}, y);</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="fu">sizeof</span>(x)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="fu">sizeof</span>(x32)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> <span class="fu">matmult</span>(x32, y32);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  1.659 s (2 allocations: 190.73 MiB)
  895.202 ms (2 allocations: 95.37 MiB)</code></pre>
</div>
</div>
<p>Indeed it is quite a bit faster.</p>
<p>Now let’s consider that on the GPU. We’ll also show random number generation directly on the GPU to avoid the cost of transferring data to the GPU.</p>
<div id="7b6383c3" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x64_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="dt">Float64</span>, n, n);</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>y64_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="dt">Float64</span>, n, n);</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">typeof</span>(x64_gpu)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># CuArray{Float64, 2, CUDA.DeviceMemory}</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>x32_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n, n);   <span class="co"># `Float32` is the default.</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>y32_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n, n);</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="fu">typeof</span>(x32_gpu)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># CuArray{Float32, 2, CUDA.DeviceMemory}</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> z64_gpu <span class="op">=</span> <span class="fu">matmult</span>(x64_gpu, y64_gpu);</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 39.353 ms (51 allocations: 1.19 KiB)</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> z32_gpu <span class="op">=</span> <span class="fu">matmult</span>(x32_gpu, y32_gpu);</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 38.409 ms (51 allocations: 1.19 KiB)</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> z64 <span class="op">=</span> <span class="fu">Array</span>(z64_gpu);</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 298.258 ms (7 allocations: 373.84 MiB)</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> z32 <span class="op">=</span> <span class="fu">Array</span>(z32_gpu);</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 147.791 ms (7 allocations: 186.92 MiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I’m quite surprised the 32-bit multiplication is no faster. I’m not sure what is going on. (If you time the <code>randn</code> you should see that the 32-bit randn generation is about half the time of the 64-bit generation.)</p>
<p>It does make sense that the transfer back takes about half as long.</p>
</section>
<section id="interacting-with-nvidias-cuda-libraries" class="level3">
<h3 class="anchored" data-anchor-id="interacting-with-nvidias-cuda-libraries">Interacting with NVIDIA’s CUDA libraries</h3>
<p>Working with objects on the GPU requires interfacing with functions provided by the CUDA Toolkit and its libraries such as <code>cuBLAS</code>, <code>cuRAND</code>, <code>cuFFT</code>, etc. These are provided for Julia with the CUDA package (see <code>CUDA/&lt;version_string&gt;/lib</code>) so all that you need installed on the machine with the GPU is the NVIDIA driver software and the Julia CUDA package. You don’t actually need NVIDIA’s CUDA Toolkit.</p>
<p>Since Julia is calling out to CUDA functions to run code on the GPU, there may be functions that won’t work on a CuArray, but the various things I tried all seem to work.</p>
<div id="bcc5ad48" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>y_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="fl">3</span>);    <span class="co"># This uses cuRAND.</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>y_gpu <span class="op">+</span> y_gpu</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>3-element CuArray{Float32, 1, CUDA.DeviceMemory}:
 -2.3132815
  0.11570158
 -1.0381625</code></pre>
<div id="26dd0b73" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sin</span>.(y_gpu)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>3-element CuArray{Float32, 1, CUDA.DeviceMemory}:
 -0.986312
 -0.784628
  0.7079291</code></pre>
<p>However, we can’t work with scalar elements of the array because that tries to do a non-vectorized calculation on a GPU-based array.</p>
<div id="7c51f952" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>y_gpu[<span class="fl">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>julia&gt; y_gpu[1]
ERROR: Scalar indexing is disallowed.
Invocation of getindex resulted in scalar indexing of a GPU array.
This is typically caused by calling an iterating implementation of a method.
Such implementations *do not* execute on the GPU, but very slowly on the CPU,
and therefore should be avoided.</code></pre>
</section>
<section id="vectorized-example" class="level3">
<h3 class="anchored" data-anchor-id="vectorized-example">Vectorized example</h3>
<p>Here we’ll offload a vectorized calculation to the GPU. Note that in this case we cannot set up the calculation as a for loop as one can’t loop through scalar operations on GPU in that fashion (more about how to loop on a GPU in the discussion of GPU kernels).</p>
<p>We’ll use <code>.=</code> for in-place calculation to avoid additional memory allocation.</p>
<div id="e40d5a8f" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">vec_calc</span>(x)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  x <span class="op">.=</span> <span class="fu">tan</span>.(x) <span class="op">.+</span> <span class="fl">3</span> <span class="op">.*</span> <span class="fu">sin</span>.(x)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">0</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">250000000</span>;</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">rand</span>(n);            <span class="co"># 2 GB of data.</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> <span class="fu">CuArray</span>(x);</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co"># CPU calculation:</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> <span class="fu">vec_calc</span>(x);     <span class="co"># Again a bit unfair to the host/CPU in that we don't exploit multiple CPU cores.</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 9.041 s (0 allocations: 0 bytes)</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co"># GPU calculation:</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Try to flush out any compilation time.</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> <span class="fu">CuArray</span>([<span class="fl">1.1</span>, <span class="fl">2.2</span>])</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@sync</span> <span class="fu">vec_calc</span>(test);  <span class="co"># This seems to take surprisingly long.</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@time</span> CUDA.<span class="pp">@sync</span> <span class="fu">vec_calc</span>(x_gpu);</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.099259 seconds (39.83 k CPU allocations: 2.770 MiB)</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@time</span> CUDA.<span class="pp">@sync</span> <span class="fu">vec_calc</span>(x_gpu);</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.003745 seconds (88 CPU allocations: 3.672 KiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I’m not sure why the initial call on <code>test</code> takes so long. I’m also not sure what explains the big difference between running the calculation on the full vector the first and second times (including why the first call involves so many separate allocations). The first time represents a 91-fold speedup and the second time a surprising 2400-fold speedup.</p>
<p>I’m also not sure of what is going on behind the scenes in terms of the Julia interpeter and/or compiler processing the <code>vec_calc</code> code to allow it to be run via CUDA calls on the GPU. <code>@code_native vec_calc(x_gpu)</code> indicates use of the <code>GPUArrays</code> package, in particular use of <code>gpu_call</code> to run a kernel on the GPU. So I think the main question is) how Julia creates a Julia-coded kernel function (such as <a href="#gpu-kernels">this example Julia kernel</a>. (Once the kernel function is created, there seem to be tools to convert LLVM code to code that runs on the GPU (more <a href="#gpu-kernels">later</a>).</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>See how small <code>n</code> can be before the time of doing the calculation on the GPU exceeds the time for doing it on the CPU.</p>
<p>Then, consider how the transfer time to/from the GPU affects the trade-off (in this case presuming the only calculation you were doing was <code>vec_calc</code> and that you needed the full output back on the host for a later calculation that can’t be done on the GPU.</p>
</div>
</div>
</section>
</section>
<section id="gpu-architecture" class="level2">
<h2 class="anchored" data-anchor-id="gpu-architecture">GPU architecture</h2>
<p>Now that we’ve seen some of what we can do with GPUs, let’s consider some about how they work and what that implies about what calculations we can usefully offload to them.</p>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<p>GPUs have thousands of “slow” (slower than the CPU) cores. Their speed comes from massive parallelization and how the code is executed in parallel on the input data.</p>
<p>Each individual computation or series of computations on the GPU is done in a thread. Threads are organized into blocks and blocks of threads are organized in a grid. The blocks and grids can be 1-, 2-, or 3-dimensional. E.g., you might have a 1-d block of 256 threads, with a grid of 3 x 3 such blocks, for a total of 256×9=2304 threads.</p>
<p>The choice of the grid/block arrangement can affect efficiency. I’m not an expert at this level of detail but we’ll see some about this in the kernel example below. Note that using more than 1-dimensional grids and blocks is purely for the conceptual convenience of the programmer and doesn’t correspond to anything on the hardware. So for the most part we’ll use a one-dimensional grid of blocks and a one-dimensional blocks of threads.</p>
<p>In general you’d want each independent calculation done in a separate thread, but one might want to do a sequence of calculations on each thread. In general, you’ll want to pipeline together multiple operations within a computation to avoid copying from CPU to GPU and back. Alternatively, this can be done by keeping the data on the GPU and calling a second kernel.</p>
</section>
<section id="glossary" class="level3">
<h3 class="anchored" data-anchor-id="glossary">Glossary</h3>
<ul>
<li><em>streaming multiprocessor</em> (SM): a set of GPU cores with registers and shared memory; each GPU has multiple SMs</li>
<li><em>threads</em>: parallel execution units</li>
<li><em>blocks</em>: threads are grouped into blocks</li>
<li><em>grid</em>: blocks are grouped into a grid</li>
<li><em>warp</em>: a group of (usually) 32 threads that operate together</li>
<li><em>kernel</em>: a function that runs on the GPU (usually one thinks of a C function, but we’ll write Julia-coded kernels)</li>
<li><em>device</em>: the GPU and its memory</li>
<li><em>host</em>: the CPU and its memory</li>
</ul>
</section>
<section id="threads-blocks-and-grids" class="level3">
<h3 class="anchored" data-anchor-id="threads-blocks-and-grids">Threads, blocks and grids</h3>
<p>Some facts about the organization of threads on a GPU.</p>
<p>Threads:</p>
<ul>
<li>threads grouped into a warp (on A100 and possibly most others, 32 threads per warp)</li>
<li>threads in a warp operate in lock step (so an <code>if</code> statement causes threads to pause to wait for other threads that follow other branches)</li>
<li>GPU can switch between warps very quickly</li>
<li>threads should be cache-aware (best to have adjacent threads use adjacent memory)</li>
</ul>
<p>Blocks:</p>
<ul>
<li>1, 2, or 3-dimensional block of threads
<ul>
<li>2 and 3 dimensions are just for code clarity, not real</li>
</ul></li>
<li>threads in a block have access to some fast shared memory on chip (for A100 this is 49 KB)</li>
<li>A100 GPU has up to 1024 threads per block</li>
</ul>
<p>Grid</p>
<ul>
<li>1, 2, or 3-dimensional grid of blocks (again for code clarity only)</li>
<li>A100 GPU has up to 2147483647 blocks in the grid (I think)</li>
</ul>
</section>
<section id="gpu-execution-and-efficiency" class="level3">
<h3 class="anchored" data-anchor-id="gpu-execution-and-efficiency">GPU execution and efficiency</h3>
<ul>
<li>GPUS are good at executing the same computation across many data elements at once (called “SIMD” for “single instruction, multiple data”.</li>
<li>Threads in a block have access to some shared memory, which is on the chip, so access is very fast.</li>
<li>Threads operate in a warp of 32 threads.</li>
<li>Threads in a warp operate in lock-step, so <code>if</code> statements can be inefficient (some threads have to wait while a branch of the <code>if</code> statement is run for other threads).</li>
<li>GPUs can switch between warps very quickly, which would happen while a warp waits for data.</li>
<li>Threads in a warp should use neighboring values in an array for efficiency (note our discussion of the cache in the CPU context).</li>
<li>Computations on a GPU generally use single precision (32 bit floats) (or even 16 or 8 byte floats), which will often run much faster than with doubles (64 bit floats).</li>
</ul>
</section>
</section>
<section id="gpu-monitoring---details" class="level2">
<h2 class="anchored" data-anchor-id="gpu-monitoring---details">GPU monitoring - details</h2>
<section id="using-nvidia-smi-for-hardware-and-activity" class="level3">
<h3 class="anchored" data-anchor-id="using-nvidia-smi-for-hardware-and-activity">Using <code>nvidia-smi</code> for hardware and activity</h3>
<p>Here’s some example output from a machine with multiple GPUs, but only one of which is available to us:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="ex">nvidia-smi</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Fri Jan  3 14:11:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:1A:00.0 Off |                  N/A |
| 22%   23C    P8              2W /  100W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
</code></pre>
<p>Note that it shows that the GPU is an NVIDIA GeForce RTX 2080 Ti that has 11264MiB of memory (i.e., ~11 GB), and that at the moment, no process is using the GPU.</p>
<p>Next, if I logon as an administrator, I can see all 10 GPUs on one of the SCF machines, and that 7 of the GPUs are in use:</p>
<pre><code>Fri Jan  3 14:16:11 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100 80GB PCIe          On  | 00000000:4F:00.0 Off |                    0 |
| N/A   38C    P0              82W / 150W |   9887MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          On  | 00000000:52:00.0 Off |                    0 |
| N/A   40C    P0              80W / 150W |  11833MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          On  | 00000000:53:00.0 Off |                    0 |
| N/A   30C    P0              71W / 150W |  10039MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          On  | 00000000:56:00.0 Off |                    0 |
| N/A   39C    P0              82W / 150W |  10027MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA A100 80GB PCIe          On  | 00000000:57:00.0 Off |                    0 |
| N/A   32C    P0              79W / 150W |   9899MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA A100 80GB PCIe          On  | 00000000:CE:00.0 Off |                    0 |
| N/A   58C    P0             154W / 150W |  53949MiB / 81920MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA A100 80GB PCIe          On  | 00000000:D1:00.0 Off |                    0 |
| N/A   42C    P0              84W / 150W |  49121MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA A100 80GB PCIe          On  | 00000000:D2:00.0 Off |                    0 |
| N/A   24C    P0              41W / 150W |      4MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   8  NVIDIA A100 80GB PCIe          On  | 00000000:D5:00.0 Off |                    0 |
| N/A   35C    P0              79W / 150W |      7MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   9  NVIDIA A100 80GB PCIe          On  | 00000000:D6:00.0 Off |                    0 |
| N/A   42C    P0              86W / 150W |      7MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     37514      C   .../users/omer_ronen/mutemb/bin/python     9874MiB |
|    1   N/A  N/A     37509      C   .../users/omer_ronen/mutemb/bin/python    11820MiB |
|    2   N/A  N/A     37506      C   .../users/omer_ronen/mutemb/bin/python    10026MiB |
|    3   N/A  N/A     37522      C   .../users/omer_ronen/mutemb/bin/python    10014MiB |
|    4   N/A  N/A     37545      C   .../users/omer_ronen/mutemb/bin/python     9886MiB |
|    5   N/A  N/A    142017      C   python                                    53936MiB |
|    6   N/A  N/A    107884      C   ...2023/conda/envs/NODE/bin/python3.12    49108MiB |
+---------------------------------------------------------------------------------------+</code></pre>
</section>
<section id="devicequery-for-hardware-information" class="level3">
<h3 class="anchored" data-anchor-id="devicequery-for-hardware-information"><code>deviceQuery</code> for hardware information</h3>
<p>The <code>deviceQuery</code> utility provided with CUDA will give details on the GPU hardware.</p>
<p>Here’s an example with one of the SCF GPU servers, showing information about the processors available on the A100 GPU and the number of threads and blocks that are possible.</p>
<pre><code>paciorek@saruman:~&gt; deviceQuery
deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 10 CUDA Capable device(s)

Device 0: "NVIDIA A100 80GB PCIe"
  CUDA Driver Version / Runtime Version          12.2 / 12.2
  CUDA Capability Major/Minor version number:    8.0
  Total amount of global memory:                 81051 MBytes (84987740160 bytes)
  (108) Multiprocessors, (064) CUDA Cores/MP:    6912 CUDA Cores
  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
  Memory Clock rate:                             1512 Mhz
  Memory Bus Width:                              5120-bit
  L2 Cache Size:                                 41943040 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total shared memory per multiprocessor:        167936 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Managed Memory:                Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 79 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</code></pre>
<p>We can create the <code>deviceQuery</code> executable like this (this would need to be modified for other systems/machines):</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load cuda   <span class="co"># This gives access to the `nvcc` compiler.</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/NVIDIA/cuda-samples</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="ex">nvcc</span> Samples/1_Utilities/deviceQuery/deviceQuery.cpp <span class="at">-I</span>/usr/local/cuda-12.2/include <span class="dt">\</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>   <span class="at">-ICommon</span> <span class="at">-o</span> ~/deviceQuery</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="monitoring-gpu-usage" class="level4">
<h4 class="anchored" data-anchor-id="monitoring-gpu-usage">Monitoring GPU usage</h4>
<p><code>gpustat</code> is useful for monitoring details of GPU usage.</p>
<p>We can install is using the Python <code>pip</code> installer.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">--user</span> gpustat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb25"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="ex">~/.local/bin/gpustat</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>saruman                   Fri Jan  3 14:33:36 2025  535.104.05
[0] NVIDIA A100 80GB PCIe | 42°C,   0 % |  9887 / 81920 MB | omer_ronen(9874M)
[1] NVIDIA A100 80GB PCIe | 39°C,   0 % | 11833 / 81920 MB | omer_ronen(11820M)
[2] NVIDIA A100 80GB PCIe | 29°C,   0 % | 10039 / 81920 MB | omer_ronen(10026M)
[3] NVIDIA A100 80GB PCIe | 38°C,   0 % | 10027 / 81920 MB | omer_ronen(10014M)
[4] NVIDIA A100 80GB PCIe | 32°C,   0 % |  9899 / 81920 MB | omer_ronen(9886M)
[5] NVIDIA A100 80GB PCIe | 60°C, 100 % | 68155 / 81920 MB | arjunpatrawala(68142M)
[6] NVIDIA A100 80GB PCIe | 47°C,  14 % | 36377 / 81920 MB | zhangyunzhe2023(36364M)
[7] NVIDIA A100 80GB PCIe | 24°C,   0 % |     4 / 81920 MB |
[8] NVIDIA A100 80GB PCIe | 34°C,   0 % |     7 / 81920 MB |
[9] NVIDIA A100 80GB PCIe | 42°C,   0 % |     7 / 81920 MB |</code></pre>
<p>There are various flags. I’m not familiar with the details but this invocation shows more information about the processes running on each GPU:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="ex">~/.local/bin/gpustat</span> <span class="at">-fup</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>saruman                   Fri Jan  3 14:35:20 2025  535.104.05
[0] NVIDIA A100 80GB PCIe | 47°C,   0 % |  9887 / 81920 MB | omer_ronen/37514(9874M)
 └─  37514 (  90%,   14GB): /scratch/users/omer_ronen/mutemb/bin/python -m mutemb.utils.data --chrom 1
[1] NVIDIA A100 80GB PCIe | 41°C,   0 % | 11833 / 81920 MB | omer_ronen/37509(11820M)
 └─  37509 ( 100%,   10GB): /scratch/users/omer_ronen/mutemb/bin/python -m mutemb.utils.data --chrom 2
[2] NVIDIA A100 80GB PCIe | 29°C,   0 % | 10039 / 81920 MB | omer_ronen/37506(10026M)
 └─  37506 (  10%,   11GB): /scratch/users/omer_ronen/mutemb/bin/python -m mutemb.utils.data --chrom 3
[3] NVIDIA A100 80GB PCIe | 38°C,   0 % | 10027 / 81920 MB | omer_ronen/37522(10014M)
 └─  37522 ( 120%,   37GB): /scratch/users/omer_ronen/mutemb/bin/python -m mutemb.utils.data --chrom 4
[4] NVIDIA A100 80GB PCIe | 32°C,   0 % |  9899 / 81920 MB | omer_ronen/37545(9886M)
 └─  37545 ( 100%,   26GB): /scratch/users/omer_ronen/mutemb/bin/python -m mutemb.utils.data --chrom 5
[5] NVIDIA A100 80GB PCIe | 60°C, 100 % | 68155 / 81920 MB | arjunpatrawala/142017(68142M)
 └─ 142017 ( 100%, 4010MB): python generate_jacobians.py
[6] NVIDIA A100 80GB PCIe | 44°C,   0 % | 27707 / 81920 MB | zhangyunzhe2023/228882(27724M)
 └─ 228882 (  50%, 1144MB): /scratch/users/zhangyunzhe2023/conda/envs/NODE/bin/python3.12 -Xfrozen_modules=off -m ipykernel_launcher -f /accounts/grad/zhangyunzhe2023/.jupyter/runtime/kernel-ef1939c2-4af1-4fe4-bf29-ae1195d5ab4a.json
[7] NVIDIA A100 80GB PCIe | 24°C,   0 % |     4 / 81920 MB |
[8] NVIDIA A100 80GB PCIe | 35°C,   0 % |     7 / 81920 MB |
[9] NVIDIA A100 80GB PCIe | 42°C,   0 % |     7 / 81920 MB |</code></pre>
</section>
</section>
</section>
<section id="gpu-kernels" class="level2">
<h2 class="anchored" data-anchor-id="gpu-kernels">GPU kernels</h2>
<p>A <em>kernel</em> is a function that can be run on the GPU.</p>
<p>Next we’ll see that we can actually <a href="https://cuda.juliagpu.org/stable/development/kernel/">write a kernel</a> in Julia and then call that kernel. Kernels are functions that encode the core computational operations that are executed in parallel.</p>
<p>In other languages, the basic mode of operation with a GPU when you are writing your own GPU code is to write a kernel using CUDA (basically C) code and then call the kernel in parallel via C, R, or Python code. In Julia, we can write the kernel using Julia syntax (though many operations (particularly non-numerical ones) will not run on the GPU…).</p>
<p>I have not investigated much to see what Julia does behind the scenes with the kernel code for it to be able to run on the GPU, but <a href="https://robertcunningham.xyz/how-does-cuda-jl-work/">this blog post</a> seems helpful. I <em>think</em> the steps involve going from LLVM code to PTX code (an assembly language for GPUs) that is then converted to GPU machine code. These steps involve tooling available outside of Julia, which seems like a reflection of the benefit of choosing to have Julia’s compilation system use LLVM.</p>
<section id="basic-example" class="level3">
<h3 class="anchored" data-anchor-id="basic-example">Basic example</h3>
<p>Here’s a basic example in which we’ll do a calculation in place. We run 1000 scalar calculations using 1000 threads.</p>
<p>We use <code>@cuda</code> to compile and run the kernel.</p>
<div id="0847f32c" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">my_kernel</span>(x)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span> <span class="fu">threadIdx</span>().x;   <span class="co"># What thread am I?</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> idx <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    x[idx] <span class="op">=</span> <span class="fu">tan</span>(x[idx]) <span class="op">+</span> <span class="fl">3</span><span class="fu">*sin</span>(x[idx]);</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">1000</span>;</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n);</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(x_gpu)[n]</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="co"># -1.5321726f0</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="pp">@cuda</span> threads<span class="op">=</span>n <span class="fu">my_kernel</span>(x_gpu);</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(x_gpu)[n]   <span class="co"># Check the computation was done by checking last element.</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="co"># -28.875708f0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are limits on the number of threads we can use.</p>
<div id="d2eefd24" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">2000</span>;</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n);</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="pp">@cuda</span> threads<span class="op">=</span>n <span class="fu">my_kernel</span>(x_gpu);</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ERROR: Number of threads in x-dimension exceeds device limit (2000 &gt; 1024).</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="multiple-blocks" class="level3">
<h3 class="anchored" data-anchor-id="multiple-blocks">Multiple blocks</h3>
<p>We need to use at least as many threads as computations, and in addition to only being able to use 1024 threads in the x dimension, we can have at most 1024 threads in a block on the A100 GPU we’re using. So we’ll need multiple blocks.</p>
<div id="bf350e97" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">my_kernel</span>(x)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> idx <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    x[idx] <span class="op">=</span> <span class="fu">tan</span>(x[idx]) <span class="op">+</span> <span class="fl">3</span><span class="fu">*sin</span>(x[idx]);</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">2000</span>;</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n);</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>initial <span class="op">=</span> <span class="fu">Array</span>(x_gpu)[n]</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fu">Int</span>(<span class="fu">ceil</span>(n<span class="op">/</span>nthreads));</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">my_kernel</span>(x_gpu);</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>(initial, <span class="fu">Array</span>(x_gpu)[n])  <span class="co"># Check that calculation was done.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s do a smaller test run in which we can check on the thread and block indexing.</p>
<div id="7f0226a9" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">my_kernel_print</span>(x)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> idx <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    x[idx] <span class="op">=</span> <span class="fu">tan</span>(x[idx]) <span class="op">+</span> <span class="fl">3</span><span class="fu">*sin</span>(x[idx]);</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="pp">@cuprintln</span> idx, i, j, <span class="fu">blockDim</span>().x, <span class="fu">blockDim</span>().y;</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">200</span>;</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n);</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">100</span>;</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fu">Int</span>(<span class="fu">ceil</span>(n<span class="op">/</span>nthreads));</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a><span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">my_kernel_print</span>(x_gpu);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When we run this, notice the output seems to be grouped based on warps of 32 threads (apart from the last set since <code>n=200</code> is not a multiple of 32).</p>
</section>
<section id="larger-computations" class="level3">
<h3 class="anchored" data-anchor-id="larger-computations">Larger computations</h3>
<p>In many cases we’ll have more tasks than the total number of GPU cores. As long as we don’t exceed the maximum size of a block or grid, we can just ask for as many threads as we have tasks and rely on the GPU to manage assigning the tasks to the GPU cores.</p>
<p>We’d want to check that the number/dimension of the block here does not exceed the maximum block size. I didn’t do that, but it ran, so it must have been ok!</p>
<p>Here we’ll run the computation we ran earlier when we did not write our own kernel and just relied on Julia to offload to the GPU behind the scene.</p>
<div id="d40c29aa" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">250000000</span>;</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n);</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fu">Int</span>(<span class="fu">ceil</span>(n<span class="op">/</span>nthreads));</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(x_gpu)[n]</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run it once to flush out any compilation/transformation time.</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>y_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="fl">5</span>);</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">my_kernel</span>(y_gpu);</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@time</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">my_kernel</span>(x_gpu);</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.002003 seconds (45 CPU allocations: 2.719 KiB)</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(x_gpu)[n]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The 2.0 ms is reasonably comparable to the 3.7 ms when we just had Julia run the <a href="./#vectorized-example">vectorized computation on the GPU</a> (from the last time we ran it). That used 64-bit floats. When I reran the code above using 64-bit floats, the time was 5.2 ms.</p>
</section>
</section>
<section id="efficient-memory-access" class="level2">
<h2 class="anchored" data-anchor-id="efficient-memory-access">Efficient memory access</h2>
<p>We’ll explore two final topics related to efficiently accessing data in memory: first accessing global GPU memory efficiently and second making use of shared GPU memory.</p>
<section id="coalesced-access-to-global-memory" class="level3">
<h3 class="anchored" data-anchor-id="coalesced-access-to-global-memory">Coalesced access to global memory</h3>
<p>If adjacent threads in a block access adjacent memory locations, a chunk of data can be obtained in a single access to global memory.</p>
<p>We’ll implement element-wise summing of two matrices. Obviously one can just do this directly with <code>CuArray</code>s in Julia, but if we implement it ourselves, it illustrates that reading a matrix by column is much more efficient than reading by row.</p>
<div id="c3340026" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">10000</span>;</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>X_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n,n);</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>Y_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n,n);</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>out_gpu <span class="op">=</span> CUDA.<span class="fu">zeros</span>(n,n);</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>X_gpu_small <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="fl">5</span>,<span class="fl">5</span>);</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>Y_gpu_small <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="fl">5</span>,<span class="fl">5</span>);</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>out_gpu_small <span class="op">=</span> CUDA.<span class="fu">zeros</span>(<span class="fl">5</span>,<span class="fl">5</span>);</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Good: Adjacent threads process elements in a column.</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">kernel_sum_bycol!</span>(X, Y, output)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    row_idx <span class="op">=</span> <span class="fu">threadIdx</span>().x <span class="op">+</span> (<span class="fu">blockIdx</span>().x <span class="op">-</span> <span class="fl">1</span>)<span class="fu">*blockDim</span>().x;</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    col_idx <span class="op">=</span> <span class="fu">blockIdx</span>().y;</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> row_idx <span class="op">&lt;=</span> <span class="fu">size</span>(X, <span class="fl">1</span>) <span class="op">&amp;&amp;</span> col_idx <span class="op">&lt;=</span> <span class="fu">size</span>(Y, <span class="fl">2</span>)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        output[row_idx, col_idx] <span class="op">=</span> X[row_idx, col_idx] <span class="op">+</span> Y[row_idx, col_idx]</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="cn">nothing</span></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="co"># x dim of grid is number of thread blocks in a column.</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a><span class="co"># y dim of grid is number of columns.</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> (<span class="fu">Int</span>(<span class="fu">ceil</span>(n<span class="op">/</span>nthreads)), n);</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Flush out any compilation time.</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">kernel_sum_bycol!</span>(X_gpu_small, Y_gpu_small, out_gpu_small);</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">kernel_sum_bycol!</span>(X_gpu, Y_gpu, out_gpu);</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 2.153 ms (47 allocations: 1.30 KiB)</span></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Bad: Adjacent threads process elements in a row.</span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">kernel_sum_byrow!</span>(X, Y, output)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>    row_idx <span class="op">=</span> <span class="fu">blockIdx</span>().y;</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>    col_idx <span class="op">=</span> <span class="fu">threadIdx</span>().x <span class="op">+</span> (<span class="fu">blockIdx</span>().x <span class="op">-</span> <span class="fl">1</span>)<span class="fu">*blockDim</span>().x;</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> row_idx <span class="op">&lt;=</span> <span class="fu">size</span>(X, <span class="fl">1</span>) <span class="op">&amp;&amp;</span> col_idx <span class="op">&lt;=</span> <span class="fu">size</span>(Y, <span class="fl">2</span>)</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>        output[row_idx, col_idx] <span class="op">=</span> X[row_idx, col_idx] <span class="op">+</span> Y[row_idx, col_idx]</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="cn">nothing</span></span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Flush out any compilation time.</span></span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">kernel_sum_byrow!</span>(X_gpu_small, Y_gpu_small, out_gpu_small);</span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">kernel_sum_byrow!</span>(X_gpu, Y_gpu, out_gpu);</span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a><span class="co"># 10.500 ms (47 allocations: 1.30 KiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="using-shared-memory" class="level3">
<h3 class="anchored" data-anchor-id="using-shared-memory">Using shared memory</h3>
<p>Accessing global GPU memory is much slower than doing computation on the GPU. So we’d like to avoid repeated access to global memory (e.g., a bad scenario would be a ratio of one arithmetic calculation per retrieval from global memory). One strategy is for multiple threads in a block to cooperate to load data from global memory into shared memory accessible by all the threads in the block. The computation can then be done on the data in shared memory.</p>
<p>Here’s a simplified example that shows how to load the data into shared memory. There’s no actual computation coded here, but if we think of the kernel density estimation problem in PS 3, one could imagine that each thread would then each do a computation that uses the entire chunk of data in shared memory.</p>
<div id="88a4e3f5" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">kernel_reader_bycol</span>(x<span class="op">::</span><span class="dt">CuDeviceArray{T}</span>) <span class="kw">where</span> T</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>  dims <span class="op">=</span> <span class="fu">size</span>(x);</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Setup shared memory for the subset of data.</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>  shared_data <span class="op">=</span> <span class="fu">CuDynamicSharedArray</span>(T, (<span class="fu">blockDim</span>().x, dims[<span class="fl">2</span>]));</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> chunk_start <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">blockDim</span>().x<span class="op">:</span>dims[<span class="fl">1</span>]</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    chunk_size <span class="op">=</span> <span class="fu">min</span>(<span class="fu">blockDim</span>().x, dims[<span class="fl">1</span>] <span class="op">-</span> chunk_start <span class="op">+</span> <span class="fl">1</span>);</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Transfer a chunk of rows in parallel, one row per thread.</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;=</span> chunk_size</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> col <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>dims[<span class="fl">2</span>]</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>        shared_data[i, col] <span class="op">=</span> x[chunk_start <span class="op">+</span> i <span class="op">-</span> <span class="fl">1</span>, col];</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>      <span class="cf">end</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sync_threads</span>()</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># At this point we'd insert code to do the actual computation, based on `idx`.</span></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each thread now has the opportunity to compute on all the data in the chunk in</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `shared_data`.</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">10000000</span>;</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="fl">10</span>;</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n, m);</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>x_gpu_small <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="fl">5</span>, m);</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fl">100</span>;  <span class="co"># This is arbitrary in this example as we are not doing an actual computation.</span></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>memsize <span class="op">=</span> nthreads <span class="op">*</span> m <span class="op">*</span> <span class="fl">4</span>;</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks shmem<span class="op">=</span>memsize <span class="fu">kernel_reader_bycol</span>(x_gpu_small);</span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@time</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks shmem<span class="op">=</span>memsize <span class="fu">kernel_reader_bycol</span>(x_gpu);</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.138480 seconds (24 CPU allocations: 752 bytes)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If <code>m</code> gets much bigger, we get an error “ERROR: Amount of dynamic shared memory exceeds device limit (400.000 KiB &gt; 48.000 KiB).” So for larger <code>m</code> we’d need to rework how we manipulate the data.</p>
<p>Let’s close by seeing if the memory access patterns make a difference in this example. Instead of accessing by column, we’ll access by row, but with the matrix transposed so it is very wide instead of very long.</p>
<p>My initial thought was that accessing by row would be slower because adjacent threads are not reading from adjacent locations in global memory.</p>
<div id="128c9752" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">kernel_reader_byrow</span>(x<span class="op">::</span><span class="dt">CuDeviceArray{T}</span>) <span class="kw">where</span> T</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>  dims <span class="op">=</span> <span class="fu">size</span>(x);</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Setup shared memory for the subset of data.</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>  shared_data <span class="op">=</span> <span class="fu">CuDynamicSharedArray</span>(T, (dims[<span class="fl">1</span>], <span class="fu">blockDim</span>().x));</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> chunk_start <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">blockDim</span>().x<span class="op">:</span>dims[<span class="fl">2</span>]</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    chunk_size <span class="op">=</span> <span class="fu">min</span>(<span class="fu">blockDim</span>().x, dims[<span class="fl">2</span>] <span class="op">-</span> chunk_start <span class="op">+</span> <span class="fl">1</span>);</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Transfer a chunk of rows in parallel, one column per thread.</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;=</span> chunk_size</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> row <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>dims[<span class="fl">1</span>]</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        shared_data[row, i] <span class="op">=</span> x[row, chunk_start <span class="op">+</span> i <span class="op">-</span> <span class="fl">1</span>];</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>      <span class="cf">end</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sync_threads</span>()</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># At this point we'd insert code to do the actual computation, based on `idx`.</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each thread now has the opportunity to compute on all the data in the chunk in</span></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `shared_data`.</span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">10000000</span>;</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="fl">10</span>;</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>x_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(m, n);</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>x_gpu_small <span class="op">=</span> CUDA.<span class="fu">randn</span>(m, <span class="fl">5</span>);</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fl">100</span>;  <span class="co"># This is arbitrary in this example as we are not doing an actual computation.</span></span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>memsize <span class="op">=</span> nthreads <span class="op">*</span> m <span class="op">*</span> <span class="fl">4</span>;</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks shmem<span class="op">=</span>memsize <span class="fu">kernel_reader_byrow</span>(x_gpu);</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>CUDA.<span class="pp">@time</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks shmem<span class="op">=</span>memsize <span class="fu">kernel_reader_byrow</span>(x_gpu);</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.105434 seconds (25 CPU allocations: 1008 bytes)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see that the access by row here is (a bit) faster. I think this is because the entire chunk of data in the wide matrix lives in a small area of global memory, while in the long matrix, each column in the chunk has adjacent values but separate columns are very far apart because the matrix is so long.</p>
<p>We might be able to improve efficiency with the wide matrix by operating by column within the wide matrix. This would involve more work to manage the indexing because we wouldn’t just have each thread manage a column (unless we used very few threads, which would presumably reduce efficiency).</p>
</section>
</section>
<section id="using-atomics-for-reduction-operations" class="level2">
<h2 class="anchored" data-anchor-id="using-atomics-for-reduction-operations">Using atomics for reduction operations</h2>
<p>One thing we haven’t seen so far is being able to have different threads write to the same memory location (e.g., to a scalar or to an element of an array). One can easily imagine needing to do this to carry out reduction operations (e.g., calculating a sum or a max or min).</p>
<p>The obvious danger is that two threads might write to the memory location at the same time and somehow cause the location not to be properly updated.</p>
<p>Suppose we want to calculate the log-likelihood (or some other loss function) across independent observations. We’d like to do the summation on the GPU to avoid passing all the log-likelihood values from GPU to CPU and then having to do the sum on the CPU.</p>
<div id="5f79a8f3" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">BenchmarkTools</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Distributions</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">100_000_000</span>;   <span class="co"># Formatted for readability.</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>norm_dist <span class="op">=</span> <span class="fu">Normal</span>(<span class="fl">0</span>,<span class="fl">1</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> <span class="fu">rand</span>(norm_dist, n);</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">loglik_kernel</span>(x, result)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> idx <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># logpdf(norm_dist, x[idx]) # Doesn't compile.</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>    CUDA.<span class="pp">@atomic</span> result[<span class="fl">1</span>] <span class="op">+=</span> <span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>x[idx]<span class="op">^</span><span class="fl">2</span>;            <span class="co"># Experimental, but nicer interface.</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">#CUDA.atomic_add!(pointer(result), -0.5*x[idx]^2);  # Stable low-level API.</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>nthreads <span class="op">=</span> <span class="fl">1024</span>;</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>nblocks <span class="op">=</span> <span class="fu">Int</span>(<span class="fu">ceil</span>(n<span class="op">/</span>nthreads));</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>samples_gpu <span class="op">=</span> <span class="fu">CuArray</span>(samples);</span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> CUDA.<span class="fu">zeros</span>(<span class="fu">typeof</span>(samples[<span class="fl">1</span>]), <span class="fl">1</span>);</span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks <span class="fu">loglik_kernel</span>(samples_gpu, result);</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 161.352 ms (34 allocations: 944 bytes)</span></span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(result)[<span class="fl">1</span>] <span class="fu">-n*log</span>(<span class="fl">2</span><span class="op">*</span><span class="cn">pi</span>)<span class="op">/</span><span class="fl">2</span>  <span class="co"># Adjust for normalizing constant as scalar computation, not on GPU.</span></span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> <span class="fu">sum</span>(<span class="fu">logpdf</span>.(norm_dist, samples))</span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 1.410 s (5 allocations: 762.94 MiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So we got about a 12-fold speedup, which is less than we’ve been getting for some of our other comparisons.</p>
<p>I was curious how much time is spent handling the reduction operation (presumably there is some loss in efficiency from having all the threads write to the same memory location). When I changed <code>result</code> to be a vector of length equal to that of <code>samples</code> and just assign the individual PDF evaluations to the corresponding elements of <code>result</code> without the atomic operation, the time was 3 milliseconds (compared to 161 above), so there is a performance degradation from the atomic operation.</p>
<section id="using-shared-memory-to-reduce-the-cost-of-atomic-operations" class="level3">
<h3 class="anchored" data-anchor-id="using-shared-memory-to-reduce-the-cost-of-atomic-operations">Using shared memory to reduce the cost of atomic operations</h3>
<p>One solution to the performance degradation is to not have all of the summing make use of the same location in memory to accumulate the result.</p>
<p>Instead we can use shared memory to more efficiently do the reduction within each thread block before doing the final reduction across blocks. Here’s an approach using a tree-like operation (as suggested by a ChatBot, but requiring some debugging on my part) to compute the partial sum within each thread block before using the atomic operation to compute the sum of the partial sums:</p>
<div id="8e43b898" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">loglik_kernel_shmem</span>(x<span class="op">::</span><span class="dt">CuDeviceArray{T}</span>, result<span class="op">::</span><span class="dt">CuDeviceArray{T}</span>) <span class="kw">where</span> T</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>  i <span class="op">=</span> <span class="fu">threadIdx</span>().x;  <span class="co"># What thread am I within the block?</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>  j <span class="op">=</span> <span class="fu">blockIdx</span>().x;   <span class="co"># What block am I in?</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span>  (j<span class="op">-</span><span class="fl">1</span>)<span class="fu">*blockDim</span>().x <span class="op">+</span> i;</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>  shared_data <span class="op">=</span> <span class="fu">CuDynamicSharedArray</span>(T, (<span class="fu">blockDim</span>().x));</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># First do the core calculation and store in shared memory.</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> idx <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>     shared_data[i] <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>x[idx]<span class="op">^</span><span class="fl">2</span>;</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>     shared_data[i] <span class="op">=</span> <span class="fl">0.0</span>; </span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Tree-like partial sum within the thread block,</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># summing pairs until the sum within the block</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># is contained in `shared_data[1]`.</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>  s <span class="op">=</span> <span class="fu">blockDim</span>().x <span class="op">÷</span> <span class="fl">2</span>;   <span class="co"># `÷` ensures `s` is Int.</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">while</span> s <span class="op">&gt;=</span> <span class="fl">1</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;=</span> s</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>      shared_data[i] <span class="op">+=</span> shared_data[i <span class="op">+</span> s];</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sync_threads</span>()</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>    s <span class="op">÷=</span> <span class="fl">2</span>;</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The first thread in the block writes the partial sum to global memory.</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> i <span class="op">==</span> <span class="fl">1</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>    CUDA.<span class="pp">@atomic</span> result[<span class="fl">1</span>] <span class="op">+=</span> shared_data[<span class="fl">1</span>];</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">end</span></span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>memsize <span class="op">=</span> nthreads <span class="op">*</span> <span class="fu">sizeof</span>(samples[<span class="fl">1</span>]);</span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>result2 <span class="op">=</span> CUDA.<span class="fu">zeros</span>(<span class="fu">typeof</span>(samples[<span class="fl">1</span>]), <span class="fl">1</span>);</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="pp">@cuda</span> threads<span class="op">=</span>nthreads blocks<span class="op">=</span>nblocks shmem<span class="op">=</span>memsize <span class="fu">loglik_kernel_shmem</span>(samples_gpu, result2);</span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 6.317 ms (34 allocations: 944 bytes)</span></span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a><span class="fu">Array</span>(result2)[<span class="fl">1</span>] <span class="fu">-n*log</span>(<span class="fl">2</span><span class="op">*</span><span class="cn">pi</span>)<span class="op">/</span><span class="fl">2</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="debugging-kernel-code" class="level3">
<h3 class="anchored" data-anchor-id="debugging-kernel-code">Debugging kernel code</h3>
<p>It can be much harder to debug kernel code than regular Julia code. If the syntax doesn’t produce valid LLVM/IR code, it may not be obvious from the error messsage what the problem is.</p>
<p>As an example in the code in the previous section, I originally had <code>s = blockDim().x / 2;</code> and <code>s /= 2;</code>. I didn’t realize that even with integer inputs, that this produced a float output type for <code>s</code> and that as a result using <code>s</code> for indexing in <code>shared_data[i + s];</code> wouldn’t work. The error message said there was a problem with the LLVM/IR code produced from the kernel, but didn’t say where and it took a binary search on my part to figure out that <code>shared_data[i + s];</code> was the problematic piece of code and that was caused by <code>s</code> being a float.</p>
<p>On an only somewhat related point, the ChatBot originally gave me while <code>s &gt;= 0</code>, which is a bug that doesn’t prevent the code from running, but does give incorrect numerical results, so we still need to be careful with what we get from ChatBots.</p>
</section>
</section>
<section id="final-comments---when-to-use-the-gpu" class="level2">
<h2 class="anchored" data-anchor-id="final-comments---when-to-use-the-gpu">Final comments - when to use the GPU</h2>
<p>To effectively use the GPU, one generally wants to have a computation in which the same calculation is being done on many data elements. Things like the vectorized examples and matrix multiplication seen above, pixel-wise processing of images/video, simulations with many independent samples, etc.</p>
<p>Situations with a lot of conditionality (if-else branching), small data sizes, and lots of data transfer to/from the GPU are generally not good use cases for the GPU.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/berkeley-stat244\.github\.io\/spring-2025");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>