---
title: "Notes 6: JIT compilation"
author: "Chris Paciorek"
date: "2025-02-06"
format:
  pdf:
    documentclass: article
    margin-left: 30mm
    margin-right: 30mm
    toc: true
  html:
    theme: cosmo
    css: ../assets/styles.css
    toc: true
    code-copy: true
    code-block-background: true
engine: jupyter
ipynb-shell-interactivity: all
execute:
  daemon: false
---

## Introduction

This document is the sixth of a set of notes, this document focusing on the basics of Just-in-Time (JIT) compilation. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.

Given that, the document heavily relies on demos, with interpretation in some cases left to the reader.

## Compilation overview

###  Compilation and interpreters

Compilation is the process of transforming code from one representation to another. This generally involves transforming high-level, human-readable code to lower level representations that are closer to the instructions the CPU actually performs. For example, the [C compiler transforms C code to (binary) machine code](https://www.geeksforgeeks.org/compiling-a-c-program-behind-the-scenes/) (the `.o` or `.so` file(s)). One can then run the binary code directly.

In contrast, R or Python code is executed by an interpreter that parses the code and evaluates it.  Generally the interpreter is itself a C program (e.g., CPython). When you are in an interactive environment (a "REPL"), there is the additional layer of the interactive interface (e.g., RStudio, IPython).

### JIT compilation overview

Unlike compilation of standard non-interactive languages like C, C++, and Fortran, the Julia compilation process happens at run-time rather than in advance, hence "just-in-time".

The compilation process involves a [series of transformation steps](https://docs.julialang.org/en/v1/devdocs/jit/), including:

 - parsing
 - macro expansion
 - type inference
 - translation to an intermediate representation (IR)
 - translation to LLVM code
 - generation of machine code

![Julia compiler steps (courtesy of the Julia manual)](compiler_diagram.png)

We'll see at the end of these notes that we can look at the output of the various steps using the macros shown in the diagram.

### LLVM

Julia uses [LLVM](https://en.wikipedia.org/wiki/LLVM), which provides the middle layers of a compilation system. For example, on a Mac, the default C/C++ compiler is Clang, which is build on LLVM.

LLVM provides an intermediate representation (IR) that is independent of the programming language and can be thought of as a high-level assembly language.
By going through LLVM's IR, one can take advantage of LLVM's tools for code optimization.

## JIT examples

### Multiple dispatch

Julia compiles version of functions for each type of inputs. Let's see that with an example.

```{julia}
function mysum(x)
  out=(typeof(x[1]))(0)
  n = length(x)
  for i in 1:n
     out += x[i]
  end
  return out
end

xfloat = [3.1, 5.7, 3.2]
xfloat32 = Float32[3.1, 5.7, 3.2]
xint = [3, 5, 4]

@time mysum(xfloat)
```
```{julia}
@time mysum(xfloat)
```
```{julia}
@time mysum(xfloat32)
```
```{julia}
@time mysum(xint)
```

### For loop speed

Let's explore the timing of Julia looping to better understand the JIT compilation.

We'll use the well-used example of the Monte Carlo approach to estimating $\pi$.


Here's a Julia implementation:

```{julia}
in_circle = 0;
num_throws = 5000;

# Run Monte Carlo simulation
for _ in 1:num_throws
  # Generate random x and y coordinates between -1 and 1.
  xpos = rand() * 2 - 1.0  # Equivalent to random.uniform(-1.0, 1.0)
  ypos = rand() * 2 - 1.0

  # Check if point is inside unit circle.
  if sqrt(xpos^2 + ypos^2) <= 1.0  # Equivalent to math.hypot()
    in_circle += 1
  end
end

# Estimate PI
pi_estimate = 4 * in_circle / num_throws
```

::: {.callout-tip title="Exercise"}
Try the following:

 1. Time the for loop above without putting it into a function (`@time begin ... end`).
 2. Put the code within a function and time it using `@time` when running the first time.
 3. Time it after running it a second time
 4. Time a vectorized version without the loop (also within a function)

Fill in your timing answers in [this Google form](https://forms.gle/deR11fRdR3i32PnT6) and we'll discuss during the next class.
:::

Here's [some Julia code](./speed.jl) to explore this further.

## JIT and global versus local variables

Using global variables is much less efficient than using local variables because the type of the global variable could change and that makes it hard to generate simple optimized code.

```{julia}
function squaresum(x, y)
  return x^2 + y^2
end

function squaresum_with_global(x)
  return x^2 + y^2
end
```


### The intermediate representations

Let's look at the LLVM (intermediate representation) code produced by the JIT compilation process. 

The LLVM code for the basic version is simple and in fact we can read and understand it. 
```{julia}
@code_llvm squaresum(4, 3)
```
In contrast the LLVM code for the version with the global is quite complicated (and only partially shown below).
```{julia}
#| eval: false
@code_llvm squaresum_with_global(4)
```
```
;  @ REPL[22]:1 within `squaresum_with_global`
define nonnull {}* @julia_squaresum_with_global_590(i64 signext %0) #0 {
top:
  %1 = alloca [3 x {}*], align 8
  %gcframe2 = alloca [4 x {}*], align 16
  %gcframe2.sub = getelementptr inbounds [4 x {}*], [4 x {}*]* %gcframe2, i64 0, i64 0
  %.sub = getelementptr inbounds [3 x {}*], [3 x {}*]* %1, i64 0, i64 0
  %2 = bitcast [4 x {}*]* %gcframe2 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 16 %2, i8 0, i64 32, i1 true)
  %thread_ptr = call i8* asm "movq %fs:0, $0", "=r"() #6
  %tls_ppgcstack = getelementptr i8, i8* %thread_ptr, i64 -8
  %3 = bitcast i8* %tls_ppgcstack to {}****
[... snip ...]
```

Let's time the two:

```{julia}
using BenchmarkTools
y = 3;
@btime squaresum_with_global(4);
@btime squaresum(4, 3);
```

That's a very simple function, so it's very fast on an *absolute* basis even with the global,
but clearly very slow *relative* to the non-global version. That  makes sense given the additional code in the  LLVM code for the global version.


### Helping out the compiler

Now let's see that using a `const` or typing the global variable avoids the problem.

```{julia}
function squaresum_with_const_global(x)
  return x^2 + z^2
end

const z = 3;
@code_llvm squaresum_with_const_global(4)
```
```{julia}
@btime squaresum_with_const_global(4);
```

sing the constant brings the speed back to the version without a global variable.
We can see in the LLVM version of the code that $z^2$ has been computed and is hard-coded into the code. So it makes sense that it's just as fast as the non-global version (in fact we'd expect it to be faster as there is only one squaring operation).

Here we add a type declaration to the global.

```{julia}

w::Int64 = 3
function squaresum_with_typed_global(x)
  return x^2 + w^2
end

@code_llvm squaresum_with_typed_global(4)
```
```{julia}
@btime squaresum_with_typed_global(4);
```

The typed global avoids the performance issue as well. (Mostly --it is a bit slower, presumably because of the retrieval of the global variable.)

Of course from the perspective of code style, avoiding global variables is generally a good idea, even if we're able in this case to avoid a performance problem.

Side note: when I was exploring this, if I change the  type of the global variable, it doesn't seem to trigger compilation or change the compiled code. So there is something I'm not understanding.


## JIT: intermediate representations

Let's look at the various representations along the Julia compilation process.

```{julia}
# using InteractiveUtils
@code_lowered squaresum(3.0, 4.0)
```
```{julia}
@code_typed squaresum(3.0, 4.0)
```
```{julia}
@code_llvm squaresum(3.0, 4.0)
```

### Assembly and machine code

Here's the assembly ('native') code, which has a very close relationship with the actual binary machine code.

```{julia}
@code_native squaresum(3.0, 4.0)
```

And here's the "binary" version, though it seems like it's still showing a version of the assembly code but with some binary annotation.

```{julia}
code_native(squaresum, binary=true)
```
