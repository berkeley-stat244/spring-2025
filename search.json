[
  {
    "objectID": "presentations/jit.html",
    "href": "presentations/jit.html",
    "title": "JIT (Just-In-Time) Compilation",
    "section": "",
    "text": "This document is an extension of Notes 5 and 6 and will focus on how you can help the JIT compiler optimize your code.\nWe talked before about how Julia runs code as illustrated in the following flowchart:\n\n\n\nJulia compiler steps (courtesy of the Julia manual)"
  },
  {
    "objectID": "presentations/jit.html#introduction",
    "href": "presentations/jit.html#introduction",
    "title": "JIT (Just-In-Time) Compilation",
    "section": "",
    "text": "This document is an extension of Notes 5 and 6 and will focus on how you can help the JIT compiler optimize your code.\nWe talked before about how Julia runs code as illustrated in the following flowchart:\n\n\n\nJulia compiler steps (courtesy of the Julia manual)"
  },
  {
    "objectID": "presentations/jit.html#jit-compilation-process",
    "href": "presentations/jit.html#jit-compilation-process",
    "title": "JIT (Just-In-Time) Compilation",
    "section": "JIT Compilation Process",
    "text": "JIT Compilation Process\n\nType inference\nJulia uses a complex algorithm to deduce output types from input types. At a high-level, it involves representing the code flow graph as a lattice with some modifications, then running operations on the lattice to determine the types of variables.\n\n\nSSA (static single-assignment) conversion\n\nSSA form\nIn SSA, each variable is assigned exactly once. This allows for easier optimization further down the pipeline because the compiler can reason about the flow of data more easily. For example, here is one piece of code before and after SSA:\ny = 1\ny = 2\nx = y\nA human can easily see that the first assignment to y is not needed, but it is more complicated for a machine. In SSA form, the code would look like this:\ny1 = 1\ny2 = 2\nx1 = y2\nIn this form, it is clear that the first assignment to y is not needed, since y1 is never used.\nFrom the manual:\nJulia uses a static single assignment intermediate representation (SSA IR) to perform optimization. This IR is different from LLVM IR, and unique to Julia. It allows for Julia specific optimizations.\n\nBasic blocks (regions with no control flow) are explicitly annotated.\nif/else and loops are turned into goto statements.\nlines with multiple operations are split into multiple lines by introducing variables.\n\n\nfunction foo(x)\n    y = sin(x)\n    if x &gt; 5.0\n        y = y + cos(x)\n    end\n    return exp(2) + y\nend;\n\nusing InteractiveUtils\n@code_typed foo(1.0)\n\nCodeInfo(\n1 ─ %1 = invoke Main.sin(x::Float64)::Float64\n│   %2 = Base.lt_float(5.0, x)::Bool\n└──      goto #3 if not %2\n2 ─ %4 = invoke Main.cos(x::Float64)::Float64\n└── %5 = Base.add_float(%1, %4)::Float64\n3 ┄ %6 = φ (#2 =&gt; %5, #1 =&gt; %1)::Float64\n│   %7 = Base.add_float(7.38905609893065, %6)::Float64\n└──      return %7\n) =&gt; Float64\n\n\nThere are four different categories of IR “nodes” that get generated from the AST, and allow for a Julia-specific SSA-IR data structure.\n\n\n\nOptimization passes\nThe optimization pipeline is a complicated process that involves many steps and can be read about in detail here. The main steps are:\n\nEarly Simplification\n\nSimplify IR. Branch prediction hints, simplify control flow, dead code elimination, …\n\nEarly Optimization\n\nReduce number of instructions. Common subexpression elimination, …\n\nLoop Optimization\n\nCanonicalize and simplify loops. Loop fusion, loop unrolling, loop interchange, …\n\nScalar Optimization\n\nMore expensive optimization passes. Global value numbering, proving branches never taken, …\n\nVectorization\n\nVectorize. Earlier passes make this easier and reduce overhead in this step.\n\nIntrinsic Lowering\n\nCustom intrinsics. Exception handling, garbage collection, …\n\nCleanup\n\nLast-chance small optimizations. Fused multiply-add, …\n\n\n\n\nExamples\nHere are some examples of the techniques the optimization pipeline employs. There are many, many more, but these are some of the common ones as mentioned in the Julia docs:\n\nDead code elimination (DCE): This optimization pass removes code that is never executed.\n\nThe conditional block is never executed, so the code inside can be removed:\nfunction foo(x)\n    if false\n        x += 1\n    end\n    return x\nend\n\nConstant propagation: This optimization pass replaces variables with their constant values.\n\nThe following function can be simplified to return 5:\nfunction foo(x)\n    x = 3\n    y = 2\n    return x + y\nend\n\nCommon subexpression elimination (CSE): This optimization pass eliminates redundant computations.\n\nThe following function may be compiled to store the value of x^2 in a temporary variable and reuse it:\nfunction foo(x)\n    return x^2 + x^2 + x^2 + x^2\nend\n\nLoop unrolling: Loops traditionally have a condition that needs to be checked at every iteration. If the number of iterations is known at compile time, the loop can be unrolled to remove the condition check.\n\nThe following loop may be unrolled to remove the condition check:\na = 0\nfor i in 1:4\n    a += i\nend\na = 0\na += 1\na += 2\na += 3\na += 4\n\nLoop fusion: This optimization pass combines multiple loops into one to reduce the number of iterations.\n\nThe following two loops may be fused into one:\na = 0\nb = 0\nfor i in 1:4\n    a += i\nend\nfor j in 1:4\n    b += j\nend\na = 0\nb = 0\nfor i in 1:4\n    a += i\n    b += j\nend\n\nLoop interchange: This optimization pass changes the order of nested loops to improve cache performance.\n\nThe following nested loops may be interchanged to improve cache performance:\nfor i in 1:4\n    for j in 1:4\n        a[i, j] = i + j\n    end\nend\nfor j in 1:4\n    for i in 1:4\n        a[i, j] = i + j\n    end\nend\n\nGlobal value numbering (GVN): This optimization pass assigns a unique number to each value computed by the program and replaces the value with its number.\n\nAfter GVN, the following code can likely be optimized further by CSE (x and z can be replaced with w and y everywhere):\nw = 3\nx = 3\ny = x + 4\nz = w + 4\nw := 3\nx := w\ny := w + 4\nz := y\n\nFused multiply-add (FMA): This optimization combines multiplication and addition instruction into a single instruction if the hardware supports it.\n\nThe following code may be compiled to use an FMA instruction:\nmul r1, r2, r3; multiply r2 and r3 and store in r1\nadd r4, r1, r5; add r1 and r5 and store in r4\n; this process is done in a single instruction\nfmadd r4, r2, r3, r5; multiply r2 and r3, add r5, and store in r4"
  },
  {
    "objectID": "presentations/jit.html#techniques-to-help-the-jit-compiler",
    "href": "presentations/jit.html#techniques-to-help-the-jit-compiler",
    "title": "JIT (Just-In-Time) Compilation",
    "section": "Techniques to help the JIT compiler",
    "text": "Techniques to help the JIT compiler\nWe talked last week about some techniques you can use to help the JIT compiler optimize your code, such as putting performance-critical code inside functions, avoiding global variables, typing your variables, and using the const keyword. Here are some more techniques, and you can read about many more in detail here:\nThere are many good tips recommended by the manual, but here are a few that I think are quite useful or surprising. Most of these boil down to type stability:\n\nWrite type-stable code\nThis code looks innocuous enough, but there is something wrong with it:\n\npos(x) = x &lt; 0 ? 0 : x;\n\n# This is equivalent to\nfunction pos(x)\n    if x &lt; 0\n        return 0\n    else\n        return x\n    end\nend;\n\n0 is an integer, but x can be any type. This function is not type-stable because the return type depends on the input type. One may use the zero function to make this type-stable:\n\npos(x) = x &lt; zero(x) ? zero(x) : x;\n\nSimilar functions exist for oneunit, typemin, and typemax.\nA similar type-stability issue may also arise when using operations that may change the type of a variable such as /:\n\nfunction foo(n)\n    x = 1\n    for i = 1:n\n        x /= rand()\n    end\n    return x\nend;\n\nThe manual outlines several possible fixes:\n\nInitialize x with x = 1.0\nDeclare the type of x explicitly as x::Float64 = 1\nUse an explicit conversion by x = oneunit(Float64)\nInitialize with the first loop iteration, to x = 1 / rand(), then loop for i = 2:10\n\n\n\nBe wary of memory allocations\n\n\n\nMemory allocation diagram from CS61C\n\n\nHeap memory allocation can be a bottleneck in your code. If you are allocating memory in a loop, you may be slowing down your code. Here is a toy code segment that repeatedly allocates memory:\n\nfunction xinc(x)\n    return [x, x+1, x+2]\nend;\n\nfunction loopinc()\n    y = 0\n    for i = 1:10^7\n        ret = xinc(i)\n        y += ret[2]\n    end\n    return y\nend;\n\nThis code, while unrealistic, is a good example of how memory allocation can slow down your code. The xinc function allocates memory every time it is called, and the loopinc function calls xinc several times. This code can be optimized by preallocating memory:\n\nfunction xinc!(ret, x)\n    ret[1] = x\n    ret[2] = x+1\n    ret[3] = x+2\nend;\n\nfunction loopinc_prealloc()\n    y = 0\n    ret = [0, 0, 0]\n    for i = 1:10^7\n        xinc!(ret, i)\n        y += ret[2]\n    end\n    return y\nend;\n\n\n@time loopinc()\n\n  0.405860 seconds (10.00 M allocations: 762.939 MiB, 7.85% gc time)\n\n\n50000015000000\n\n@time loopinc_prealloc()\n\n  0.003780 seconds (1 allocation: 80 bytes)\n\n\n50000015000000\n\n\n\n\nBe wary of memory allocations, again\n\nx = rand(1000);\nfunction sum_global()\n    s = 0.0\n    for i in x\n        s += i\n    end\n    return s\nend;\n@time sum_global()\n\n  0.010564 seconds (3.68 k allocations: 78.109 KiB, 98.37% compilation time)\n\n\n493.0660016458528\n\n\n\nfunction sum_local()\n    s = 0.0\n    x = rand(1000)\n    for i in x\n        s += i\n    end\n    return s\nend;\n@time sum_local()\n\n  0.000018 seconds (1 allocation: 7.938 KiB)\n\n\n500.14039588612354\n\n\nThe global nature of x prevents the compiler from making many optimizations, especially since it is not typed. Because it is global, and x needs to persist, it requires heap memory allocation. The local version of x is typed and stack-allocated, which allows the compiler to optimize the code better. Stack allocation is usually much faster than heap allocation.\n\n\nFunction barriers\nTry to separate functionality into different functions as much as possible. Often there is some setup, work, and cleanup to be done - it is a good idea to separate these into different functions. This will help with compiler optimizations, but it often makes the code more readable and reusable.\nConsider the following (strange) code. a will be an array of Int64s or Float64s, depending on the random value, but it can only be determined at runtime. Though this is a contrived example, sometimes there are legitimate cases where things cannot be determined until runtime.\n\nfunction strange_twos(n)\n    a = Vector{rand(Bool) ? Int64 : Float64}(undef, n)\n    for i = 1:n\n        a[i] = 2\n    end\n    return a\nend;\n@time strange_twos(10^6)\n\n  0.058851 seconds (999.54 k allocations: 22.883 MiB, 11.64% gc time, 12.85% compilation time)\n\n\n1000000-element Vector{Float64}:\n 2.0\n 2.0\n 2.0\n 2.0\n 2.0\n 2.0\n 2.0\n 2.0\n 2.0\n 2.0\n ⋮\n 2.0\n 2.0\n 2.0\n 2.0\n 2.0\n 2.0\n 2.0\n 2.0\n 2.0\n\n\nInstead, separating out the type determination into a different function can help the compiler:\n\nfunction fill_twos!(a)\n    for i = eachindex(a)\n        a[i] = 2\n    end\nend;\n\nfunction strange_twos_better(n)\n    a = Vector{rand(Bool) ? Int64 : Float64}(undef, n)\n    fill_twos!(a)\n    return a\nend;\n\n@time strange_twos_better(10^6)\n\n  0.015667 seconds (1.88 k allocations: 7.752 MiB, 84.70% compilation time)\n\n\n1000000-element Vector{Int64}:\n 2\n 2\n 2\n 2\n 2\n 2\n 2\n 2\n 2\n 2\n ⋮\n 2\n 2\n 2\n 2\n 2\n 2\n 2\n 2\n 2"
  },
  {
    "objectID": "presentations/jit.html#loopvectorization.turbo",
    "href": "presentations/jit.html#loopvectorization.turbo",
    "title": "JIT (Just-In-Time) Compilation",
    "section": "LoopVectorization.@turbo",
    "text": "LoopVectorization.@turbo\nHere is some Julia code for a naive matrix multiplication algorithm:\n\nfunction AmulB!(C, A, B)\n    for m ∈ axes(A, 1), n ∈ axes(B, 2)\n        Cₘₙ = zero(eltype(C)) # element type\n        for k ∈ axes(A, 2)\n            Cₘₙ += A[m,k] * B[k,n]\n        end\n        C[m,n] = Cₘₙ\n    end\nend;\n\n@time AmulB!(rand(1000,1000), rand(1000,1000), rand(1000,1000))\n\n  1.430879 seconds (6 allocations: 22.888 MiB, 0.61% gc time)\n\n\nThe LoopVectorization.jl package offers the @turbo macro, which optimizes loops using memory-efficient SIMD (vectorized) instructions. However, one can only apply this to loops that meet certain conditions as outlined in the package README.\n\nusing LoopVectorization;\n\nfunction AmulB_turbo!(C, A, B)\n    @turbo for m ∈ indices((A,C), 1), n ∈ indices((B,C), 2) # indices((A,C),1) == axes(A,1) == axes(C,1)\n        Cₘₙ = zero(eltype(C))\n        for k ∈ indices((A,B), (2,1)) # indices((A,B), (2,1)) == axes(A,2) == axes(B,1)\n            Cₘₙ += A[m,k] * B[k,n]\n        end\n        C[m,n] = Cₘₙ\n    end\nend;\n\n@time AmulB_turbo!(rand(1000,1000), rand(1000,1000), rand(1000,1000))\n\n  0.306820 seconds (6 allocations: 22.888 MiB, 24.73% gc time)\n\n\nFor comparison, here is BLAS matrix multiplication:\n\nusing LinearAlgebra;\nBLAS.set_num_threads(1);\n\nfunction BLAS_mul(C, A, B)\n    BLAS.gemm!('N', 'N', 1.0, A, B, 0.0, C)\nend;\n\n@time BLAS_mul(rand(1000,1000), rand(1000,1000), rand(1000,1000))\n\n  0.053857 seconds (6 allocations: 22.888 MiB)\n\n\n1000×1000 Matrix{Float64}:\n 257.347  250.699  259.414  253.959  …  254.238  244.401  252.12   253.333\n 251.841  244.416  256.263  253.47      249.079  247.258  243.135  251.561\n 254.138  238.289  254.656  253.811     253.318  252.332  253.364  253.145\n 251.22   247.39   256.726  252.656     257.637  244.648  245.677  244.167\n 258.11   244.375  257.429  256.45      256.245  246.194  248.168  253.504\n 246.412  236.408  246.487  249.249  …  248.086  237.045  238.311  239.815\n 262.37   252.564  265.677  260.038     259.599  255.904  260.195  258.454\n 255.148  245.884  255.709  255.605     253.4    240.644  248.818  247.677\n 256.731  255.629  259.755  257.053     265.486  254.591  252.26   256.328\n 253.182  246.763  254.289  252.019     252.003  243.137  243.963  251.144\n   ⋮                                 ⋱                             \n 254.807  242.887  253.561  253.259     253.612  245.988  252.223  247.241\n 247.035  242.627  249.837  249.149     253.356  242.434  244.824  251.413\n 248.386  245.498  249.989  251.713     253.862  246.192  251.703  245.239\n 252.463  249.915  255.904  250.251     252.322  250.311  250.289  250.606\n 249.883  249.381  257.565  256.184  …  254.819  247.749  247.745  248.4\n 245.933  239.043  249.694  245.766     246.747  241.968  242.75   245.631\n 249.824  246.622  257.6    251.993     253.216  246.655  247.832  247.114\n 253.707  248.121  253.528  251.932     254.092  246.517  247.045  251.519\n 260.6    255.655  261.687  259.607     263.396  254.516  253.826  256.438"
  },
  {
    "objectID": "notes/cuda-samples/CHANGELOG.html",
    "href": "notes/cuda-samples/CHANGELOG.html",
    "title": "",
    "section": "",
    "text": "Added graphConditionalNodes Sample\n\n\n\n\n\nAdded cuDLA samples\nFixed jitLto regression\n\n\n\n\n\nlibNVVM samples received updates\nFixed jitLto Case issues\nEnabled HOST_COMPILER flag to the makefiles for GCC which is untested but may still work.\n\n\n\n\n\nAdded new sample for Large Kernels\n\n\n\n\n\nAdded new flags for JIT compiling\nRemoved deprecated APIs in Hopper Architecture\n\n\n\n\n\nAdded new folder structure for samples\nAdded support of Visual Studio 2022 to all samples supported on Windows.\nAll CUDA samples are now only available on GitHub. They are no longer available via CUDA toolkit.\n\n\n\n\n\nAdded cuDLAHybridMode. Demonstrate usage of cuDLA in hybrid mode.\nAdded cuDLAStandaloneMode. Demonstrate usage of cuDLA in standalone mode.\nAdded cuDLAErrorReporting. Demonstrate DLA error detection via CUDA.\nAdded graphMemoryNodes. Demonstrates memory allocations and frees within CUDA graphs using Graph APIs and Stream Capture APIs.\nAdded graphMemoryFootprint. Demonstrates how graph memory nodes re-use virtual addresses and physical memory.\nAll samples from CUDA toolkit are now available on GitHub.\n\n\n\n\n\nAdded support for VS Code on linux platform.\n\n\n\n\n\nAdded cdpQuadtree. Demonstrates Quad Trees implementation using CUDA Dynamic Parallelism.\nUpdated simpleVulkan, simpleVulkanMMAP and vulkanImageCUDA. Demonstrates use of SPIR-V shaders.\n\n\n\n\n\nAdded streamOrderedAllocationIPC. Demonstrates Inter Process Communication using one process per GPU for computation.\nAdded simpleCUBLAS_LU. Demonstrates batched matrix LU decomposition using cuBLAS API cublas&lt;t&gt;getrfBatched()\nUpdated simpleVulkan. Demonstrates use of timeline semaphore.\nUpdated multiple samples to use pinned memory using cudaMallocHost().\n\n\n\n\n\nAdded streamOrderedAllocation. Demonstrates stream ordered memory allocation on a GPU using cudaMallocAsync and cudaMemPool family of APIs.\nAdded streamOrderedAllocationP2P. Demonstrates peer-to-peer access of stream ordered memory allocated using cudaMallocAsync and cudaMemPool family of APIs.\nDropped Visual Studio 2015 support from all the windows supported samples.\nFreeImage is no longer distributed with the CUDA Samples. On Windows, see the Dependencies section for more details on how to set up FreeImage. On Linux, it is recommended to install FreeImage with your distribution’s package manager.\nAll the samples using CUDA Pipeline & Arrive-wait barriers are been updated to use new cuda::pipeline and cuda::barrier interfaces.\nUpdated all the samples to build with parallel build option --threads of nvcc cuda compiler.\nAdded cudaNvSciNvMedia. Demonstrates CUDA-NvMedia interop via NvSciBuf/NvSciSync APIs.\nAdded simpleGL. Demonstrates interoperability between CUDA and OpenGL.\n\n\n\n\n\nAdded watershedSegmentationNPP. Demonstrates how to use the NPP watershed segmentation function.\nAdded batchedLabelMarkersAndLabelCompressionNPP. Demonstrates how to use the NPP label markers generation and label compression functions based on a Union Find (UF) algorithm including both single image and batched image versions.\nDropped Visual Studio 2012, 2013 support from all the windows supported samples.\nAdded kernel performing warp aggregated atomic max in multi buckets using cg::labeled_partition & cg::reduce in warpAggregatedAtomicsCG.\nAdded extended CG shuffle mechanics to shfl_scan sample.\nAdded cudaOpenMP. Demonstrates how to use OpenMP API to write an application for multiple GPUs.\nAdded simpleZeroCopy. Demonstrates how to use zero copy, kernels can read and write directly to pinned system memory.\n\n\n\n\n\nAdded dmmaTensorCoreGemm. Demonstrates double precision GEMM computation using the Double precision Warp Matrix Multiply and Accumulate (WMMA) API introduced with CUDA 11 in Ampere chip family tensor cores.\nAdded bf16TensorCoreGemm. Demonstrates __nv_bfloat16 (e8m7) GEMM computation using the __nv_bfloat16 WMMA API introduced with CUDA 11 in Ampere chip family tensor cores.\nAdded tf32TensorCoreGemm. Demonstrates tf32 (e8m10) GEMM computation using the tf32 WMMA API introduced with CUDA 11 in Ampere chip family tensor cores.\nAdded globalToShmemAsyncCopy. Demonstrates async copy of data from global to shared memory when on compute capability 8.0 or higher. Also demonstrates arrive-wait barrier for synchronization.\nAdded simpleAWBarrier. Demonstrates arrive wait barriers.\nAdded simpleAttributes. Demonstrates the stream attributes that affect L2 locality.\nAdded warp aggregated atomic multi bucket increments kernel using labeled_partition cooperative groups in warpAggregatedAtomicsCG which can be used on compute capability 7.0 and above GPU architectures.\nAdded binaryPartitionCG. Demonstrates binary partition cooperative groups and reduction within the thread block.\nAdded two new reduction kernels in reduction one which demonstrates reduce_add_sync intrinstic supported on compute capability 8.0 and another which uses cooperative_groups::reduce function which does thread_block_tile level reduction introduced from CUDA 11.0.\nAdded cudaCompressibleMemory. Demonstrates compressible memory allocation using cuMemMap API.\nAdded simpleVulkanMMAP. Demonstrates Vulkan CUDA Interop via cuMemMap APIs.\nAdded concurrentKernels. Demonstrates the use of CUDA streams for concurrent execution of several kernels on a GPU.\nDropped Mac OSX support from all samples.\n\n\n\n\n\nAdded simpleD3D11. Demonstrates CUDA-D3D11 External Resource Interoperability APIs for updating D3D11 buffers from CUDA and synchronization between D3D11 and CUDA with Keyed Mutexes.\nAdded simpleDrvRuntime. Demonstrates CUDA Driver and Runtime APIs working together to load fatbinary of a CUDA kernel.\nAdded vectorAddMMAP. Demonstrates how cuMemMap API allows the user to specify the physical properties of their memory while retaining the contiguous nature of their access.\nAdded memMapIPCDrv. Demonstrates Inter Process Communication using cuMemMap APIs.\nAdded cudaNvSci. Demonstrates CUDA-NvSciBuf/NvSciSync Interop.\nAdded jacobiCudaGraphs. Demonstrates Instantiated CUDA Graph Update with Jacobi Iterative Method using different approaches.\nAdded cuSolverSp_LinearSolver. Demonstrates cuSolverSP’s LU, QR and Cholesky factorization.\nAdded MersenneTwisterGP11213. Demonstrates the Mersenne Twister random number generator GP11213 in cuRAND.\n\n\n\n\n\nAdded vulkanImageCUDA. Demonstrates how to perform Vulkan image - CUDA Interop.\nAdded nvJPEG_encoder. Demonstrates encoding of jpeg images using NVJPEG Library.\nAdded Windows OS support to nvJPEG sample.\nAdded boxFilterNPP. Demonstrates how to use NPP FilterBox function to perform a box filter.\nAdded cannyEdgeDetectorNPP. Demonstrates the nppiFilterCannyBorder_8u_C1R Canny Edge Detection image filter function.\n\n\n\n\n\nAdded NV12toBGRandResize. Demonstrates how to convert and resize NV12 frames to BGR planars frames using CUDA in batch.\nAdded EGLStream_CUDA_Interop. Demonstrates data exchange between CUDA and EGL Streams.\nAdded cuSolverDn_LinearSolver. Demonstrates cuSolverDN’s LU, QR and Cholesky factorization.\nAdded support of Visual Studio 2019 to all samples supported on Windows.\n\n\n\n\n\nAdded immaTensorCoreGemm. Demonstrates integer GEMM computation using the Warp Matrix Multiply and Accumulate (WMMA) API for integers employing the Tensor Cores.\nAdded simpleIPC. Demonstrates Inter Process Communication with one process per GPU for computation.\nAdded nvJPEG. Demonstrates single and batched decoding of jpeg images using NVJPEG Library.\nAdded bandwidthTest. It measures the memcopy bandwidth of the GPU and memcpy bandwidth across PCI-e.\nAdded reduction. Demonstrates several important optimization strategies for Data-Parallel Algorithms like reduction.\nUpdate all the samples to support CUDA 10.1.\n\n\n\n\n\nAdded simpleCudaGraphs. Demonstrates CUDA Graphs creation, instantiation and launch using Graphs APIs and Stream Capture APIs.\nAdded conjugateGradientCudaGraphs. Demonstrates conjugate gradient solver on GPU using CUBLAS and CUSPARSE library calls captured and called using CUDA Graph APIs.\nAdded simpleVulkan. Demonstrates Vulkan - CUDA Interop.\nAdded simpleD3D12. Demonstrates DX12 - CUDA Interop.\nAdded UnifiedMemoryPerf. Demonstrates performance comparision of various memory types involved in system.\nAdded p2pBandwidthLatencyTest. Demonstrates Peer-To-Peer (P2P) data transfers between pairs of GPUs and computes latency and bandwidth.\nAdded systemWideAtomics. Demonstrates system wide atomic instructions.\nAdded simpleCUBLASXT. Demonstrates CUBLAS-XT library which performs GEMM operations over multiple GPUs.\nAdded Windows OS support to conjugateGradientMultiDeviceCG sample.\nRemoved support of Visual Studio 2010 from all samples.\n\n\n\n\nThis is the first release of CUDA Samples on GitHub: * Added vectorAdd_nvrtc. Demonstrates runtime compilation library using NVRTC of a simple vectorAdd kernel. * Added warpAggregatedAtomicsCG. Demonstrates warp aggregated atomics using Cooperative Groups. * Added deviceQuery. Enumerates the properties of the CUDA devices present in the system. * Added matrixMul. Demonstrates a matrix multiplication using shared memory through tiled approach. * Added matrixMulDrv. Demonstrates a matrix multiplication using shared memory through tiled approach, uses CUDA Driver API. * Added cudaTensorCoreGemm. Demonstrates a GEMM computation using the Warp Matrix Multiply and Accumulate (WMMA) API introduced in CUDA 9, as well as the new Tensor Cores introduced in the Volta chip family. * Added simpleVoteIntrinsics which uses *_sync equivalent of the vote intrinsics _any, _all added since CUDA 9.0. * Added shfl_scan which uses *_sync equivalent of the shfl intrinsics added since CUDA 9.0. * Added conjugateGradientMultiBlockCG. Demonstrates a conjugate gradient solver on GPU using Multi Block Cooperative Groups. * Added conjugateGradientMultiDeviceCG. Demonstrates a conjugate gradient solver on multiple GPUs using Multi Device Cooperative Groups, also uses unified memory prefetching and usage hints APIs. * Added simpleCUBLAS. Demonstrates how perform GEMM operations using CUBLAS library. * Added simpleCUFFT. Demonstrates how perform FFT operations using CUFFT library."
  },
  {
    "objectID": "notes/cuda-samples/CHANGELOG.html#changelog",
    "href": "notes/cuda-samples/CHANGELOG.html#changelog",
    "title": "",
    "section": "",
    "text": "Added graphConditionalNodes Sample\n\n\n\n\n\nAdded cuDLA samples\nFixed jitLto regression\n\n\n\n\n\nlibNVVM samples received updates\nFixed jitLto Case issues\nEnabled HOST_COMPILER flag to the makefiles for GCC which is untested but may still work.\n\n\n\n\n\nAdded new sample for Large Kernels\n\n\n\n\n\nAdded new flags for JIT compiling\nRemoved deprecated APIs in Hopper Architecture\n\n\n\n\n\nAdded new folder structure for samples\nAdded support of Visual Studio 2022 to all samples supported on Windows.\nAll CUDA samples are now only available on GitHub. They are no longer available via CUDA toolkit.\n\n\n\n\n\nAdded cuDLAHybridMode. Demonstrate usage of cuDLA in hybrid mode.\nAdded cuDLAStandaloneMode. Demonstrate usage of cuDLA in standalone mode.\nAdded cuDLAErrorReporting. Demonstrate DLA error detection via CUDA.\nAdded graphMemoryNodes. Demonstrates memory allocations and frees within CUDA graphs using Graph APIs and Stream Capture APIs.\nAdded graphMemoryFootprint. Demonstrates how graph memory nodes re-use virtual addresses and physical memory.\nAll samples from CUDA toolkit are now available on GitHub.\n\n\n\n\n\nAdded support for VS Code on linux platform.\n\n\n\n\n\nAdded cdpQuadtree. Demonstrates Quad Trees implementation using CUDA Dynamic Parallelism.\nUpdated simpleVulkan, simpleVulkanMMAP and vulkanImageCUDA. Demonstrates use of SPIR-V shaders.\n\n\n\n\n\nAdded streamOrderedAllocationIPC. Demonstrates Inter Process Communication using one process per GPU for computation.\nAdded simpleCUBLAS_LU. Demonstrates batched matrix LU decomposition using cuBLAS API cublas&lt;t&gt;getrfBatched()\nUpdated simpleVulkan. Demonstrates use of timeline semaphore.\nUpdated multiple samples to use pinned memory using cudaMallocHost().\n\n\n\n\n\nAdded streamOrderedAllocation. Demonstrates stream ordered memory allocation on a GPU using cudaMallocAsync and cudaMemPool family of APIs.\nAdded streamOrderedAllocationP2P. Demonstrates peer-to-peer access of stream ordered memory allocated using cudaMallocAsync and cudaMemPool family of APIs.\nDropped Visual Studio 2015 support from all the windows supported samples.\nFreeImage is no longer distributed with the CUDA Samples. On Windows, see the Dependencies section for more details on how to set up FreeImage. On Linux, it is recommended to install FreeImage with your distribution’s package manager.\nAll the samples using CUDA Pipeline & Arrive-wait barriers are been updated to use new cuda::pipeline and cuda::barrier interfaces.\nUpdated all the samples to build with parallel build option --threads of nvcc cuda compiler.\nAdded cudaNvSciNvMedia. Demonstrates CUDA-NvMedia interop via NvSciBuf/NvSciSync APIs.\nAdded simpleGL. Demonstrates interoperability between CUDA and OpenGL.\n\n\n\n\n\nAdded watershedSegmentationNPP. Demonstrates how to use the NPP watershed segmentation function.\nAdded batchedLabelMarkersAndLabelCompressionNPP. Demonstrates how to use the NPP label markers generation and label compression functions based on a Union Find (UF) algorithm including both single image and batched image versions.\nDropped Visual Studio 2012, 2013 support from all the windows supported samples.\nAdded kernel performing warp aggregated atomic max in multi buckets using cg::labeled_partition & cg::reduce in warpAggregatedAtomicsCG.\nAdded extended CG shuffle mechanics to shfl_scan sample.\nAdded cudaOpenMP. Demonstrates how to use OpenMP API to write an application for multiple GPUs.\nAdded simpleZeroCopy. Demonstrates how to use zero copy, kernels can read and write directly to pinned system memory.\n\n\n\n\n\nAdded dmmaTensorCoreGemm. Demonstrates double precision GEMM computation using the Double precision Warp Matrix Multiply and Accumulate (WMMA) API introduced with CUDA 11 in Ampere chip family tensor cores.\nAdded bf16TensorCoreGemm. Demonstrates __nv_bfloat16 (e8m7) GEMM computation using the __nv_bfloat16 WMMA API introduced with CUDA 11 in Ampere chip family tensor cores.\nAdded tf32TensorCoreGemm. Demonstrates tf32 (e8m10) GEMM computation using the tf32 WMMA API introduced with CUDA 11 in Ampere chip family tensor cores.\nAdded globalToShmemAsyncCopy. Demonstrates async copy of data from global to shared memory when on compute capability 8.0 or higher. Also demonstrates arrive-wait barrier for synchronization.\nAdded simpleAWBarrier. Demonstrates arrive wait barriers.\nAdded simpleAttributes. Demonstrates the stream attributes that affect L2 locality.\nAdded warp aggregated atomic multi bucket increments kernel using labeled_partition cooperative groups in warpAggregatedAtomicsCG which can be used on compute capability 7.0 and above GPU architectures.\nAdded binaryPartitionCG. Demonstrates binary partition cooperative groups and reduction within the thread block.\nAdded two new reduction kernels in reduction one which demonstrates reduce_add_sync intrinstic supported on compute capability 8.0 and another which uses cooperative_groups::reduce function which does thread_block_tile level reduction introduced from CUDA 11.0.\nAdded cudaCompressibleMemory. Demonstrates compressible memory allocation using cuMemMap API.\nAdded simpleVulkanMMAP. Demonstrates Vulkan CUDA Interop via cuMemMap APIs.\nAdded concurrentKernels. Demonstrates the use of CUDA streams for concurrent execution of several kernels on a GPU.\nDropped Mac OSX support from all samples.\n\n\n\n\n\nAdded simpleD3D11. Demonstrates CUDA-D3D11 External Resource Interoperability APIs for updating D3D11 buffers from CUDA and synchronization between D3D11 and CUDA with Keyed Mutexes.\nAdded simpleDrvRuntime. Demonstrates CUDA Driver and Runtime APIs working together to load fatbinary of a CUDA kernel.\nAdded vectorAddMMAP. Demonstrates how cuMemMap API allows the user to specify the physical properties of their memory while retaining the contiguous nature of their access.\nAdded memMapIPCDrv. Demonstrates Inter Process Communication using cuMemMap APIs.\nAdded cudaNvSci. Demonstrates CUDA-NvSciBuf/NvSciSync Interop.\nAdded jacobiCudaGraphs. Demonstrates Instantiated CUDA Graph Update with Jacobi Iterative Method using different approaches.\nAdded cuSolverSp_LinearSolver. Demonstrates cuSolverSP’s LU, QR and Cholesky factorization.\nAdded MersenneTwisterGP11213. Demonstrates the Mersenne Twister random number generator GP11213 in cuRAND.\n\n\n\n\n\nAdded vulkanImageCUDA. Demonstrates how to perform Vulkan image - CUDA Interop.\nAdded nvJPEG_encoder. Demonstrates encoding of jpeg images using NVJPEG Library.\nAdded Windows OS support to nvJPEG sample.\nAdded boxFilterNPP. Demonstrates how to use NPP FilterBox function to perform a box filter.\nAdded cannyEdgeDetectorNPP. Demonstrates the nppiFilterCannyBorder_8u_C1R Canny Edge Detection image filter function.\n\n\n\n\n\nAdded NV12toBGRandResize. Demonstrates how to convert and resize NV12 frames to BGR planars frames using CUDA in batch.\nAdded EGLStream_CUDA_Interop. Demonstrates data exchange between CUDA and EGL Streams.\nAdded cuSolverDn_LinearSolver. Demonstrates cuSolverDN’s LU, QR and Cholesky factorization.\nAdded support of Visual Studio 2019 to all samples supported on Windows.\n\n\n\n\n\nAdded immaTensorCoreGemm. Demonstrates integer GEMM computation using the Warp Matrix Multiply and Accumulate (WMMA) API for integers employing the Tensor Cores.\nAdded simpleIPC. Demonstrates Inter Process Communication with one process per GPU for computation.\nAdded nvJPEG. Demonstrates single and batched decoding of jpeg images using NVJPEG Library.\nAdded bandwidthTest. It measures the memcopy bandwidth of the GPU and memcpy bandwidth across PCI-e.\nAdded reduction. Demonstrates several important optimization strategies for Data-Parallel Algorithms like reduction.\nUpdate all the samples to support CUDA 10.1.\n\n\n\n\n\nAdded simpleCudaGraphs. Demonstrates CUDA Graphs creation, instantiation and launch using Graphs APIs and Stream Capture APIs.\nAdded conjugateGradientCudaGraphs. Demonstrates conjugate gradient solver on GPU using CUBLAS and CUSPARSE library calls captured and called using CUDA Graph APIs.\nAdded simpleVulkan. Demonstrates Vulkan - CUDA Interop.\nAdded simpleD3D12. Demonstrates DX12 - CUDA Interop.\nAdded UnifiedMemoryPerf. Demonstrates performance comparision of various memory types involved in system.\nAdded p2pBandwidthLatencyTest. Demonstrates Peer-To-Peer (P2P) data transfers between pairs of GPUs and computes latency and bandwidth.\nAdded systemWideAtomics. Demonstrates system wide atomic instructions.\nAdded simpleCUBLASXT. Demonstrates CUBLAS-XT library which performs GEMM operations over multiple GPUs.\nAdded Windows OS support to conjugateGradientMultiDeviceCG sample.\nRemoved support of Visual Studio 2010 from all samples.\n\n\n\n\nThis is the first release of CUDA Samples on GitHub: * Added vectorAdd_nvrtc. Demonstrates runtime compilation library using NVRTC of a simple vectorAdd kernel. * Added warpAggregatedAtomicsCG. Demonstrates warp aggregated atomics using Cooperative Groups. * Added deviceQuery. Enumerates the properties of the CUDA devices present in the system. * Added matrixMul. Demonstrates a matrix multiplication using shared memory through tiled approach. * Added matrixMulDrv. Demonstrates a matrix multiplication using shared memory through tiled approach, uses CUDA Driver API. * Added cudaTensorCoreGemm. Demonstrates a GEMM computation using the Warp Matrix Multiply and Accumulate (WMMA) API introduced in CUDA 9, as well as the new Tensor Cores introduced in the Volta chip family. * Added simpleVoteIntrinsics which uses *_sync equivalent of the vote intrinsics _any, _all added since CUDA 9.0. * Added shfl_scan which uses *_sync equivalent of the shfl intrinsics added since CUDA 9.0. * Added conjugateGradientMultiBlockCG. Demonstrates a conjugate gradient solver on GPU using Multi Block Cooperative Groups. * Added conjugateGradientMultiDeviceCG. Demonstrates a conjugate gradient solver on multiple GPUs using Multi Device Cooperative Groups, also uses unified memory prefetching and usage hints APIs. * Added simpleCUBLAS. Demonstrates how perform GEMM operations using CUBLAS library. * Added simpleCUFFT. Demonstrates how perform FFT operations using CUFFT library."
  },
  {
    "objectID": "notes/notes8.html",
    "href": "notes/notes8.html",
    "title": "Notes 8: GPUs",
    "section": "",
    "text": "This document is the eighth of a set of notes, this document focusing on the parallelization via GPUs. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.\n\n\n\n\n\n\nGPU code run separately from rendering\n\n\n\nIn most cases, I have set the code chunks to not execute when rendering this document, since that would require rendering to HTML/PDF on a machine with a GPU. In some cases I’ve copied in output from running the chunk manually outside of the rendering process, sometimes as part of code comments.",
    "crumbs": [
      "Course Notes",
      "Notes 8: GPUs"
    ]
  },
  {
    "objectID": "notes/notes8.html#introduction",
    "href": "notes/notes8.html#introduction",
    "title": "Notes 8: GPUs",
    "section": "",
    "text": "This document is the eighth of a set of notes, this document focusing on the parallelization via GPUs. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.\n\n\n\n\n\n\nGPU code run separately from rendering\n\n\n\nIn most cases, I have set the code chunks to not execute when rendering this document, since that would require rendering to HTML/PDF on a machine with a GPU. In some cases I’ve copied in output from running the chunk manually outside of the rendering process, sometimes as part of code comments.",
    "crumbs": [
      "Course Notes",
      "Notes 8: GPUs"
    ]
  },
  {
    "objectID": "notes/notes8.html#gpu-overview",
    "href": "notes/notes8.html#gpu-overview",
    "title": "Notes 8: GPUs",
    "section": "GPU overview",
    "text": "GPU overview\n(This is repeated from the last set of notes.)\nSome features of GPUs:\n\nMany processing units (thousands) that are slow individually compared to the CPU but provide massive parallelism.\nThey have somewhat more limited memory (though modern GPUs like the A100 can have 80 GB of GPU memory).\nThey can only use data in their own memory, not in the CPU’s memory, so one must transfer data back and forth between the CPU (the host) and the GPU (the device). This copying can, in some computations, constitute a very large fraction of the overall computation. So it is best to create the data and/or leave the data (for subsequent calculations) on the GPU when possible and to limit transfers.\nFor large-scale work, a GPU server will have multiple GPUs, e.g., 8 A100s or 8 H100s. We won’t discuss running code on multiple GPUs nor on GPUs distributed across multiple machines.\nWe’ll focus on NVIDIA GPUs and NVIDIA’s CUDA platform for running code on the GPU, but other manufacturers produce other kinds of GPUs accessed via different interfaces.\n\n\nAccessing a GPU on the SCF\nWe have one (somewhat old) GPU on the gpu partition that is available for all to use equally.\nAfter logging into an SCF standalone/login machine such as arwen, gandalf, or radagast, you can ask for access to a GPU through the SLURM scheduler.\nsrun -p gpu --gpus=1 --pty bash\nnvidia-smi\nA large variety of other, more powerful GPUs, are available through “partitions” that were purchased by faculty research grants. You’re welcome to use them, but your job could be preempted (killed) at any moment by higher-priority jobs.\nThe partitions with GPUs include jsteinhardt, yss, and yugroup. Most of the GPUs are in jsteinhardt.\nHere’s how you’d request an A100 GPU in the jsteinhardt partition.\nsrun -p gpu -jsteinhardt --gpus=A100:1 --pty bash\nnvidia-smi\nOne can also use sbatch to run a background job.\nMore details on using the SCF cluster and the GPUs in it are available in our quick start guide.\n\n\nGPU availability\nIf you’re on a machine with a (NVIDIA) GPU, you should be able to run nvidia-smi from the command line to see what GPUs are available to you. Depending on how the system is set up, if you’re running through a scheduler such as Slurm, you may not see all the GPUs that are physically on the machine.\nThe environment variable CUDA_VISIBLE_DEVICES will show the ID(s) of the GPU(s) available to you. You generally won’t need to use this, but it can be useful to check this variable.\n#| eval: false\necho $CUDA_VISIBLE_DEVICES\nWe can check in Julia. Here’s what we would see if a GPU is available (also requiring that CUDA.jl is set up to use a GPU even if one is available).\n\nusing CUDA\nCUDA.has_cuda_gpu()\n\ntrue\n\n\n\n\n\n\nInvoke using CUDA on a machine with a GPU\n\n\n\nI invoked using CUDA on a machine without a GPU and it caused some pre-compilation that then prevented me from using the GPU on a machine with a GPU, giving messages like “Error: CUDA.jl could not find an appropriate CUDA runtime to use”. Untangling that took some effort (in particular manually removing CUDA-related Julia packages via rm -r ~/.julia/compiled/v1.10/CUDA*).\nSo if you’re on a cluster or other system with multiple machines (such as the SCF), some with and without GPUs, it’s best if you only use CUDA on a machine with a GPU.\n\n\nThere are various other Julia CUDA package functions that we can explore for checking details of the GPU, such as CUDA.total_memory().",
    "crumbs": [
      "Course Notes",
      "Notes 8: GPUs"
    ]
  },
  {
    "objectID": "notes/notes8.html#basic-gpu-offloading",
    "href": "notes/notes8.html#basic-gpu-offloading",
    "title": "Notes 8: GPUs",
    "section": "Basic GPU offloading",
    "text": "Basic GPU offloading\nBefore we get into the complexities of how GPUs work and what one has to do to write one’s own code to use the GPU, let’s see how easily one can offload standard calculations (in particular linear algebra) to the GPU.\nThe main package we’ll use is the CUDA package. It provides:\n\nthe CuArray type for working with arrays on the GPU,\nwrappers for functions in NVIDIA’s CUDA libraries (the NVIDIA CUDA toolkit is distinct from the Julia CUDA package) that allow one to use Julia functions/operators with CuArrays, and\ntooling for writing CUDA kernels in Julia.\n\nIn this section we’ll see the first two, before looking at writing kernels in the last section of these notes.\nNote that this kind of offloading of linear algebra and vectorized computations to the GPU can be done similarly with PyTorch, JAX and CuPy in Python.\n\nMatrix multiplication example\nMatrix multiplication is simple, but it’s at the heart of a lot of algorithms, including deep learning, so it’s a simple, but non-trivial example of a situation where we could offload a part of a computation to the GPU for big speedups.\nFirst we’ll set up the matrices, generating them on the host/CPU (and they’ll therefore be in CPU memory) and transferring to GPU (device) memory.\n\nusing BenchmarkTools\nusing CUDA\nusing LinearAlgebra\n\nn = 7000;\n\nx = randn(n, n);\ny = randn(n, n);\nx_gpu = CuArray(x);\ny_gpu = CuArray(y);\n\n## These use 64-bit numbers:\ntypeof(x)\n# Matrix{Float64} (alias for Array{Float64, 2})\ntypeof(x_gpu)\n# CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}\n\nCUDA.total_memory()\n# 85097971712         # This is using an A100 GPU with 85 GB GPU memory.\nCUDA.used_memory()\n# 784000000\nn*n*8*2               # \"Theoretical\" memory use.\n# 784000000\n\nNow let’s compare the speed of the multiplication on CPU (host) vs. GPU (device). As usual we’ll put our computation into functions to allow for JIT compilation and avoid use of global variables whose type can change.\n\nLinearAlgebra.BLAS.set_num_threads(1);  # To keep things simple but perhaps unfair to the CPU.\n\nfunction matmult(x, y)\n    z = x * y;\n    return z\nend\n\n@btime out = matmult(x, y);  \n# 10.280 s (2 allocations: 373.84 MiB)\n\n@btime CUDA.@sync z_gpu = matmult(x_gpu, y_gpu);\n# 41.270 ms (51 allocations: 1.19 KiB)\n\nz_gpu = matmult(x_gpu, y_gpu);\ntypeof(z_gpu)\n# CuArray{Float64, 2, CUDA.DeviceMemory}\nz[1]\n# ERROR: Scalar indexing is disallowed.\n# Invocation of getindex resulted in scalar indexing of a GPU array.\n\n# Moving the result back to the host.\nz = Array(z_gpu);\nz[1]\n\nSo the speedup was about 250-fold!\nIn class we’ll use nvidia-smi and top to monitor the GPU and CPU.\n\n\n\n\n\n\nTiming GPU code\n\n\n\nSome comments on timing GPU code:\n\nI believe it’s reasonable to use @btime to time GPU code\nWe could also use CUDA.@time.\nSince GPU code is executed asynchronously, we should use CUDA.@sync or synchronize to make sure the execution is finished.\n\n\n\nGPUs use a lot of electricity. It’s possible that an administrator of a computing cluster might throttle the power consumption of a GPU server and this could affect computational speed.\n\n\nMultiple dispatch with GPU operations\nGiven that we use * with two CuArray matrices, Julia’s multiple dispatch system is clearly involved in determining what code to call to execute the matrix multiplication on the GPU.\nDigging around a bit in the source code for the LinearAlgebra and CUDA packages, it looks like the result of the dispatch is to call LinearAlgebra.generic_matmatmul!, with the method that works for CuArrays in the lib/cublas/linalg.jl file in the CUDA package. That presumably invokes the cuBLAS versions of the BLAS functions found in lib/cublas/libcublas.jl and lib/cublas/wrappers.jl where we see calls out to native C-based cuBLAS functions. But I don’t see where the C code is.\nAt a high level that makes sense, though the details of all the wrapping and type inheritance/dispatch that is going on might be hard to trace through carefully.\n\n\nCopying data to/from the GPU\nIf we copy a lot of data back and forth to the GPU, that can end up being a large fraction of the time of a GPU-based calculation. We’d like to:\n\ngenerate data on the GPU if we can,\nkeep data on the GPU for multiple steps in a calculation, and\navoid copying output back to the device if possible.\n\nLet’s see how the time of copying compares to the time of doing the matrix multiplication.\n\nx = randn(n, n);\n\n# To the GPU:\n@btime CUDA.@sync x_gpu = CuArray(x);\n# 61.138 ms (10 allocations: 288 bytes)\nCUDA.@time CUDA.@sync x_gpu = CuArray(x);\n#  0.071553 seconds (12 CPU allocations: 320 bytes) (1 GPU allocation: 373.840 MiB, 0.03% memmgmt time)\n\n# From the GPU:\nCUDA.@time CUDA.@sync out = Array(z_gpu);\n# 0.329900 seconds (7 CPU allocations: 373.840 MiB)\n\nSo the transfer back takes much longer than the actual calculation!\nThe time for the transfer back to the host includes the time to allocate space on the host. We’d have to do more work to disentangle the transfer time from the allocation time (e.g., timing y = Vector{Float64}(undef, n);).\n\n\n64- vs 32-bit numbers\nIn most contexts, computation on a GPU will use 32-bit real numbers for efficiency. This reduces memory use and speeds up computation because half as many bytes are being allocated, copied, and computed with. Of course we want to keep in mind the reduced precision in case that could be a problem for certain computations.\nLet’s first simply see if matrix multiplication on the CPU is faster with 32-bit numbers.\n\nusing BenchmarkTools\n\nfunction matmult(x, y)\n    z = x * y\n    return z\nend\n\nn = 5000;\n\nx = randn(n, n);\ny = randn(n, n);\n\n@btime matmult(x, y);\n\nx32 = convert(Array{Float32}, x);\ny32 = convert(Array{Float32}, y);\nsizeof(x)\nsizeof(x32)\n\n@btime matmult(x32, y32);\n\n  1.546 s (2 allocations: 190.73 MiB)\n  944.545 ms (2 allocations: 95.37 MiB)\n\n\nIndeed it is quite a bit faster.\nNow let’s consider that on the GPU. We’ll also show random number generation directly on the GPU to avoid the cost of transferring data to the GPU.\n\nx64_gpu = CUDA.randn(Float64, n, n);\ny64_gpu = CUDA.randn(Float64, n, n);\ntypeof(x64_gpu)\n# CuArray{Float64, 2, CUDA.DeviceMemory}\n\nx32_gpu = CUDA.randn(n, n);   # `Float32` is the default.\ny32_gpu = CUDA.randn(n, n);\ntypeof(x32_gpu)\n# CuArray{Float32, 2, CUDA.DeviceMemory}\n\n@btime CUDA.@sync z64_gpu = matmult(x64_gpu, y64_gpu);\n# 39.353 ms (51 allocations: 1.19 KiB)\n@btime CUDA.@sync z32_gpu = matmult(x32_gpu, y32_gpu);\n# 38.409 ms (51 allocations: 1.19 KiB)\n\n@btime CUDA.@sync z64 = Array(z64_gpu);\n# 298.258 ms (7 allocations: 373.84 MiB)\n@btime CUDA.@sync z32 = Array(z32_gpu);\n# 147.791 ms (7 allocations: 186.92 MiB)\n\nI’m quite surprised the 32-bit multiplication is no faster. I’m not sure what is going on. (If you time the randn you should see that the 32-bit randn generation is about half the time of the 64-bit generation.)\nIt does make sense that the transfer back takes about half as long.\n\n\nInteracting with NVIDIA’s CUDA libraries\nWorking with objects on the GPU requires interfacing with functions provided by the CUDA Toolkit and its libraries such as cuBLAS, cuRAND, cuFFT, etc. These are provided for Julia with the CUDA package (see CUDA/&lt;version_string&gt;/lib) so all that you need installed on the machine with the GPU is the NVIDIA driver software and the Julia CUDA package. You don’t actually need NVIDIA’s CUDA Toolkit.\nSince Julia is calling out to CUDA functions to run code on the GPU, there may be functions that won’t work on a CuArray, but the various things I tried all seem to work.\n\ny_gpu = CUDA.randn(3);    # This uses cuRAND.\ny_gpu + y_gpu\n\n3-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n -2.3132815\n  0.11570158\n -1.0381625\n\nsin.(y_gpu)\n\n3-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n -0.986312\n -0.784628\n  0.7079291\nHowever, we can’t work with scalar elements of the array because that tries to do a non-vectorized calculation on a GPU-based array.\n\ny_gpu[1]\n\njulia&gt; y_gpu[1]\nERROR: Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore should be avoided.\n\n\nVectorized example\nHere we’ll offload a vectorized calculation to the GPU. Note that in this case we cannot set up the calculation as a for loop as one can’t loop through scalar operations on GPU in that fashion (more about how to loop on a GPU in the discussion of GPU kernels).\nWe’ll use .= for in-place calculation to avoid additional memory allocation.\n\nfunction vec_calc(x)\n  x .= tan.(x) .+ 3 .* sin.(x)\n  return 0\nend\n\nn = 250000000;\n\nx = rand(n);            # 2 GB of data.\nx_gpu = CuArray(x);\n\n# CPU calculation:\n@btime vec_calc(x);     # Again a bit unfair to the host/CPU in that we don't exploit multiple CPU cores.\n# 9.041 s (0 allocations: 0 bytes)\n\n# GPU calculation:\n# Try to flush out any compilation time.\ntest = CuArray([1.1, 2.2])\nCUDA.@sync vec_calc(test);  # This seems to take surprisingly long.\n\nCUDA.@time CUDA.@sync vec_calc(x_gpu);\n# 0.099259 seconds (39.83 k CPU allocations: 2.770 MiB)\nCUDA.@time CUDA.@sync vec_calc(x_gpu);\n# 0.003745 seconds (88 CPU allocations: 3.672 KiB)\n\nI’m not sure why the initial call on test takes so long. I’m also not sure what explains the big difference between running the calculation on the full vector the first and second times (including why the first call involves so many separate allocations). The first time represents a 91-fold speedup and the second time a surprising 2400-fold speedup.\nI’m also not sure of what is going on behind the scenes in terms of the Julia interpeter and/or compiler processing the vec_calc code to allow it to be run via CUDA calls on the GPU. @code_native vec_calc(x_gpu) indicates use of the GPUArrays package, in particular use of gpu_call to run a kernel on the GPU. So I think the main question is) how Julia creates a Julia-coded kernel function (such as this example Julia kernel. (Once the kernel function is created, there seem to be tools to convert LLVM code to code that runs on the GPU (more later).\n\n\n\n\n\n\nExercise\n\n\n\nSee how small n can be before the time of doing the calculation on the GPU exceeds the time for doing it on the CPU.\nThen, consider how the transfer time to/from the GPU affects the trade-off (in this case presuming the only calculation you were doing was vec_calc and that you needed the full output back on the host for a later calculation that can’t be done on the GPU.",
    "crumbs": [
      "Course Notes",
      "Notes 8: GPUs"
    ]
  },
  {
    "objectID": "notes/notes8.html#gpu-architecture",
    "href": "notes/notes8.html#gpu-architecture",
    "title": "Notes 8: GPUs",
    "section": "GPU architecture",
    "text": "GPU architecture\nNow that we’ve seen some of what we can do with GPUs, let’s consider some about how they work and what that implies about what calculations we can usefully offload to them.\n\nOverview\nGPUs have thousands of “slow” (slower than the CPU) cores. Their speed comes from massive parallelization and how the code is executed in parallel on the input data.\nEach individual computation or series of computations on the GPU is done in a thread. Threads are organized into blocks and blocks of threads are organized in a grid. The blocks and grids can be 1-, 2-, or 3-dimensional. E.g., you might have a 1-d block of 256 threads, with a grid of 3 x 3 such blocks, for a total of 256×9=2304 threads.\nThe choice of the grid/block arrangement can affect efficiency. I’m not an expert at this level of detail but we’ll see some about this in the kernel example below. Note that using more than 1-dimensional grids and blocks is purely for the conceptual convenience of the programmer and doesn’t correspond to anything on the hardware. So for the most part we’ll use a one-dimensional grid of blocks and a one-dimensional blocks of threads.\nIn general you’d want each independent calculation done in a separate thread, but one might want to do a sequence of calculations on each thread. In general, you’ll want to pipeline together multiple operations within a computation to avoid copying from CPU to GPU and back. Alternatively, this can be done by keeping the data on the GPU and calling a second kernel.\n\n\nGlossary\n\nstreaming multiprocessor (SM): a set of GPU cores with registers and shared memory; each GPU has multiple SMs\nthreads: parallel execution units\nblocks: threads are grouped into blocks\ngrid: blocks are grouped into a grid\nwarp: a group of (usually) 32 threads that operate together\nkernel: a function that runs on the GPU (usually one thinks of a C function, but we’ll write Julia-coded kernels)\ndevice: the GPU and its memory\nhost: the CPU and its memory\n\n\n\nThreads, blocks and grids\nSome facts about the organization of threads on a GPU.\nThreads:\n\nthreads grouped into a warp (on A100 and possibly most others, 32 threads per warp)\nthreads in a warp operate in lock step (so an if statement causes threads to pause to wait for other threads that follow other branches)\nGPU can switch between warps very quickly\nthreads should be cache-aware (best to have adjacent threads use adjacent memory)\n\nBlocks:\n\n1, 2, or 3-dimensional block of threads\n\n2 and 3 dimensions are just for code clarity, not real\n\nthreads in a block have access to some fast shared memory on chip (for A100 this is 49 KB)\nA100 GPU has up to 1024 threads per block\n\nGrid\n\n1, 2, or 3-dimensional grid of blocks (again for code clarity only)\nA100 GPU has up to 2147483647 blocks in the grid (I think)\n\n\n\nGPU execution and efficiency\n\nGPUS are good at executing the same computation across many data elements at once (called “SIMD” for “single instruction, multiple data”.\nThreads in a block have access to some shared memory, which is on the chip, so access is very fast.\nThreads operate in a warp of 32 threads.\nThreads in a warp operate in lock-step, so if statements can be inefficient (some threads have to wait while a branch of the if statement is run for other threads).\nGPUs can switch between warps very quickly, which would happen while a warp waits for data.\nThreads in a warp should use neighboring values in an array for efficiency (note our discussion of the cache in the CPU context).\nComputations on a GPU generally use single precision (32 bit floats) (or even 16 or 8 byte floats), which will often run much faster than with doubles (64 bit floats).",
    "crumbs": [
      "Course Notes",
      "Notes 8: GPUs"
    ]
  },
  {
    "objectID": "notes/notes8.html#gpu-monitoring---details",
    "href": "notes/notes8.html#gpu-monitoring---details",
    "title": "Notes 8: GPUs",
    "section": "GPU monitoring - details",
    "text": "GPU monitoring - details\n\nUsing nvidia-smi for hardware and activity\nHere’s some example output from a machine with multiple GPUs, but only one of which is available to us:\nnvidia-smi\nFri Jan  3 14:11:23 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:1A:00.0 Off |                  N/A |\n| 22%   23C    P8              2W /  100W |       1MiB /  11264MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n\nNote that it shows that the GPU is an NVIDIA GeForce RTX 2080 Ti that has 11264MiB of memory (i.e., ~11 GB), and that at the moment, no process is using the GPU.\nNext, if I logon as an administrator, I can see all 10 GPUs on one of the SCF machines, and that 7 of the GPUs are in use:\nFri Jan  3 14:16:11 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A100 80GB PCIe          On  | 00000000:4F:00.0 Off |                    0 |\n| N/A   38C    P0              82W / 150W |   9887MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   1  NVIDIA A100 80GB PCIe          On  | 00000000:52:00.0 Off |                    0 |\n| N/A   40C    P0              80W / 150W |  11833MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   2  NVIDIA A100 80GB PCIe          On  | 00000000:53:00.0 Off |                    0 |\n| N/A   30C    P0              71W / 150W |  10039MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   3  NVIDIA A100 80GB PCIe          On  | 00000000:56:00.0 Off |                    0 |\n| N/A   39C    P0              82W / 150W |  10027MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   4  NVIDIA A100 80GB PCIe          On  | 00000000:57:00.0 Off |                    0 |\n| N/A   32C    P0              79W / 150W |   9899MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   5  NVIDIA A100 80GB PCIe          On  | 00000000:CE:00.0 Off |                    0 |\n| N/A   58C    P0             154W / 150W |  53949MiB / 81920MiB |     99%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   6  NVIDIA A100 80GB PCIe          On  | 00000000:D1:00.0 Off |                    0 |\n| N/A   42C    P0              84W / 150W |  49121MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   7  NVIDIA A100 80GB PCIe          On  | 00000000:D2:00.0 Off |                    0 |\n| N/A   24C    P0              41W / 150W |      4MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   8  NVIDIA A100 80GB PCIe          On  | 00000000:D5:00.0 Off |                    0 |\n| N/A   35C    P0              79W / 150W |      7MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   9  NVIDIA A100 80GB PCIe          On  | 00000000:D6:00.0 Off |                    0 |\n| N/A   42C    P0              86W / 150W |      7MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A     37514      C   .../users/omer_ronen/mutemb/bin/python     9874MiB |\n|    1   N/A  N/A     37509      C   .../users/omer_ronen/mutemb/bin/python    11820MiB |\n|    2   N/A  N/A     37506      C   .../users/omer_ronen/mutemb/bin/python    10026MiB |\n|    3   N/A  N/A     37522      C   .../users/omer_ronen/mutemb/bin/python    10014MiB |\n|    4   N/A  N/A     37545      C   .../users/omer_ronen/mutemb/bin/python     9886MiB |\n|    5   N/A  N/A    142017      C   python                                    53936MiB |\n|    6   N/A  N/A    107884      C   ...2023/conda/envs/NODE/bin/python3.12    49108MiB |\n+---------------------------------------------------------------------------------------+\n\n\ndeviceQuery for hardware information\nThe deviceQuery utility provided with CUDA will give details on the GPU hardware.\nHere’s an example with one of the SCF GPU servers, showing information about the processors available on the A100 GPU and the number of threads and blocks that are possible.\npaciorek@saruman:~&gt; deviceQuery\ndeviceQuery Starting...\n\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 10 CUDA Capable device(s)\n\nDevice 0: \"NVIDIA A100 80GB PCIe\"\n  CUDA Driver Version / Runtime Version          12.2 / 12.2\n  CUDA Capability Major/Minor version number:    8.0\n  Total amount of global memory:                 81051 MBytes (84987740160 bytes)\n  (108) Multiprocessors, (064) CUDA Cores/MP:    6912 CUDA Cores\n  GPU Max Clock rate:                            1410 MHz (1.41 GHz)\n  Memory Clock rate:                             1512 Mhz\n  Memory Bus Width:                              5120-bit\n  L2 Cache Size:                                 41943040 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total shared memory per multiprocessor:        167936 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  2048\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n  Run time limit on kernels:                     No\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Enabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device supports Managed Memory:                Yes\n  Device supports Compute Preemption:            Yes\n  Supports Cooperative Kernel Launch:            Yes\n  Supports MultiDevice Co-op Kernel Launch:      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 79 / 0\n  Compute Mode:\n     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;\nWe can create the deviceQuery executable like this (this would need to be modified for other systems/machines):\nmodule load cuda   # This gives access to the `nvcc` compiler.\ngit clone https://github.com/NVIDIA/cuda-samples\nnvcc Samples/1_Utilities/deviceQuery/deviceQuery.cpp -I/usr/local/cuda-12.2/include \\\n   -ICommon -o ~/deviceQuery\n\nMonitoring GPU usage\ngpustat is useful for monitoring details of GPU usage.\nWe can install is using the Python pip installer.\npip install --user gpustat\n~/.local/bin/gpustat\nsaruman                   Fri Jan  3 14:33:36 2025  535.104.05\n[0] NVIDIA A100 80GB PCIe | 42°C,   0 % |  9887 / 81920 MB | omer_ronen(9874M)\n[1] NVIDIA A100 80GB PCIe | 39°C,   0 % | 11833 / 81920 MB | omer_ronen(11820M)\n[2] NVIDIA A100 80GB PCIe | 29°C,   0 % | 10039 / 81920 MB | omer_ronen(10026M)\n[3] NVIDIA A100 80GB PCIe | 38°C,   0 % | 10027 / 81920 MB | omer_ronen(10014M)\n[4] NVIDIA A100 80GB PCIe | 32°C,   0 % |  9899 / 81920 MB | omer_ronen(9886M)\n[5] NVIDIA A100 80GB PCIe | 60°C, 100 % | 68155 / 81920 MB | arjunpatrawala(68142M)\n[6] NVIDIA A100 80GB PCIe | 47°C,  14 % | 36377 / 81920 MB | zhangyunzhe2023(36364M)\n[7] NVIDIA A100 80GB PCIe | 24°C,   0 % |     4 / 81920 MB |\n[8] NVIDIA A100 80GB PCIe | 34°C,   0 % |     7 / 81920 MB |\n[9] NVIDIA A100 80GB PCIe | 42°C,   0 % |     7 / 81920 MB |\nThere are various flags. I’m not familiar with the details but this invocation shows more information about the processes running on each GPU:\n~/.local/bin/gpustat -fup\nsaruman                   Fri Jan  3 14:35:20 2025  535.104.05\n[0] NVIDIA A100 80GB PCIe | 47°C,   0 % |  9887 / 81920 MB | omer_ronen/37514(9874M)\n └─  37514 (  90%,   14GB): /scratch/users/omer_ronen/mutemb/bin/python -m mutemb.utils.data --chrom 1\n[1] NVIDIA A100 80GB PCIe | 41°C,   0 % | 11833 / 81920 MB | omer_ronen/37509(11820M)\n └─  37509 ( 100%,   10GB): /scratch/users/omer_ronen/mutemb/bin/python -m mutemb.utils.data --chrom 2\n[2] NVIDIA A100 80GB PCIe | 29°C,   0 % | 10039 / 81920 MB | omer_ronen/37506(10026M)\n └─  37506 (  10%,   11GB): /scratch/users/omer_ronen/mutemb/bin/python -m mutemb.utils.data --chrom 3\n[3] NVIDIA A100 80GB PCIe | 38°C,   0 % | 10027 / 81920 MB | omer_ronen/37522(10014M)\n └─  37522 ( 120%,   37GB): /scratch/users/omer_ronen/mutemb/bin/python -m mutemb.utils.data --chrom 4\n[4] NVIDIA A100 80GB PCIe | 32°C,   0 % |  9899 / 81920 MB | omer_ronen/37545(9886M)\n └─  37545 ( 100%,   26GB): /scratch/users/omer_ronen/mutemb/bin/python -m mutemb.utils.data --chrom 5\n[5] NVIDIA A100 80GB PCIe | 60°C, 100 % | 68155 / 81920 MB | arjunpatrawala/142017(68142M)\n └─ 142017 ( 100%, 4010MB): python generate_jacobians.py\n[6] NVIDIA A100 80GB PCIe | 44°C,   0 % | 27707 / 81920 MB | zhangyunzhe2023/228882(27724M)\n └─ 228882 (  50%, 1144MB): /scratch/users/zhangyunzhe2023/conda/envs/NODE/bin/python3.12 -Xfrozen_modules=off -m ipykernel_launcher -f /accounts/grad/zhangyunzhe2023/.jupyter/runtime/kernel-ef1939c2-4af1-4fe4-bf29-ae1195d5ab4a.json\n[7] NVIDIA A100 80GB PCIe | 24°C,   0 % |     4 / 81920 MB |\n[8] NVIDIA A100 80GB PCIe | 35°C,   0 % |     7 / 81920 MB |\n[9] NVIDIA A100 80GB PCIe | 42°C,   0 % |     7 / 81920 MB |",
    "crumbs": [
      "Course Notes",
      "Notes 8: GPUs"
    ]
  },
  {
    "objectID": "notes/notes8.html#gpu-kernels",
    "href": "notes/notes8.html#gpu-kernels",
    "title": "Notes 8: GPUs",
    "section": "GPU kernels",
    "text": "GPU kernels\nA kernel is a function that can be run on the GPU.\nNext we’ll see that we can actually write a kernel in Julia and then call that kernel. Kernels are functions that encode the core computational operations that are executed in parallel.\nIn other languages, the basic mode of operation with a GPU when you are writing your own GPU code is to write a kernel using CUDA (basically C) code and then call the kernel in parallel via C, R, or Python code. In Julia, we can write the kernel using Julia syntax (though many operations (particularly non-numerical ones) will not run on the GPU…).\nI have not investigated much to see what Julia does behind the scenes with the kernel code for it to be able to run on the GPU, but this blog post seems helpful. I think the steps involve going from LLVM code to PTX code (an assembly language for GPUs) that is then converted to GPU machine code. These steps involve tooling available outside of Julia, which seems like a reflection of the benefit of choosing to have Julia’s compilation system use LLVM.\n\nBasic example\nHere’s a basic example in which we’ll do a calculation in place. We run 1000 scalar calculations using 1000 threads.\nWe use @cuda to compile and run the kernel.\n\nfunction my_kernel(x)\n  idx = threadIdx().x;   # What thread am I?\n  if idx &lt;= length(x)\n    x[idx] = tan(x[idx]) + 3*sin(x[idx]);\n  end\n  return\nend\n\nn = 1000;\nx_gpu = CUDA.randn(n);\nArray(x_gpu)[n]\n# -1.5321726f0\n@cuda threads=n my_kernel(x_gpu);\nArray(x_gpu)[n]   # Check the computation was done by checking last element.\n# -28.875708f0\n\nThere are limits on the number of threads we can use.\n\nn = 2000;\nx_gpu = CUDA.randn(n);\n@cuda threads=n my_kernel(x_gpu);\n# ERROR: Number of threads in x-dimension exceeds device limit (2000 &gt; 1024).\n\n\n\nMultiple blocks\nWe need to use at least as many threads as computations, and in addition to only being able to use 1024 threads in the x dimension, we can have at most 1024 threads in a block on the A100 GPU we’re using. So we’ll need multiple blocks.\n\nfunction my_kernel(x)\n  i = threadIdx().x;  # What thread am I within the block?\n  j = blockIdx().x;   # What block am I in?\n  idx =  (j-1)*blockDim().x + i;\n  if idx &lt;= length(x)\n    x[idx] = tan(x[idx]) + 3*sin(x[idx]);\n  end\n  return\nend\n\nn = 2000;\nnthreads = 1024;\nx_gpu = CUDA.randn(n);\ninitial = Array(x_gpu)[n]\nnblocks = Int(ceil(n/nthreads));\n\n@cuda threads=nthreads blocks=nblocks my_kernel(x_gpu);\n(initial, Array(x_gpu)[n])  # Check that calculation was done.\n\nLet’s do a smaller test run in which we can check on the thread and block indexing.\n\nfunction my_kernel_print(x)\n  i = threadIdx().x;  # What thread am I within the block?\n  j = blockIdx().x;   # What block am I in?\n  idx =  (j-1)*blockDim().x + i;\n  if idx &lt;= length(x)\n    x[idx] = tan(x[idx]) + 3*sin(x[idx]);\n    @cuprintln idx, i, j, blockDim().x, blockDim().y;\n  end\n  return\nend\n\nn = 200;\nx_gpu = CUDA.randn(n);\nnthreads = 100;\nnblocks = Int(ceil(n/nthreads));\n@cuda threads=nthreads blocks=nblocks my_kernel_print(x_gpu);\n\nWhen we run this, notice the output seems to be grouped based on warps of 32 threads (apart from the last set since n=200 is not a multiple of 32).\n\n\nLarger computations\nIn many cases we’ll have more tasks than the total number of GPU cores. As long as we don’t exceed the maximum size of a block or grid, we can just ask for as many threads as we have tasks and rely on the GPU to manage assigning the tasks to the GPU cores.\nWe’d want to check that the number/dimension of the block here does not exceed the maximum block size. I didn’t do that, but it ran, so it must have been ok!\nHere we’ll run the computation we ran earlier when we did not write our own kernel and just relied on Julia to offload to the GPU behind the scene.\n\nn = 250000000;\nx_gpu = CUDA.randn(n);\nnthreads = 1024;\nnblocks = Int(ceil(n/nthreads));\nArray(x_gpu)[n]\n\n# Run it once to flush out any compilation/transformation time.\ny_gpu = CUDA.randn(5);\nCUDA.@sync @cuda threads=nthreads blocks=nblocks my_kernel(y_gpu);\n\nCUDA.@time CUDA.@sync @cuda threads=nthreads blocks=nblocks my_kernel(x_gpu);\n# 0.002003 seconds (45 CPU allocations: 2.719 KiB)\nArray(x_gpu)[n]\n\nThe 2.0 ms is reasonably comparable to the 3.7 ms when we just had Julia run the vectorized computation on the GPU (from the last time we ran it). That used 64-bit floats. When I reran the code above using 64-bit floats, the time was 5.2 ms.",
    "crumbs": [
      "Course Notes",
      "Notes 8: GPUs"
    ]
  },
  {
    "objectID": "notes/notes8.html#efficient-memory-access",
    "href": "notes/notes8.html#efficient-memory-access",
    "title": "Notes 8: GPUs",
    "section": "Efficient memory access",
    "text": "Efficient memory access\nWe’ll explore two final topics related to efficiently accessing data in memory: first accessing global GPU memory efficiently and second making use of shared GPU memory.\n\nCoalesced access to global memory\nIf adjacent threads in a block access adjacent memory locations, a chunk of data can be obtained in a single access to global memory.\nHere’s a very basic example where we see that reading a matrix by column is more efficient than reading by row. From what I’ve read, I was expecting to see more of a difference, but we do get almost a 2-fold speedup working by column rather than by row.\n\nn = 1000;\nin_gpu = CUDA.randn(n,n);\nout_gpu = CUDA.zeros(n,n);\n\nin_gpu_small = CUDA.randn(5,5);\nout_gpu_small = CUDA.zeros(5,5);\n\n# Good: Adjacent threads process same column.\nfunction kernel_bycol!(output, input)\n    idx_x = threadIdx().x    # Column index\n    idx_y = blockIdx().x     # Row index\n    \n    if idx_x &lt;= size(input, 1) && idx_y &lt;= size(input, 2)\n        output[idx_x, idx_y] = input[idx_x, idx_y]\n    end\n    return nothing\nend\n\nnthreads = n;\nnblocks = n;\n\n# Flush out any compilation time.\nCUDA.@sync @cuda threads=nthreads blocks=nblocks kernel_bycol!(out_gpu_small, in_gpu_small);\n\nCUDA.@time CUDA.@sync @cuda threads=nthreads blocks=nblocks kernel_bycol!(out_gpu, in_gpu);\n# 0.000106 seconds (33 CPU allocations: 960 bytes)\n\n# Bad: Adjacent threads process same row.\nfunction kernel_byrow!(output, input)\n    idx_x = blockIdx().x     # Column index\n    idx_y = threadIdx().x    # Row index\n    \n    if idx_x &lt;= size(input, 1) && idx_y &lt;= size(input, 2)\n        output[idx_x, idx_y] = input[idx_x, idx_y]\n    end\n    return nothing\nend\n\n# Flush out any compilation time.\nCUDA.@sync @cuda threads=nthreads blocks=nblocks kernel_byrow!(out_gpu_small, in_gpu_small);\n\nCUDA.@time CUDA.@sync @cuda threads=nthreads blocks=nblocks kernel_byrow!(out_gpu, in_gpu);\n# 0.000254 seconds (33 CPU allocations: 960 bytes)\n\n\n\nUsing shared memory\nAccessing global GPU memory is much slower than doing computation on the GPU. So we’d like to avoid repeated access to global memory (e.g., a bad scenario would be a ratio of one arithmetic calculation per retrieval from global memory). One strategy is for multiple threads in a block to cooperate to load data from global memory into shared memory accessible by all the threads in the block. The computation can then be done on the data in shared memory.\nHere’s a simplified example that shows how to load the data into shared memory. There’s no actual computation, but if we think of the kernel density estimation problem in PS 3, one could imagine that each thread would then each do a computation that uses the entire chunk of data in shared memory.\n\nfunction kernel_reader_bycol(x::CuDeviceArray{T}) where T\n  i = threadIdx().x;  # What thread am I within the block?\n  j = blockIdx().x;   # What block am I in?\n  idx =  (j-1)*blockDim().x + i;\n  dims = size(x);\n  \n  # Setup shared memory for the subset of data.\n  shared_data = CuDynamicSharedArray(T, (blockDim().x, dims[2]));\n\n  for chunk_start = 1:blockDim().x:dims[1]\n    chunk_size = min(blockDim().x, dims[1] - chunk_start + 1);\n    ## Transfer a chunk of rows in parallel, one row per thread.\n    if i &lt;= chunk_size\n      for col in 1:dims[2]\n        shared_data[i, col] = x[chunk_start + i - 1, col];\n      end\n    end\n    sync_threads()\n    \n    # At this point we'd insert code to do the actual computation, based on `idx`.\n    # Each thread now has the opportunity to compute on all the data in the chunk in\n    # `shared_data`.\n    \n  end\n\n  return\nend\n\nn = 10000000;\nm = 10;\nx_gpu = CUDA.randn(n, m);\nx_gpu_small = CUDA.randn(5, m);\n\nnthreads = 1024;\nnblocks = 100;  # This is arbitrary in this example as we are not doing an actual computation.\n\nmemsize = nthreads * m * 4;\n\nCUDA.@sync @cuda threads=nthreads blocks=nblocks shmem=memsize kernel_reader_bycol(x_gpu_small);\n\nCUDA.@time @cuda threads=nthreads blocks=nblocks shmem=memsize kernel_reader_bycol(x_gpu);\n# 0.138480 seconds (24 CPU allocations: 752 bytes)\n\nIf m gets much bigger, we get an error “ERROR: Amount of dynamic shared memory exceeds device limit (400.000 KiB &gt; 48.000 KiB).” So for larger m we’d need to rework how we manipulate the data.\nLet’s close by seeing if the memory access patterns make a difference in this example. Instead of accessing by column, we’ll access by row, but with the matrix transposed so it is very wide instead of very long.\nMy initial thought was that accessing by row would be slower because adjacent threads are not reading from adjacent locations in global memory.\n\nfunction kernel_reader_byrow(x::CuDeviceArray{T}) where T\n  i = threadIdx().x;  # What thread am I within the block?\n  j = blockIdx().x;   # What block am I in?\n  idx =  (j-1)*blockDim().x + i;\n  dims = size(x);\n  \n  # Setup shared memory for the subset of data.\n  shared_data = CuDynamicSharedArray(T, (dims[1], blockDim().x));\n\n  for chunk_start = 1:blockDim().x:dims[2]\n    chunk_size = min(blockDim().x, dims[2] - chunk_start + 1);\n    ## Transfer a chunk of rows in parallel, one column per thread.\n    if i &lt;= chunk_size\n      for row in 1:dims[1]\n        shared_data[row, i] = x[row, chunk_start + i - 1];\n      end\n    end\n    sync_threads()\n    \n    # At this point we'd insert code to do the actual computation, based on `idx`.\n    # Each thread now has the opportunity to compute on all the data in the chunk in\n    # `shared_data`.\n    \n  end\n\n  return\nend\n\nn = 10000000;\nm = 10;\nx_gpu = CUDA.randn(m, n);\nx_gpu_small = CUDA.randn(m, 5);\n\nnthreads = 1024;\nnblocks = 100;  # This is arbitrary in this example as we are not doing an actual computation.\n\nmemsize = nthreads * m * 4;\n\nCUDA.@sync @cuda threads=nthreads blocks=nblocks shmem=memsize kernel_reader_byrow(x_gpu);\n\nCUDA.@time CUDA.@sync @cuda threads=nthreads blocks=nblocks shmem=memsize kernel_reader_byrow(x_gpu);\n# 0.105434 seconds (25 CPU allocations: 1008 bytes)\n\nWe see that the access by row here is (a bit) faster. I think this is because the entire chunk of data in the wide matrix lives in a small area of global memory, while in the long matrix, each column in the chunk has adjacent values but separate columns are very far apart because the matrix is so long.\nWe might be able to improve efficiency with the wide matrix by operating by column within the wide matrix. This would involve more work to manage the indexing because we wouldn’t just have each thread manage a column (unless we used very few threads, which would presumably reduce efficiency).\nClearly there’s more to investigate/understand here, but I believe that PS3 will illustrate the potential benefit of using shared memory.",
    "crumbs": [
      "Course Notes",
      "Notes 8: GPUs"
    ]
  },
  {
    "objectID": "notes/notes8.html#final-comments---when-to-use-the-gpu",
    "href": "notes/notes8.html#final-comments---when-to-use-the-gpu",
    "title": "Notes 8: GPUs",
    "section": "Final comments - when to use the GPU",
    "text": "Final comments - when to use the GPU\nTo effectively use the GPU, one generally wants to have a computation in which the same calculation is being done on many data elements. Things like the vectorized examples and matrix multiplication seen above, pixel-wise processing of images/video, simulations with many independent samples, etc.\nSituations with a lot of conditionality (if-else branching), small data sizes, and lots of data transfer to/from the GPU are generally not good use cases for the GPU.",
    "crumbs": [
      "Course Notes",
      "Notes 8: GPUs"
    ]
  },
  {
    "objectID": "notes/notes6.html",
    "href": "notes/notes6.html",
    "title": "Notes 6: JIT compilation",
    "section": "",
    "text": "This document is the sixth of a set of notes, this document focusing on the basics of Just-in-Time (JIT) compilation. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.",
    "crumbs": [
      "Course Notes",
      "Notes 6: JIT compilation"
    ]
  },
  {
    "objectID": "notes/notes6.html#introduction",
    "href": "notes/notes6.html#introduction",
    "title": "Notes 6: JIT compilation",
    "section": "",
    "text": "This document is the sixth of a set of notes, this document focusing on the basics of Just-in-Time (JIT) compilation. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.",
    "crumbs": [
      "Course Notes",
      "Notes 6: JIT compilation"
    ]
  },
  {
    "objectID": "notes/notes6.html#compilation-overview",
    "href": "notes/notes6.html#compilation-overview",
    "title": "Notes 6: JIT compilation",
    "section": "Compilation overview",
    "text": "Compilation overview\n\nCompilation and interpreters\nCompilation is the process of transforming code from one representation to another. This generally involves transforming high-level, human-readable code to lower level representations that are closer to the instructions the CPU actually performs. For example, the C compiler transforms C code to (binary) machine code (the .o or .so file(s)). One can then run the binary code directly.\nIn contrast, R or Python code is executed by an interpreter that parses the code and evaluates it. Generally the interpreter is itself a C program (e.g., CPython). When you are in an interactive environment (a “REPL”), there is the additional layer of the interactive interface (e.g., RStudio, IPython).\n\n\nJIT compilation overview\nUnlike compilation of standard non-interactive languages like C, C++, and Fortran, the Julia compilation process happens at run-time rather than in advance, hence “just-in-time”.\nThe compilation process involves a series of transformation steps, including:\n\nparsing\nmacro expansion\ntype inference\ntranslation to an intermediate representation (IR)\ntranslation to LLVM code\ngeneration of machine code\n\n\n\n\nJulia compiler steps (courtesy of the Julia manual)\n\n\nWe’ll see at the end of these notes that we can look at the output of the various steps using the macros shown in the diagram.\n\n\nLLVM\nJulia uses LLVM, which provides the middle layers of a compilation system. For example, on a Mac, the default C/C++ compiler is Clang, which is build on LLVM.\nLLVM provides an intermediate representation (IR) that is independent of the programming language and can be thought of as a high-level assembly language. By going through LLVM’s IR, one can take advantage of LLVM’s tools for code optimization.",
    "crumbs": [
      "Course Notes",
      "Notes 6: JIT compilation"
    ]
  },
  {
    "objectID": "notes/notes6.html#jit-examples",
    "href": "notes/notes6.html#jit-examples",
    "title": "Notes 6: JIT compilation",
    "section": "JIT examples",
    "text": "JIT examples\n\nMultiple dispatch\nJulia compiles version of functions for each type of inputs. Let’s see that with an example.\n\nfunction mysum(x)\n  out=(typeof(x[1]))(0)\n  n = length(x)\n  for i in 1:n\n     out += x[i]\n  end\n  return out\nend\n\nxfloat = [3.1, 5.7, 3.2]\nxfloat32 = Float32[3.1, 5.7, 3.2]\nxint = [3, 5, 4]\n\n@time mysum(xfloat)\n\n  0.010575 seconds (2.86 k allocations: 194.250 KiB, 99.75% compilation time)\n\n\n12.0\n\n\n\n@time mysum(xfloat)\n\n  0.000009 seconds (1 allocation: 16 bytes)\n\n\n12.0\n\n\n\n@time mysum(xfloat32)\n\n  0.009736 seconds (2.85 k allocations: 193.719 KiB, 99.74% compilation time)\n\n\n11.999999f0\n\n\n\n@time mysum(xint)\n\n  0.013541 seconds (2.85 k allocations: 193.688 KiB, 99.83% compilation time)\n\n\n12\n\n\n\n\nFor loop speed\nLet’s explore the timing of Julia looping to better understand the JIT compilation.\nWe’ll use the well-used example of the Monte Carlo approach to estimating \\(\\pi\\).\nHere’s a Julia implementation:\n\nin_circle = 0;\nnum_throws = 5000;\n\n# Run Monte Carlo simulation\nfor _ in 1:num_throws\n  # Generate random x and y coordinates between -1 and 1.\n  xpos = rand() * 2 - 1.0  # Equivalent to random.uniform(-1.0, 1.0)\n  ypos = rand() * 2 - 1.0\n\n  # Check if point is inside unit circle.\n  if sqrt(xpos^2 + ypos^2) &lt;= 1.0  # Equivalent to math.hypot()\n    in_circle += 1\n  end\nend\n\n# Estimate PI\npi_estimate = 4 * in_circle / num_throws\n\n3.176\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry the following:\n\nTime the for loop above without putting it into a function (@time begin ... end).\nPut the code within a function and time it using @time when running the first time.\nTime it after running it a second time\nTime a vectorized version without the loop (also within a function)\n\nFill in your timing answers in this Google form and we’ll discuss during the next class.\n\n\nHere’s some Julia code to explore this further.",
    "crumbs": [
      "Course Notes",
      "Notes 6: JIT compilation"
    ]
  },
  {
    "objectID": "notes/notes6.html#jit-and-global-versus-local-variables",
    "href": "notes/notes6.html#jit-and-global-versus-local-variables",
    "title": "Notes 6: JIT compilation",
    "section": "JIT and global versus local variables",
    "text": "JIT and global versus local variables\nUsing global variables is much less efficient than using local variables because the type of the global variable could change and that makes it hard to generate simple optimized code.\n\nfunction squaresum(x, y)\n  return x^2 + y^2\nend\n\nfunction squaresum_with_global(x)\n  return x^2 + y^2\nend\n\nsquaresum_with_global (generic function with 1 method)\n\n\n\nThe intermediate representations\nLet’s look at the LLVM (intermediate representation) code produced by the JIT compilation process.\nThe LLVM code for the basic version is simple and in fact we can read and understand it.\n\n@code_llvm squaresum(4, 3)\n\n;  @ In[7]:1 within `squaresum`\ndefine i64 @julia_squaresum_952(i64 signext %0, i64 signext %1) #0 {\ntop:\n;  @ In[7]:2 within `squaresum`\n; ┌ @ intfuncs.jl:332 within `literal_pow`\n; │┌ @ int.jl:88 within `*`\n    %2 = mul i64 %0, %0\n    %3 = mul i64 %1, %1\n; └└\n; ┌ @ int.jl:87 within `+`\n   %4 = add i64 %3, %2\n; └\n  ret i64 %4\n}\n\n\nIn contrast the LLVM code for the version with the global is quite complicated (and only partially shown below).\n\n@code_llvm squaresum_with_global(4)\n\n;  @ REPL[22]:1 within `squaresum_with_global`\ndefine nonnull {}* @julia_squaresum_with_global_590(i64 signext %0) #0 {\ntop:\n  %1 = alloca [3 x {}*], align 8\n  %gcframe2 = alloca [4 x {}*], align 16\n  %gcframe2.sub = getelementptr inbounds [4 x {}*], [4 x {}*]* %gcframe2, i64 0, i64 0\n  %.sub = getelementptr inbounds [3 x {}*], [3 x {}*]* %1, i64 0, i64 0\n  %2 = bitcast [4 x {}*]* %gcframe2 to i8*\n  call void @llvm.memset.p0i8.i64(i8* align 16 %2, i8 0, i64 32, i1 true)\n  %thread_ptr = call i8* asm \"movq %fs:0, $0\", \"=r\"() #6\n  %tls_ppgcstack = getelementptr i8, i8* %thread_ptr, i64 -8\n  %3 = bitcast i8* %tls_ppgcstack to {}****\n[... snip ...]\nLet’s time the two:\n\nusing BenchmarkTools\ny = 3;\n@btime squaresum_with_global(4);\n@btime squaresum(4, 3);\n\n  33.189 ns (0 allocations: 0 bytes)\n  1.299 ns (0 allocations: 0 bytes)\n\n\nThat’s a very simple function, so it’s very fast on an absolute basis even with the global, but clearly very slow relative to the non-global version. That makes sense given the additional code in the LLVM code for the global version.\n\n\nHelping out the compiler\nNow let’s see that using a const or typing the global variable avoids the problem.\n\nfunction squaresum_with_const_global(x)\n  return x^2 + z^2\nend\n\nconst z = 3;\n@code_llvm squaresum_with_const_global(4)\n\n;  @ In[10]:1 within `squaresum_with_const_global`\ndefine i64 @julia_squaresum_with_const_global_1132(i64 signext %0) #0 {\ntop:\n;  @ In[10]:2 within `squaresum_with_const_global`\n; ┌ @ intfuncs.jl:332 within `literal_pow`\n; │┌ @ int.jl:88 within `*`\n    %1 = mul i64 %0, %0\n; └└\n; ┌ @ int.jl:87 within `+`\n   %2 = add i64 %1, 9\n; └\n  ret i64 %2\n}\n\n\n\n@btime squaresum_with_const_global(4);\n\n  1.299 ns (0 allocations: 0 bytes)\n\n\nsing the constant brings the speed back to the version without a global variable. We can see in the LLVM version of the code that \\(z^2\\) has been computed and is hard-coded into the code. So it makes sense that it’s just as fast as the non-global version (in fact we’d expect it to be faster as there is only one squaring operation).\nHere we add a type declaration to the global.\n\nw::Int64 = 3\nfunction squaresum_with_typed_global(x)\n  return x^2 + w^2\nend\n\n@code_llvm squaresum_with_typed_global(4)\n\n;  @ In[12]:2 within `squaresum_with_typed_global`\ndefine i64 @julia_squaresum_with_typed_global_1138(i64 signext %0) #0 {\ntop:\n;  @ In[12]:3 within `squaresum_with_typed_global`\n; ┌ @ intfuncs.jl:332 within `literal_pow`\n; │┌ @ int.jl:88 within `*`\n    %1 = mul i64 %0, %0\n; └└\n  %w2 = load atomic i64*, i64** inttoptr (i64 140478571173520 to i64**) unordered, align 16\n; ┌ @ intfuncs.jl:332 within `literal_pow`\n; │┌ @ int.jl:88 within `*`\n    %unbox = load i64, i64* %w2, align 8\n    %2 = mul i64 %unbox, %unbox\n; └└\n; ┌ @ int.jl:87 within `+`\n   %3 = add i64 %2, %1\n; └\n  ret i64 %3\n}\n\n\n\n@btime squaresum_with_typed_global(4);\n\n  2.586 ns (0 allocations: 0 bytes)\n\n\nThe typed global avoids the performance issue as well. (Mostly –it is a bit slower, presumably because of the retrieval of the global variable.)\nOf course from the perspective of code style, avoiding global variables is generally a good idea, even if we’re able in this case to avoid a performance problem.\nSide note: when I was exploring this, if I change the type of the global variable, it doesn’t seem to trigger compilation or change the compiled code. So there is something I’m not understanding.",
    "crumbs": [
      "Course Notes",
      "Notes 6: JIT compilation"
    ]
  },
  {
    "objectID": "notes/notes6.html#jit-intermediate-representations",
    "href": "notes/notes6.html#jit-intermediate-representations",
    "title": "Notes 6: JIT compilation",
    "section": "JIT: intermediate representations",
    "text": "JIT: intermediate representations\nLet’s look at the various representations along the Julia compilation process.\n\n# using InteractiveUtils\n@code_lowered squaresum(3.0, 4.0)\n\n\nCodeInfo(\n1 ─ %1 = Main.:^\n│   %2 = Core.apply_type(Base.Val, 2)\n│   %3 = (%2)()\n│   %4 = Base.literal_pow(%1, x, %3)\n│   %5 = Main.:^\n│   %6 = Core.apply_type(Base.Val, 2)\n│   %7 = (%6)()\n│   %8 = Base.literal_pow(%5, y, %7)\n│   %9 = %4 + %8\n└──      return %9\n)\n\n\n\n\n@code_typed squaresum(3.0, 4.0)\n\n\nCodeInfo(\n1 ─ %1 = Base.mul_float(x, x)::Float64\n│   %2 = Base.mul_float(y, y)::Float64\n│   %3 = Base.add_float(%1, %2)::Float64\n└──      return %3\n) =&gt; Float64\n\n\n\n\n@code_llvm squaresum(3.0, 4.0)\n\n;  @ In[7]:1 within `squaresum`\ndefine double @julia_squaresum_1532(double %0, double %1) #0 {\ntop:\n;  @ In[7]:2 within `squaresum`\n; ┌ @ intfuncs.jl:332 within `literal_pow`\n; │┌ @ float.jl:411 within `*`\n    %2 = fmul double %0, %0\n    %3 = fmul double %1, %1\n; └└\n; ┌ @ float.jl:409 within `+`\n   %4 = fadd double %2, %3\n; └\n  ret double %4\n}\n\n\n\nAssembly and machine code\nHere’s the assembly (‘native’) code, which has a very close relationship with the actual binary machine code.\n\n@code_native squaresum(3.0, 4.0)\n\n    .text\n    .file   \"squaresum\"\n    .globl  julia_squaresum_1548            # -- Begin function julia_squaresum_1548\n    .p2align    4, 0x90\n    .type   julia_squaresum_1548,@function\njulia_squaresum_1548:                   # @julia_squaresum_1548\n; ┌ @ In[7]:1 within `squaresum`\n# %bb.0:                                # %top\n    push    rbp\n    mov rbp, rsp\n; │ @ In[7]:2 within `squaresum`\n; │┌ @ intfuncs.jl:332 within `literal_pow`\n; ││┌ @ float.jl:411 within `*`\n    vmulsd  xmm0, xmm0, xmm0\n    vmulsd  xmm1, xmm1, xmm1\n; │└└\n; │┌ @ float.jl:409 within `+`\n    vaddsd  xmm0, xmm0, xmm1\n; │└\n    pop rbp\n    ret\n.Lfunc_end0:\n    .size   julia_squaresum_1548, .Lfunc_end0-julia_squaresum_1548\n; └\n                                        # -- End function\n    .section    \".note.GNU-stack\",\"\",@progbits\n\n\nAnd here’s the “binary” version, though it seems like it’s still showing a version of the assembly code but with some binary annotation.\n\ncode_native(squaresum, binary=true)\n\n; WARNING: This code may not match what actually runs.\n    .text\n    .file   \"squaresum\"\n    .globl  julia_squaresum_1565            # -- Begin function julia_squaresum_1565\n    .p2align    4, 0x90\n    .type   julia_squaresum_1565,@function\njulia_squaresum_1565:                   # @julia_squaresum_1565\n; ┌ @ In[7]:1 within `squaresum`\n# %bb.0:                                # %top\n    push    rbp                             # encoding: [0x55]\n    mov rbp, rsp                        # encoding: [0x48,0x89,0xe5]\n    push    r15                             # encoding: [0x41,0x57]\n    push    r14                             # encoding: [0x41,0x56]\n    push    r13                             # encoding: [0x41,0x55]\n    push    r12                             # encoding: [0x41,0x54]\n    push    rbx                             # encoding: [0x53]\n    and rsp, -32                        # encoding: [0x48,0x83,0xe4,0xe0]\n    sub rsp, 96                         # encoding: [0x48,0x83,0xec,0x60]\n    mov qword ptr [rsp + 24], rsi       # 8-byte Spill\n                                        # encoding: [0x48,0x89,0x74,0x24,0x18]\n    vxorps  xmm0, xmm0, xmm0                # encoding: [0xc5,0xf8,0x57,0xc0]\n    vmovaps ymmword ptr [rsp + 32], ymm0    # encoding: [0xc5,0xfc,0x29,0x44,0x24,0x20]\n    #APP\n    mov rax, qword ptr fs:[0]           # encoding: [0x64,0x48,0x8b,0x04,0x25,0x00,0x00,0x00,0x00]\n    #NO_APP\n    mov r13, qword ptr [rax - 8]        # encoding: [0x4c,0x8b,0x68,0xf8]\n; │ @ In[7]:2 within `squaresum`\n    mov qword ptr [rsp + 32], 8         # encoding: [0x48,0xc7,0x44,0x24,0x20,0x08,0x00,0x00,0x00]\n    mov rax, qword ptr [r13]            # encoding: [0x49,0x8b,0x45,0x00]\n    mov qword ptr [rsp + 40], rax       # encoding: [0x48,0x89,0x44,0x24,0x28]\n    lea rax, [rsp + 32]                 # encoding: [0x48,0x8d,0x44,0x24,0x20]\n    mov qword ptr [r13], rax            # encoding: [0x49,0x89,0x45,0x00]\n    movabs  rax, 140479200408320            # encoding: [0x48,0xb8,0x00,0xb3,0xd6,0xdc,0xc3,0x7f,0x00,0x00]\n    mov qword ptr [rsp], rax            # encoding: [0x48,0x89,0x04,0x24]\n    mov qword ptr [rsp + 8], rdi        # encoding: [0x48,0x89,0x7c,0x24,0x08]\n    movabs  rax, 140479199822272            # encoding: [0x48,0xb8,0xc0,0xc1,0xcd,0xdc,0xc3,0x7f,0x00,0x00]\n    mov qword ptr [rsp + 16], rax       # encoding: [0x48,0x89,0x44,0x24,0x10]\n    movabs  r14, offset ijl_apply_generic   # encoding: [0x49,0xbe,A,A,A,A,A,A,A,A]\n                                        #   fixup A - offset: 2, value: ijl_apply_generic, kind: FK_Data_8\n    movabs  r15, 140479235733168            # encoding: [0x49,0xbf,0xb0,0xb6,0xf1,0xde,0xc3,0x7f,0x00,0x00]\n    mov r12, rsp                        # encoding: [0x49,0x89,0xe4]\n    mov rdi, r15                        # encoding: [0x4c,0x89,0xff]\n    mov rsi, r12                        # encoding: [0x4c,0x89,0xe6]\n    mov edx, 3                          # encoding: [0xba,0x03,0x00,0x00,0x00]\n    vzeroupper                              # encoding: [0xc5,0xf8,0x77]\n    call    r14                             # encoding: [0x41,0xff,0xd6]\n    mov rbx, rax                        # encoding: [0x48,0x89,0xc3]\n    mov qword ptr [rsp + 56], rax       # encoding: [0x48,0x89,0x44,0x24,0x38]\n    movabs  rax, 140479200408320            # encoding: [0x48,0xb8,0x00,0xb3,0xd6,0xdc,0xc3,0x7f,0x00,0x00]\n    mov qword ptr [rsp], rax            # encoding: [0x48,0x89,0x04,0x24]\n    mov rax, qword ptr [rsp + 24]       # 8-byte Reload\n                                        # encoding: [0x48,0x8b,0x44,0x24,0x18]\n    mov qword ptr [rsp + 8], rax        # encoding: [0x48,0x89,0x44,0x24,0x08]\n    movabs  rax, 140479199822272            # encoding: [0x48,0xb8,0xc0,0xc1,0xcd,0xdc,0xc3,0x7f,0x00,0x00]\n    mov qword ptr [rsp + 16], rax       # encoding: [0x48,0x89,0x44,0x24,0x10]\n    mov rdi, r15                        # encoding: [0x4c,0x89,0xff]\n    mov rsi, r12                        # encoding: [0x4c,0x89,0xe6]\n    mov edx, 3                          # encoding: [0xba,0x03,0x00,0x00,0x00]\n    call    r14                             # encoding: [0x41,0xff,0xd6]\n    mov qword ptr [rsp + 48], rax       # encoding: [0x48,0x89,0x44,0x24,0x30]\n    mov qword ptr [rsp], rbx            # encoding: [0x48,0x89,0x1c,0x24]\n    mov qword ptr [rsp + 8], rax        # encoding: [0x48,0x89,0x44,0x24,0x08]\n    movabs  rdi, 140479201572752            # encoding: [0x48,0xbf,0x90,0x77,0xe8,0xdc,0xc3,0x7f,0x00,0x00]\n    mov rsi, r12                        # encoding: [0x4c,0x89,0xe6]\n    mov edx, 2                          # encoding: [0xba,0x02,0x00,0x00,0x00]\n    call    r14                             # encoding: [0x41,0xff,0xd6]\n    mov rcx, qword ptr [rsp + 40]       # encoding: [0x48,0x8b,0x4c,0x24,0x28]\n    mov qword ptr [r13], rcx            # encoding: [0x49,0x89,0x4d,0x00]\n    lea rsp, [rbp - 40]                 # encoding: [0x48,0x8d,0x65,0xd8]\n    pop rbx                             # encoding: [0x5b]\n    pop r12                             # encoding: [0x41,0x5c]\n    pop r13                             # encoding: [0x41,0x5d]\n    pop r14                             # encoding: [0x41,0x5e]\n    pop r15                             # encoding: [0x41,0x5f]\n    pop rbp                             # encoding: [0x5d]\n    ret                                     # encoding: [0xc3]\n.Lfunc_end0:\n    .size   julia_squaresum_1565, .Lfunc_end0-julia_squaresum_1565\n; └\n                                        # -- End function\n    .section    \".note.GNU-stack\",\"\",@progbits",
    "crumbs": [
      "Course Notes",
      "Notes 6: JIT compilation"
    ]
  },
  {
    "objectID": "notes/notes2.html",
    "href": "notes/notes2.html",
    "title": "Notes 2: Memory and scope",
    "section": "",
    "text": "This document is the second of a set of notes, this document focusing on memory, storage of objects, and variable scope. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.",
    "crumbs": [
      "Course Notes",
      "Notes 2: Memory and scope"
    ]
  },
  {
    "objectID": "notes/notes2.html#introduction",
    "href": "notes/notes2.html#introduction",
    "title": "Notes 2: Memory and scope",
    "section": "",
    "text": "This document is the second of a set of notes, this document focusing on memory, storage of objects, and variable scope. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.",
    "crumbs": [
      "Course Notes",
      "Notes 2: Memory and scope"
    ]
  },
  {
    "objectID": "notes/notes2.html#memory-use",
    "href": "notes/notes2.html#memory-use",
    "title": "Notes 2: Memory and scope",
    "section": "Memory use",
    "text": "Memory use\n\nMutable objects\nLet’s see if Julia behaves as we would expect if we try to change objects in different ways.\n\nx = \"hello\"\nx[1] = \"a\" \n\nx = [3.1, 2.1]\nx[2] = 5.5\n\nconst tmpVar = [3.1, 2.1]\ntmpVar[2] = 5.5 \ntmpVar = \"foo\"   \n\nx = (3, 5, \"hello\")\nx[2] = 7  \n\nconst tmpVarTuple = (3, 5, \"hello\")\ntmpVarTuple = (5, 9)\n\nBe careful as const objects cannot be deleted or reassigned.\nOne nice aspect of this is that you can define a variable without fear that it will be used in some other way.\n\n\nModifying objects in place\nUse of &lt;function_name&gt;!() indicates the function operates on the inputs in place and modifies arguments (non-black box execution).\n\nt = ['a', 'b', 'c'];\npush!(t, 'd')\n\n4-element Vector{Char}:\n 'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\n 'b': ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\n 'c': ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\n 'd': ASCII/Unicode U+0064 (category Ll: Letter, lowercase)\n\npop!(t)\n\n'd': ASCII/Unicode U+0064 (category Ll: Letter, lowercase)\n\n\n\npush(t, 'e')\n\nUndefVarError: `push` not defined\n\nx = Dict(\"test\" =&gt; 3, \"tmp\" =&gt; [2.1, 3.5], 7 =&gt; \"weird\")\n\nDict{Any, Any} with 3 entries:\n  7      =&gt; \"weird\"\n  \"test\" =&gt; 3\n  \"tmp\"  =&gt; [2.1, 3.5]\n\npop!(x, \"tmp\");\npush!(x, 'b' =&gt; 3);\nx\n\nDict{Any, Any} with 3 entries:\n  7      =&gt; \"weird\"\n  \"test\" =&gt; 3\n  'b'    =&gt; 3\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSee if you can create a function that does not have “!” at the end of the name that modifies an input argument.\n\n\n\n\nMemory use and copying\nWe can use === to see if objects are identical. For mutable objects this involves looking at whether the data is stored at the same place in memory.\n\na = \"banana\"\n\n\"banana\"\n\nb = \"banana\"\n\n\"banana\"\n\na === b\n\ntrue\n\na ≡ b\n\ntrue\n\n\na = [1, 2, 3];\nb = a;\nc = [1, 2, 3];\n\na === b\n\ntrue\n\na === c\n\nfalse\n\na == c\n\ntrue\n\n\na = [1, 2, [4,7]]\n\n3-element Vector{Any}:\n 1\n 2\n  [4, 7]\n\nc = [1, 2, [3]]\n\n3-element Vector{Any}:\n 1\n 2\n  [3]\n\n\nc[3] = a[3]\n\n2-element Vector{Int64}:\n 4\n 7\n\n\na === c\n\nfalse\n\na[3] === c[3]\n\ntrue\n\n\n\n\nAliasing\nThis avoids copying but can be dangerous. The behavior is like Python, but not like R.\n\na = [1, 2, 3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\nb = a\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\na[1] = 99\n\n99\n\nb\n\n3-element Vector{Int64}:\n 99\n  2\n  3\n\n\n\n\nUsing copies rather than aliases\n\nx = [1, 2, 3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\ny = x[:]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\npop!(x)\n\n3\n\ny\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIt would be useful to know if there’s a way to make a copy of an array without having to know its dimension.\n\n\n\n\nObjects in memory\nIf we want to see the memory address of an object, you can use pointer_from_objref.\nIn which of these cases do you expect that the same object in memory is referenced?\n\na = [1, 2, 3]\nb = a\nc = [1, 2, 3]\npointer_from_objref(a)\npointer_from_objref(b)\npointer_from_objref(c)\n\ntmp = [4,7]\na = [1, 2, tmp, [4,7]]\npointer_from_objref(tmp)\npointer_from_objref(a[3])\npointer_from_objref(a[4])\n\nWhat happens with pointer_from_objref on immutable objects?\n\nx = (3,5)\npointer_from_objref(x)\n\nERROR: pointer_from_objref cannot be used on immutable objects\n\ny = \"hello\"\n\n\"hello\"\n\npointer_from_objref(y)\n\nPtr{Nothing} @0x00007f2682837a78\n\n\nThere’s a bit more info in the help for pointer_from_objref.\n\n\nPass by reference\nJulia uses pass by reference. If you pass a mutable object into a function and modify it, that affects the state of the object outside of the function; no local copy of the object is made. This is efficient in terms of copying and memory use, but it does not following functional programming principles.\n\nfunction array_modifier(x)\n    push!(x, 12)\n    return Nothing\nend\n\narray_modifier (generic function with 1 method)\n\n\ny = [1, 2, 3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\narray_modifier(y)\n\nNothing\n\ny\n\n4-element Vector{Int64}:\n  1\n  2\n  3\n 12\n\n\nWe should instead define the function as array_modifier! to be consistent with Julia’s syntax.\n\nfunction array_modifier!(x)\n    push!(x, 12)\n    return Nothing\nend\n\narray_modifier! (generic function with 1 method)\n\n\ny = [1, 2, 3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\narray_modifier!(y)\n\nNothing\n\ny\n\n4-element Vector{Int64}:\n  1\n  2\n  3\n 12\n\n\n\n\n\n\n\n\nUsing tuples to prevent side effects\n\n\n\nIf we use a tuple as input to a function, we don’t have to worry about the input being modified; any attempt at modification will throw an error.\n\n\n\n\nExample\nConsider this function, modified from a Python function that was modified from an R function that I was looking at with a student who wanted to reduce the memory use of his code.\n\nfunction fastcount(xvar, yvar)\n    naline = isnan.(xvar)\n    naline[isnan.(yvar)] .= 1\n    localx = xvar[:]\n    localy = yvar[:]\n    localx[naline] .= 0\n    localy[naline] .= 0\n    useline = .!naline\n    # Rest of code...\nend\n\nfastcount (generic function with 1 method)\n\n\nusing Random, Distributions\nn = 20;\nx = rand(Normal(), n);\ny = rand(Normal(), n);\nx[[3, 5]] .= NaN;\ny[[1, 7]] .= NaN;\nfastcount(x, y);\n\n\n\n\n\n\n\nExercise\n\n\n\nDetermine all the places where additional memory is allocated (including for any temporary arrays).\n\n\n\n\nArrays and pointers\nIf we have an array made of numbers all of the same type, the values can be stored contiguously in memory.\n\nn = Int(1e7);\nx = randn(Float32, n);\nsizeof(x)\n\n40000000\n\ntypeof(x)\n\nVector{Float32} (alias for Array{Float32, 1})\n\n\nWhat about an array where the elements are not all the same type?\n\n\n\n\n\n\nExercise\n\n\n\nWhat does the following code indicate about how arrays of heterogeneous elements are stored? And how many bytes is a pointer?\n\n\n\nx = [1.3, 2.5, 7.4, \"hello\"]\n\n4-element Vector{Any}:\n 1.3\n 2.5\n 7.4\n  \"hello\"\n\nsizeof(x)\n\n32\n\ntypeof(x)\n\nVector{Any} (alias for Array{Any, 1})\n\ndevs = randn(n);\nx[1] = devs;\nsizeof(x)\n\n32\n\n\n\npointer_from_objref(devs)\n\nPtr{Nothing} @0x00007f267f0cc010\n\npointer_from_objref(x[1])\n\nPtr{Nothing} @0x00007f267f0cc010\n\ndevs[1]\n\n0.13862731447680088\n\nx[1][1]\n\n0.13862731447680088\n\ndevs[1] = 3.0\n\n3.0\n\nx[1][1]\n\n3.0\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCheck your understanding by creating an array of matrices where each of the individual matrices are just pointers to the same underlying matrix. Modify the underlying matrix. Modify one of the matrices. See what happens.\nSide note: suppose x is a matrix. What’s the difference between [x, x], [x x], and [x; x]?",
    "crumbs": [
      "Course Notes",
      "Notes 2: Memory and scope"
    ]
  },
  {
    "objectID": "notes/notes2.html#scope",
    "href": "notes/notes2.html#scope",
    "title": "Notes 2: Memory and scope",
    "section": "Scope",
    "text": "Scope\n\nLexical scoping\nJulia uses lexical scoping, which means that lookup of non-local variables occurs in the scope in which a function is defined, not the scope from which it was called. This means that code is easier to reason about (where the behavior of a function doesn’t depend on where it is called from) and is modular.\n\n\n\n\n\n\nExercise: Lexical scoping\n\n\n\nExperiment with the following cases and make sure you understand how the lookup / scoping is working. Predict the result before running the code.\n\n\nCase 1: Will the code print 3 or 7?\n\n# Case 1\nx = 3\nfunction f2()\n    print(x)\nend\n\nfunction f()\n    x = 7\n    f2()\nend\n\nf() # what will happen?\n\nCase 2: Will the code print 3, 7, or 100?\n\nx = 3\nfunction f2()\n    print(x)\nend\n\nfunction f()\n    x = 7\n    f2()\nend\n\nx = 100\nf() # what will happen?\n\nCase 3: Will the code print 3, 7, or 100?\n\nx = 3\nfunction f()\n    function f2()\n        print(x)\n    end\n    x = 7\n    f2()\nend\n\nx = 100\nf() # what will happen?\n\nCase 4: Will the code print 3 or 100 or produce an error?\n\nx = 3\nfunction f()\n    function f2()\n        print(x)\n    end\n    f2()\nend\n\nx = 100\nf() # what will happen?\n\n\n\nClosures\nHere’s a somewhat tricky scoping example:\n\ny = 100\nfunction fun_constructor()\n    y = 10\n    function g(x)\n        return x + y\n    end\n    return g\nend\n\n## fun_constructor() creates functions\nmyfun = fun_constructor()\nmyfun(3)\n\n\n\n\n\n\n\nExercise: Lexical scoping\n\n\n\nTry to understand what is going on with fun_constructor. What do you expect myfun(3) to return? Where is myfun defined?\nExtra: modify fun_constructor in such a way that you can determine if g can modify y in the enclosing scope.\n\n\nThe above is an example of a closure, a useful concept in functional programming that provides functionality similar to object-oriented programming. y is “bound” up/captured in the enclosing scope of g/myfun.",
    "crumbs": [
      "Course Notes",
      "Notes 2: Memory and scope"
    ]
  },
  {
    "objectID": "notes/notes2.html#global-and-local-scopes",
    "href": "notes/notes2.html#global-and-local-scopes",
    "title": "Notes 2: Memory and scope",
    "section": "Global and local scopes",
    "text": "Global and local scopes\n\nGlobal and local variables\nWe can access global variables from within functions via Julia’s scoping rules, as seen previously.\nTo modify global variables, we need to use global.\n\nx = 100\n\n100\n\n\nfunction test()\n  global x\n  println(x)\n  x = 3;\n  return nothing\nend\n\ntest (generic function with 1 method)\n\n\ntest()\n\n100\n\nprint(x)\n\n3\n\n\nThis is like Python. Also note the difference in behavior compared to being able to modify the captured variable in the closure without any explicit syntax.\nInterestingly, this doesn’t work to be able to access both a local and global variable of the same name. Nor does it work in Python, but it does work in R. Clearly there is some processing that determines assignment operations before the code in the function body is evaluated.\n\ny = 100\n\nfunction test()\n  println(y)\n  y = 3;\n  return nothing\nend\n\ntest()\n\nERROR: UndefVarError: `y` not defined\nNote that use of global in Section 8 (Looping and Counting) of Think Julia seems incorrect/unnecessary.\n\n\nModules\nYou can isolate code from your working context using module.\n\nx = 0;\n\nmodule testmod\n  x = 99;\nend\n\ntestmod.x\n\n99\n\nx\n\n0\nEach module has its own global scope. And each code block has its own local scope (as we saw with the for loop in Notes 1).\n\nfor i in 1:3\n  tmp = i*7\nend\n\nprint(tmp)\n\nERROR: UndefVarError: `tmp` not defined\n\nprint(i)\n\nERROR: UndefVarError: `i` not defined\nScoping gets rather more complicated.\nThe use of using adds variables from a package/module to the current scope.\n\nA = rand(3, 3);\neigvals(A)\n\nERROR: UndefVarError: `eigvals` not defined\n\nA = rand(3, 3);\nusing LinearAlgebra\neigvals(A)\n\n3-element Vector{Float64}:\n -0.37070516708703843\n -0.35038525717381275\n  1.5056321634365166\n\n\n\n\nLet blocks\nYou can also use let to create a new scope:\n\nx = 0\n\n0\n\n\nlet x = 5\n  print(x)\nend\n\n5\n\n\nprint(x)\n\n0",
    "crumbs": [
      "Course Notes",
      "Notes 2: Memory and scope"
    ]
  },
  {
    "objectID": "notes/notes4.html",
    "href": "notes/notes4.html",
    "title": "Notes 4: Managing Julia",
    "section": "",
    "text": "This document is the fourth of a set of notes, this document focusing on managing and interacting with Julia, including working with and creating Julia packages. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader. Note that for many of the package/project-related demos, I haven’t run the code as part of the document rendering as the output is large and it affects that state of Julia in ways that are not helpful.\n\n\n\n\n\n\nWarning\n\n\n\nThere’s a lot of configuration details in the section on using packages. It’s not critical to absorb it all at this point, but it’s here for when you need it. In part I went into detail as I was trying to understand the details myself.",
    "crumbs": [
      "Course Notes",
      "Notes 4: Managing Julia"
    ]
  },
  {
    "objectID": "notes/notes4.html#introduction",
    "href": "notes/notes4.html#introduction",
    "title": "Notes 4: Managing Julia",
    "section": "",
    "text": "This document is the fourth of a set of notes, this document focusing on managing and interacting with Julia, including working with and creating Julia packages. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader. Note that for many of the package/project-related demos, I haven’t run the code as part of the document rendering as the output is large and it affects that state of Julia in ways that are not helpful.\n\n\n\n\n\n\nWarning\n\n\n\nThere’s a lot of configuration details in the section on using packages. It’s not critical to absorb it all at this point, but it’s here for when you need it. In part I went into detail as I was trying to understand the details myself.",
    "crumbs": [
      "Course Notes",
      "Notes 4: Managing Julia"
    ]
  },
  {
    "objectID": "notes/notes4.html#style-guide",
    "href": "notes/notes4.html#style-guide",
    "title": "Notes 4: Managing Julia",
    "section": "Style guide",
    "text": "Style guide\nSome highlights from the Julia style guide include:\n\n4 spaces per indentation level\nAppend ! to names of functions that modify inputs. Mutated input(s) should appear first in the arguments.\nType and module names: use capitalized/UpperCamelCase.\nFunction names: use lower-case squashed names (with underscores/snake_case as needed) (e.g., isequal or possibly is_equal).\nUse isa and &lt;: rather than == for type comparisons.\nFor hard-coded numbers, use Ints rather than Floats for integer-valued numbers, e.g., 2 * x rather than 2.0 * x. (The latter will unnecessarily force conversion to Float when x is an Int.)",
    "crumbs": [
      "Course Notes",
      "Notes 4: Managing Julia"
    ]
  },
  {
    "objectID": "notes/notes4.html#file-operations-io",
    "href": "notes/notes4.html#file-operations-io",
    "title": "Notes 4: Managing Julia",
    "section": "File operations (I/O)",
    "text": "File operations (I/O)\nHere are some basic examples of reading data from files.\n\nfilename = joinpath(\"..\", \"data\", \"cpds.csv\")\nlines = readlines(filename)\n\nusing CSV, DataFrames\n\ndata = CSV.read(filename, DataFrame);\ntypeof(data.year)\n\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\ntypeof(data.country)\n\n\nPooledVector{String15, UInt32, Vector{UInt32}} (alias for PooledArrays.PooledArray{String15, UInt32, 1, Array{UInt32, 1}})",
    "crumbs": [
      "Course Notes",
      "Notes 4: Managing Julia"
    ]
  },
  {
    "objectID": "notes/notes4.html#packages",
    "href": "notes/notes4.html#packages",
    "title": "Notes 4: Managing Julia",
    "section": "Packages",
    "text": "Packages\nA package is the way that code is distributed as a bundle of add-on functionality to a language. Julia packages contain source code files, tests, documentation, and metadata (e.g., about dependence on other packages).\n\nUsing packages\nWe can make variables/functions from packages available with either using or import. using brings all the variables into the current namespace, while import makes them available via module.variable. So import is cleaner and safer in terms of name conflicts.\nI’m having some trouble getting this to display correctly in the rendered document, so the code chunks below have not been run.\n\nA = rand(3, 3);\neigvals(A)\n\nLoadError: UndefVarError: `eigvals` not defined\nUndefVarError: `eigvals` not defined\n\nStacktrace:\n [1] top-level scope\n   @ In[4]:2\n\n\n\nimport LinearAlgebra\neigvals(A)\n\nLoadError: UndefVarError: `eigvals` not defined\nUndefVarError: `eigvals` not defined\n\nStacktrace:\n [1] top-level scope\n   @ In[5]:2\n\n\n\nLinearAlgebra.eigvals(A)\n\n3-element Vector{Float64}:\n -0.06241509371362185\n  0.2918476413970819\n  1.63255763020013\n\n\nIf we want to access variables/functions from a file, we can use include, which will evaluate the file contents in the global scope. This is used a lot in packages to allow the package code to be split up into multiple files.\n\ninclude(\"linecount.jl\")\nlinecount(\"notes4.qmd\")\n\n594\n\n\n\n\nInstalling and managing packages on your system\nThe examples below show how things work on the SCF Linux machines, but the situation should be similar on your personal machine.\n\nDepots\nA depot is where installed packages, projects/environments, and various other things are stored.\nYou can see the various depots via\n\nBase.DEPOT_PATH\n\n3-element Vector{String}:\n \"/accounts/vis/paciorek/.julia\"\n \"/system/linux/julia-1.10.4/local/share/julia\"\n \"/system/linux/julia-1.10.4/share/julia\"\n\n\n\n\nPackage installation locations\nJulia will look in various locations for packages that are available to be used. The locations can be seen via\n\nBase.load_path()\n\n3-element Vector{String}:\n \"/accounts/vis/paciorek/.julia/environments/v1.10/Project.toml\"\n \"/system/linux/julia-1.10.4/share/julia/stdlib/v1.10\"\n \"/usr/local/linux/julia-1.10.4/share/julia/environments/v1.10/Project.toml\"\n\n\nNote that this includes the active project, the standard library, and (in some cases) a system project. More on projects in a bit.\nJulia comes with some packages pre-installed. These include LinearAlgebra, Distributed, Random, and some others.\nYou can see them in the stdlib directory in the main system depot, e.g.,\nls /usr/local/linux/julia-1.10.4/share/julia/stdlib/v1.10/\nPackages that you (or a system administrator) install will be in the packages subdirectory of the depots.\nFor example, for packages you install:\nls ~/.julia/packages\nIn some cases (probably not on your personal machine) there will be system-installed packages:\nls /usr/local/linux/julia-1.10.4/share/julia/packages\nAs you’d expect, a given package will have various versions, and multiple versions can be installed at the same time.\ngrep version ~/.julia/packages/BenchmarkTools/*/Project.toml\nIn general, Julia will avoid installing multiple copies of the same package version in different locations on a machine/system.\n\n\nPrecompilation\nJulia will often precompiled packages. This is separate from runtime just-in-time (JIT) compilation of code. I have not had a chance to understand what the precompilation is doing.\n\n\n\nProjects/environments\nA project (or environment) defines a set of packages and their versions. An important aspect of this is reproducibility.\nThe Project.toml file for a project gives high-level information about a project, including the direct package dependencies.\ncat ~/.julia/environments/v1.10/Project.toml\nThe Manifest.toml file gives the detailed dependency information, including package versions for all direct and indirect package dependencies.\ncat ~/.julia/environments/v1.10/Manifest.toml\nThese files can be handled via version control to ensure reproducibility.\nAs noted previously, multiple projects can make use of the same installed package version.\n\n\nPkg: the Julia package manager\nThe package manager allows you to activate projects and add packages to projects, among other things.\nYou can use the package manager either in the Pkg REPL by pressing ], or by calling the Pkg API via Pkg., after using Pkg. (If you use ], you can use the Backspace key to get back to the regular Julia REPL.)\nHere we’ll use the API as I think that makes the demo clearer.\n\nActivating a project\nAnytime you use Julia, you’ll be working in the context of a project. By default this will be your default project, which is located at ~/.julia/environments/&lt;version&gt;.\n\nusing Pkg\n\nBase.active_project()\nPkg.status()\n\nStatus `~/.julia/environments/v1.10/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.5.0\n  [078ea971] CPpkg v1.0.0-DEV `/accounts/vis/paciorek/.julia/dev/CPpkg#main`\n  [336ed68f] CSV v0.10.14\n  [052768ef] CUDA v5.4.2\n  [a93c6f00] DataFrames v1.6.1\n  [31a5f54b] Debugger v0.7.9\n  [31c24e10] Distributions v0.25.109\n  [e2ba6199] ExprTools v0.1.10\n  [6a86dc24] FiniteDiff v2.23.1\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.24.2\n⌃ [682c06a0] JSON v0.21.3\n  [30363a11] NetCDF v0.12.0\n  [14b8a8f1] PkgTemplates v0.7.50\n  [91a5bcdd] Plots v1.40.4\n  [c46f51b8] ProfileView v1.7.2\n  [4c0109c6] QuartoNotebookRunner v0.12.0 `https://github.com/PumasAI/QuartoNotebookRunner.jl.git#main`\n  [6f49c342] RCall v0.14.1\n  [276daf66] SpecialFunctions v2.4.0\n  [a8a75453] StatProfilerHTML v1.6.0\n  [fd094767] Suppressor v0.2.7\n  [7feac75d] TestPkg v1.0.0-DEV `/accounts/vis/paciorek/.julia/dev/TestPkg#main`\nInfo Packages marked with ⌃ have new versions available and may be upgradable.\n\n\nWe can activate (and create if needed) a project (results not shown):\n\nPkg.activate(\"MyNewProject\")\nPkg.status()\n\n\n\nAdding packages to a project\nBefore you can invoke using or import to access a package for the first time in the context of a project, you’ll need to “add” the package to the project (but see caveats in the next section).\n\nusing Pkg\nPkg.add(\"BenchmarkTools\")\n# Or to add a specific version:\n# Pkg.add(name = \"BenchmarkTools\", version = \"1.5.0\")\nusing BenchmarkTools\n\nJulia will prompt you to add a package if it’s not part of a project. And if the version of the package is not installed anywhere on your system, it will download and install it (to ~/.julia/packages). Information about what packages/versions are available and where they are on the internet is contained in a registry.\n\n\n\nIsolated projects\nI don’t fully understand the reasoning, but if you activate a project, you still have access to packages from your default project and (if it exists) the system default project. So this seems to make the project not fully isolated. According to the Julia docs, this is because those default projects are shared environments.\nIn the following, note that I never added JSON to the active project, but it’s accessible from my default project.\n\nBase.load_path()\n\n3-element Vector{String}:\n \"/accounts/vis/paciorek/.julia/environments/v1.10/Project.toml\"\n \"/system/linux/julia-1.10.4/share/julia/stdlib/v1.10\"\n \"/usr/local/linux/julia-1.10.4/share/julia/environments/v1.10/Project.toml\"\n\n\n\nusing JSON\n\nSo good practice when trying to isolate a project is probably to use Pkg.add to add any packages you use to your project, rather than relying on those packages being available from elsewhere on the load path. Alternatively you may want to avoid adding packages to the default project entirely.\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new project. Add some of the packages that we’ve used so far to the project (e.g, Distributions and Plots). Switch back and forth between your new project and the default project. Figure out where the packages are installed on your filesystem.",
    "crumbs": [
      "Course Notes",
      "Notes 4: Managing Julia"
    ]
  },
  {
    "objectID": "notes/notes4.html#creating-packages",
    "href": "notes/notes4.html#creating-packages",
    "title": "Notes 4: Managing Julia",
    "section": "Creating packages",
    "text": "Creating packages\nA package is the way of distributing additional Julia functionality. It generally contains modules (containing code), tests, and documentation. You can also register your package to the Julia General Registry, so that it is easily available via Pkg.add.\n\nCreating your package\nThe Julia documentation provides a nice overview of creating a package. One standard way is to use the PackageTemplates package.\n\nusing PkgTemplates\n\nt = Template(;\n           user=\"paciorek\",\n           authors=[\"Christopher Paciorek\"],\n           plugins=[\n               License(name=\"MIT\"),\n               Git(),\n               GitHubActions(),\n           ],\n)\n\nt(\"CPpkg\") \n\nRunning t(CPpkg) produces this output, indicating what is happening on the filesystem:\njulia&gt; t(\"CPpkg\")\n[ Info: Running prehooks\n[ Info: Running hooks\n  Activating project at `~/.julia/dev/CPpkg`\n  No Changes to `~/.julia/dev/CPpkg/Project.toml`\n  No Changes to `~/.julia/dev/CPpkg/Manifest.toml`\nPrecompiling project...\n  1 dependency successfully precompiled in 1 seconds\n        Info We haven't cleaned this depot up for a bit, running Pkg.gc()...\n      Active manifest files: 3 found\n      Active artifact files: 139 found\n      Active scratchspaces: 5 found\n     Deleted 5 package installations (1.827 MiB)\n  Activating project at `~/.julia/environments/v1.10`\n[ Info: Running posthooks\n[ Info: New package is at /accounts/vis/paciorek/.julia/dev/CPpkg\n\"/accounts/vis/paciorek/.julia/dev/CPpkg\"\n\n\nParts of the package\n\nCode in src.\nWe’ll look at the basic structure of code in src as outlined in the documentation.\nThen we’ll add some functionality to src/CPpkg.jl or to another file that we include via include(\"functions.jl\").\n\n\nGit repository\nRunning the template created a Git repository for the package. Nice.\ncd ~/.julia/dev/CPpkg\ngit status\nls -l .git\nNote that because a Git repository was created, we need to commit any changes to src to have them reflected in the package when we use it in Julia (in a moment).\ngit add src/CPpkg.jl\ngit commit -m'Add initial test function.'\n\n\nTesting\nRunning the template created a skeleton of a test directory. Also nice.\nEven nicer, it created the GitHub Actions workflow (.github/workflows/CI.yml) so that your tests would run via continuous integration on GitHub whenever you push changes to the main branch of the repository.\n\n\nAccessing a local package\nWe can try out the (local) package like this:\n\nPkg.add(path=\"/accounts/vis/paciorek/.julia/dev/CPpkg\")\nusing CPpkg\ntest()\n\nIt looks like we add a package directly from GitHub like this:\n\nPkg.add(url=\"https://github.com/JuliaLang/Example.jl\", rev=\"master\") \n\n\n\n\nLooking at an example package\nLet’s look at the BenchmarkTools package as an example package whose structure looks fairly straightforward. The file src/BenchmarkTools.jl is the entry point that defines the package and pulls in code from the other source files.\n\nDocumentation\nIt appears that ?BenchmarkTools shows the contents of README.md. There is also information in docs, but I haven’t figured out the details.\nWe can see docstrings in the src files though some are fairly brief. execution.jl shows the extensive docstring for @benchmark though it uses different syntax than seen earlier in these notes, using @doc raw\"\"\" and julia-repl.\n\n\nExported objects\nIf we look in BenchmarkTools.jl (in ~/.julia/packages/BenchmarkTools/&lt;version_hash&gt;/src), we see that loadparams! from parameters.jl is exported, but estimate_overhead is not. The exports define the user interface or “API” of the package.\n\nusing BenchmarkTools\nloadparams!\n\nloadparams! (generic function with 3 methods)\n\n\n\nestimate_overhead\n\nLoadError: UndefVarError: `estimate_overhead` not defined\nUndefVarError: `estimate_overhead` not defined\n\n\nWe can access estimate_overhead in the BenchmarkTools module:\n\nimport BenchmarkTools\nBenchmarkTools.estimate_overhead\n\nestimate_overhead (generic function with 1 method)\n\n\n\n\nAdditional modules\nOne can organize code within a package into multiple modules nested within the main module for the package.\nFor example, in the LinearAlgebra package (part of Julia’s standard library), src/blas.jl contains the BLAS module. To make it available, LinearAlgebra.jl uses include(\"blas.jl\") and export BLAS. Here’s how we can access the BLAS (sub)module:\n\nimport LinearAlgebra\nLinearAlgebra.BLAS.get_num_threads()\n\n4\n\n\nor\n\nusing LinearAlgebra\nBLAS.get_num_threads()\n\n4\n\n\n\n\n\n\n\n\nExercise\n\n\n\nExplore the structure of a package of interest to you in ~/.julia/packages. (You’ll need to add the package to invoke installation if it’s not already part of one of your projects.)",
    "crumbs": [
      "Course Notes",
      "Notes 4: Managing Julia"
    ]
  },
  {
    "objectID": "notes/notes4.html#errors-and-messaging",
    "href": "notes/notes4.html#errors-and-messaging",
    "title": "Notes 4: Managing Julia",
    "section": "Errors and messaging",
    "text": "Errors and messaging\n\nTracebacks\nWe can see the function frames if we force an error to occur.\nHere we have a function that uses recursion, but the same thing happens when the nested function calls are not recursive.\n\nfunction factorial(x)\n  if x == 0\n    sqrt(-1)\n    return 1\n  else\n    return x*factorial(x-1)\n  end\nend\n\nfactorial(3)\n\nLoadError: DomainError with -1.0:\nsqrt was called with a negative real argument but will only return a complex result if called with a complex argument. Try sqrt(Complex(x)).\nDomainError with -1.0:\nsqrt was called with a negative real argument but will only return a complex result if called with a complex argument. Try sqrt(Complex(x)).\n\nStacktrace:\n [1] throw_complex_domainerror(f::Symbol, x::Float64)\n   @ Base.Math ./math.jl:33\n [2] sqrt\n   @ ./math.jl:686 [inlined]\n [3] sqrt(x::Int64)\n   @ Base.Math ./math.jl:1578\n [4] factorial(x::Int64)\n   @ Main ./In[18]:3\n [5] factorial(x::Int64) (repeats 3 times)\n   @ Main ./In[18]:6\n [6] top-level scope\n   @ In[18]:10\n\n\n\n\nFinding the source code\nIf I wanted to dig into the source code to better understand an error or try to understand how it works, here’s an example of finding the functions indicated in the traceback:\npaciorek@gandalf:~&gt; type julia\njulia is /usr/local/linux/julia-1.10.4/bin/julia\npaciorek@gandalf:~&gt; cd /usr/local/linux/julia-1.10.4\npaciorek@gandalf:/usr/local/linux/julia-1.10.4&gt; find . -name math.jl\n./share/julia/base/math.jl\n./share/julia/test/math.jl\n\n\nTry-catch\nOne standard way to make your code more robust is to anticipate where errors may occur and give informative messaging and/or proceed despite the error.\n\ntry\n    data = open(\"bad_file.txt\")\ncatch exc\n    println(\"Something went wrong: $exc\")\nend\n\nSomething went wrong: SystemError(\"opening file \\\"bad_file.txt\\\"\", 2, nothing)\n\n\n\n\nWarnings and logging\nYou can print out useful information with @info, @warn, and @error, going from information messages to more serious issues. The latter doesn’t actually throw a formal exception (error) and stop execution, it just indicates a less extreme error from which the code can still proceed.\nUsers can then control the level of logging information that they see. In this example we set the logging level for warnings and more serious messages, so the Info message is not shown. (This code is not run here because the rendering process is causing problems.)\n\nfunction test() \n    println(\"Hello from test.\")\n    @info \"Some info\"\n    @warn \"A warning\"\n    @error \"An error\"\nend\n\nusing Logging\n\n# Show warning messages and above (this excludes info messages)\nglobal_logger(ConsoleLogger(stderr, Logging.Warn))\n\ntest()\n\nThese are examples of macros, which start with @.\n\n\nDebugging statements\n\nx = (3,5)\n@show x\n\n@debug \"The sum of some values $(sum(rand(100)))\"\n\nx = (3, 5)\n\n\nThe debug statement is only run if debugging is enabled (e.g., via JULIA_DEBUG=all when invoking Julia or in the Julia interactive session).\n\nENV[\"JULIA_DEBUG\"] = \"all\"\n@debug \"The sum of some values $(sum(rand(100)))\"\n\n┌ Debug: The sum of some values 56.41687419126446\n└ @ Main In[21]:2\n\n\nWe’ll see the Julia debugger (and possibly Julia debugging in VS Code) later.",
    "crumbs": [
      "Course Notes",
      "Notes 4: Managing Julia"
    ]
  },
  {
    "objectID": "notes/notes4.html#assertions",
    "href": "notes/notes4.html#assertions",
    "title": "Notes 4: Managing Julia",
    "section": "Assertions",
    "text": "Assertions\nOne can use assertions to check values for robustness or as sanity checks while developing code.\n\nfunction mysum(x, y)\n  @assert(isa(x, Number) && isa(y, Number), \"non-numeric inputs\")\n  return x + y\nend\n\nmysum (generic function with 1 method)\n\n\nOf course that’s not a good example as we can more robustly and elegantly deal with type checking by declaring types for the function arguments!\nFor errors that may arise because of user inputs, one generally wants to use exceptions.\n\nExceptions\nJulia has a variety of kinds of runtime errors (aka exceptions).\nHere’s a DomainError.\n\nsqrt(-1)\n\nLoadError: DomainError with -1.0:\nsqrt was called with a negative real argument but will only return a complex result if called with a complex argument. Try sqrt(Complex(x)).\nDomainError with -1.0:\nsqrt was called with a negative real argument but will only return a complex result if called with a complex argument. Try sqrt(Complex(x)).\n\nStacktrace:\n [1] throw_complex_domainerror(f::Symbol, x::Float64)\n   @ Base.Math ./math.jl:33\n [2] sqrt\n   @ ./math.jl:686 [inlined]\n [3] sqrt(x::Int64)\n   @ Base.Math ./math.jl:1578\n [4] top-level scope\n   @ In[23]:1\n\n\nAnd here’s how we can trap an error and throw an exception (again a DomainError) ourselves.\n\nfunction mysqrt(x)\n   if x &lt; 0\n      throw(DomainError(x, \"sqrt: input must be non-negative.\"))\n   else\n      return sqrt(x)\n   end\nend\n\nmysqrt(-3)\n\nLoadError: DomainError with -3:\nsqrt: input must be non-negative.\nDomainError with -3:\nsqrt: input must be non-negative.\n\nStacktrace:\n [1] mysqrt(x::Int64)\n   @ Main ./In[24]:4\n [2] top-level scope\n   @ In[24]:10",
    "crumbs": [
      "Course Notes",
      "Notes 4: Managing Julia"
    ]
  },
  {
    "objectID": "notes/notes4.html#documentation-1",
    "href": "notes/notes4.html#documentation-1",
    "title": "Notes 4: Managing Julia",
    "section": "Documentation",
    "text": "Documentation\nDocstrings come before the function in triple quotes. Note that I figured out the syntax below (e.g., use of jldoctest) by looking at Julia source code.\n\"\"\"\n    times3(x)\n\nMultiplies the input by three.\n\nThe return type is the same as the input type.\n\n# Examples\n```jldoctest\njulia&gt; x = 7.5;\njulia&gt; times3(x)\n22.5\n\njulia&gt; x = [7.5, 3];\njulia&gt; times3.(x)\n2-element Vector{Float64}:\n 22.5\n  9.0\n```\n\"\"\"\nfunction times3(x)\n  return 3*x\nend\nIf you do that, you can then do ?times3 to see the docstring. Nice.\n\n\n\n\n\n\nTips for docstrings\n\n\n\n\nAs many of you have probably discoverd using a Chatbot is a good way to get a first draft of a doc string.\nFollowing the format/style of docstrings for base Julia functions is a good idea.\nProvide examples! (You know that when you look at docstrings, that’s often what you want.)",
    "crumbs": [
      "Course Notes",
      "Notes 4: Managing Julia"
    ]
  },
  {
    "objectID": "notes/notes4.html#testing-1",
    "href": "notes/notes4.html#testing-1",
    "title": "Notes 4: Managing Julia",
    "section": "Testing",
    "text": "Testing\nWe’ll cover testing separately.\nYou can run the tests for an existing package like this:\n\nPkg.test(\"CSV\")",
    "crumbs": [
      "Course Notes",
      "Notes 4: Managing Julia"
    ]
  },
  {
    "objectID": "notes/notes4.html#macros",
    "href": "notes/notes4.html#macros",
    "title": "Notes 4: Managing Julia",
    "section": "Macros (@)",
    "text": "Macros (@)\nMacros modify existing code or generate new code. They’re a convenient shortcut to do a variety of things.\nWe’ve already seen assertions related to messaging.\nHere’s a basic example of timing some code:\n\n@time 3 + 7;\n\n  0.000001 seconds\n\n\nThe @. broadcasting macro ‘distributes’ the . broadcasting to all operations. These two lines of code are equivalent.\n\n@. x^2 + y^2 ≤ 1\nx.^2 .+ y.^2 .≤ 1\n\nMacros execute when the Julia code is parsed, before the code is actually run.",
    "crumbs": [
      "Course Notes",
      "Notes 4: Managing Julia"
    ]
  },
  {
    "objectID": "howtos/accessJulia.html",
    "href": "howtos/accessJulia.html",
    "title": "Accessing Julia",
    "section": "",
    "text": "Julia should be straightforward to install on your own computer.\nJulia is also available on all the SCF machines. If you’re in a shell/terminal, you’ll first need to run module load julia.",
    "crumbs": [
      "How tos",
      "Accessing Julia"
    ]
  },
  {
    "objectID": "howtos/accessJulia.html#installing-and-accessing-julia",
    "href": "howtos/accessJulia.html#installing-and-accessing-julia",
    "title": "Accessing Julia",
    "section": "",
    "text": "Julia should be straightforward to install on your own computer.\nJulia is also available on all the SCF machines. If you’re in a shell/terminal, you’ll first need to run module load julia.",
    "crumbs": [
      "How tos",
      "Accessing Julia"
    ]
  },
  {
    "objectID": "howtos/accessJulia.html#using-julia-packages",
    "href": "howtos/accessJulia.html#using-julia-packages",
    "title": "Accessing Julia",
    "section": "Using Julia packages",
    "text": "Using Julia packages\nOne gets access to a Julia package using using &lt;packageName&gt; (or import &lt;packageName).\nIf the package is not part of your project (all Julia work is done in a project), you’ll need to add it like this, using the BenchmarkTools package as an example:\nusing Pkg\nPkg.add(\"BenchmarkTools\")\nIf the package is not installed on the machine, Julia will prompt you to install it. Packages will generally be installed in ~/.julia/packages (where ~ is shorthand for the location of your home directory).",
    "crumbs": [
      "How tos",
      "Accessing Julia"
    ]
  },
  {
    "objectID": "howtos/accessJulia.html#using-julia-in-jupyter-notebooks",
    "href": "howtos/accessJulia.html#using-julia-in-jupyter-notebooks",
    "title": "Accessing Julia",
    "section": "Using Julia in Jupyter notebooks",
    "text": "Using Julia in Jupyter notebooks\nTo use Julia in a notebook on the SCF JupyterHub, you can just select Julia as the kernel from the dropdown in the top right.\nTo use Julia in a notebook on your own machine, you need to install the Julia kernel for Jupyter.\nusing Pkg\nPkg.add(\"IJulia\")\ninstallkernel(\"Julia\")\nThat should create a kernel called “Julia” that you can select as the kernel when you are in a Jupyter notebook.",
    "crumbs": [
      "How tos",
      "Accessing Julia"
    ]
  },
  {
    "objectID": "howtos/installPython.html",
    "href": "howtos/installPython.html",
    "title": "Installing Python",
    "section": "",
    "text": "The focus of the course is Julia, but I plan to spend a small amount of time on the use of GPUs via Python. You can use Python through the SCF (and you’ll need to do so for GPU access), but if you do want install Python on your own machine, here is some information\nWe recommend using the Miniforge distribution as your Python 3.12 installation.\nOnce you’ve installed Python, please install the following packages:\n\nnumpy\nscipy\npandas\njax\npytorch\n\nAssuming you installed Miniforge, you should be able to do this from the command line:\nconda install -c conda-forge numpy scipy pandas jax\nconda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia",
    "crumbs": [
      "How tos",
      "Installing Python"
    ]
  },
  {
    "objectID": "howtos/useQuarto.html",
    "href": "howtos/useQuarto.html",
    "title": "Using Quarto",
    "section": "",
    "text": "Quarto is a system for publishing technical content. We’ll use with source files written in the Quarto flavor of Markdown, which can include mathematical notation using LaTeX syntax and code chunks that are evaluated during the publishing process. One then “renders” the source file into either HTML or PDF.",
    "crumbs": [
      "How tos",
      "Using Quarto"
    ]
  },
  {
    "objectID": "howtos/useQuarto.html#installing-and-using-quarto",
    "href": "howtos/useQuarto.html#installing-and-using-quarto",
    "title": "Using Quarto",
    "section": "Installing and using Quarto",
    "text": "Installing and using Quarto\nUnless you plan to generate your problem set solutions on the SCF, you’ll need to install Quarto.\nOnce installed, you should be able to run commands such as quarto render FILE and quarto preview FILE from the command line.\nTo render to PDF, you’ll need a LaTeX engine installed. A good minimal solution if don’t already have LaTeX installed (e.g., via MicTeX in Windows or MacTeX on MacOS) is to install tinytex: quarto install tinytex.\nQuarto also runs from the Windows Command shell or PowerShell.\nquarto convert converts back and forth between the Jupyter notebook (.ipynb) and Quarto Markdown (.qmd) formats. So if you prefer, you can develop in a notebook and then convert to qmd and then render to prepare a nice-looking PDF for problem set/project/presentation submission.\nThe Quarto manual has more details on using Quarto specifically in the context of using Julia.\n\nExample Quarto document\nHere’s example content of a Quarto document. The first part is the YAML header/metadata giving details of how the document should be processed.\n---\ntitle: \"Problem Set 1 Solutions\"\nauthor: \"Chris Paciorek\"\ndate: \"2025-01-21\"\nengine: jupyter\n---\n\nHere's some math: $\\int \\pi(\\theta)d\\theta = 1$.\n\nHere's a code chunk that is evaluated.\n\n```{julia}\nx = 3;\nprintln(\"The result is $(x*7).\")\n```\nMore details are available in the Quarto manual, including options for controlling the output from code chunks.\n\n\nRendering engines\nBy default, Quarto uses Jupyter to process code chunks. (Note that engine: jupyter is specified above in the metadata but is not needed.)\nThere are other rendering engines one can use to process the code chunks, with various advantages and disadvantages.\n\nJupyter engine\nThe Jupyter engine requires that the IJulia kernel installed – see the howto on accessing Julia.\nTo use a specific kernel, you can replace engine: jupyter with jupyter: &lt;kernelname&gt;, where &lt;kernelname&gt; is the name of the Jupyter kernel to be used. On the SCF, this could be jupyter: julia-1.10 as there is a kernel named julia-1.10.\nSome downsides of this engine are:\n\nAll output from a chunk is printed after the chunk rather than immediately after the line of code causing the output.\nOnly the output from the last line of code in a chunk is printed out (except if print() is used explicitly).\n\n\n\nJulia engine\nThere is also now a Julia engine for Quarto. To use it, simply have engine: julia in the metadata.\nThe rendering uses the QuartoNotebookRunner.jl package. It will supposedly install it in an isolated way, and that worked on my laptop, but I had package version incompatibilities on the SCF.\nSome downsides of this engine, as with the Jupyter engine, are:\n\nAll output from a chunk is printed after the chunk rather than immediately after the line of code causing the output.\nOnly the output from the last line of code in a chunk is printed out (except if print() is used explicitly).\n\n\n\nKnitr engine (via R)\nThe knitr engine requires that you have R installed on your computer, with the rmarkdown R package installed.\nTo use it, simply have engine: knitr in the metadata.\nSome upsides of this engine involve having the output from the code chunks print out nicely:\n\nOutput from a chunk is interspersed with the line of code creating the output.\nOutput from all lines is printed out.\n\nHowever, I have had some difficulties with the knitr engine, including\n\nErrors when trying to have plots included in the output.\nHaving output from simply invoking an object name (e.g., x rather than print(x)) sometimes appear in the terminal/console rather than in the rendered document.\nThe #| error: true chunk execution option does not seem to work (I’ve filed a bug report about this.).",
    "crumbs": [
      "How tos",
      "Using Quarto"
    ]
  },
  {
    "objectID": "office_hours.html",
    "href": "office_hours.html",
    "title": "Office hours",
    "section": "",
    "text": "Chris (Evans 495 or Zoom (make sure to email me in advance to use Zoom)\n\nTuesday 2-3 pm\nWednesday 3-4 pm\nThursday 2:30-3:30 pm\nBy appointment\nFeel free to drop by if my door is open, though I won’t always be able to help at the time.",
    "crumbs": [
      "Office Hours"
    ]
  },
  {
    "objectID": "ps/ps2.html",
    "href": "ps/ps2.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "I haven’t fully worked these problems myself, so if you run into any strange issues, please post on Ed as there could be mistakes/oversights on my part.\nPlease submit as a PDF to Gradescope.\nPlease generate the PDF using Quarto. Feel free to work in a Jupyter notebook and then convert to Quarto before rendering to PDF. Other formats that look professional and are designed for working with code and math notation may also be fine - just check with me via a public post on Ed first.\nRemember to note at the start of your document the names of any other students that you worked with on the problem set (or indicating you didn’t work with anyone if that was the case) and then indicate in the text or in code comments any specific ideas or code you borrowed from another student or any online reference (including ChatGPT or the like).\nIn general, your solution should not just be code - you should have text describing how you approached the problem and what decisions/conclusions you made, though for simple problems, this can be quite short. Your code should have comments indicating what each function or block of code does, and for any lines of code or code constructs that may be hard to understand, a comment indicating what that code does.\nYou do not need to (and should not) show exhaustive output, but in general you should show short examples of what your code does to demonstrate its functionality. The output should be produced as a result of the code chunks being run during the rendering process, not by copy-pasting of output from running the code separately (and definitely not as screenshots).\nI do not recommend writing initial answers using a ChatBot, as I think you are likely to fool yourself in terms of how much you are learning about Julia and programming concepts/skills more generally. But it’s up to you to decide how heavily to rely on a ChatBot. And refining your initial answers using a ChatBot seems like a good strategy. Using your own knowledge and information online to check the results of a ChatBot and using a ChatBot to check your own coding can both be important/useful.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "ps/ps2.html#comments",
    "href": "ps/ps2.html#comments",
    "title": "Problem Set 2",
    "section": "",
    "text": "I haven’t fully worked these problems myself, so if you run into any strange issues, please post on Ed as there could be mistakes/oversights on my part.\nPlease submit as a PDF to Gradescope.\nPlease generate the PDF using Quarto. Feel free to work in a Jupyter notebook and then convert to Quarto before rendering to PDF. Other formats that look professional and are designed for working with code and math notation may also be fine - just check with me via a public post on Ed first.\nRemember to note at the start of your document the names of any other students that you worked with on the problem set (or indicating you didn’t work with anyone if that was the case) and then indicate in the text or in code comments any specific ideas or code you borrowed from another student or any online reference (including ChatGPT or the like).\nIn general, your solution should not just be code - you should have text describing how you approached the problem and what decisions/conclusions you made, though for simple problems, this can be quite short. Your code should have comments indicating what each function or block of code does, and for any lines of code or code constructs that may be hard to understand, a comment indicating what that code does.\nYou do not need to (and should not) show exhaustive output, but in general you should show short examples of what your code does to demonstrate its functionality. The output should be produced as a result of the code chunks being run during the rendering process, not by copy-pasting of output from running the code separately (and definitely not as screenshots).\nI do not recommend writing initial answers using a ChatBot, as I think you are likely to fool yourself in terms of how much you are learning about Julia and programming concepts/skills more generally. But it’s up to you to decide how heavily to rely on a ChatBot. And refining your initial answers using a ChatBot seems like a good strategy. Using your own knowledge and information online to check the results of a ChatBot and using a ChatBot to check your own coding can both be important/useful.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "ps/ps2.html#problems",
    "href": "ps/ps2.html#problems",
    "title": "Problem Set 2",
    "section": "Problems",
    "text": "Problems\n\nThis problem explores declaring types for function arguments and thinking about subtypes.\n\nWrite a function that sums the elements of a dictionary, but only if the values are all integer or floating point scalar numbers, handled by setting the type of the argument to your function (i.e., it should be Dictionary with types specified for the keys and values). Hint: you will need to use the &lt;: syntax. The keys can be any type.\nAdd another method for the function that, for a dictionary whose elements are all either strings or character literals, concatenates the strings or character literals.\n\nLet’s enhance your Newton function from PS1.\n\nAllow the objective function to take arbitrary additional arguments, passed along from your function into the objective function.\nRobustify your Newton function. Add error trapping using Julia’s exception system. This should catch divergence as well as convergence to a local maximum (the goal of the function is to find a local minimum). Add useful messaging with @info, @warn, @error.\nAdd handling for the following situations:\n\nWhen the next point has a objective value than the current point. In that case you can backtrack toward the current point, perhaps first halfway back, then halfway on the remaining half, etc., until you find a new point whose value lower than that of the current point.\nWhen the next value is outside the range of the previous values. In that case you can use the technique of bisection to find a new point within the valid interval and then proceed with Newton iterations from there.\n\nSet up a few tests for your Newton function using Julia’s testing framework (to be discussed in class Thursday Feb. 6).\n\nTips: Here are some thoughts about functions you can use as you develop your code and for formal tests. One function you can test on is the sine or cosine function. If the starting point is near enough to a local minimum it should converge to that minimum, but if it is further away it should converge to the nearby local maximum instead. And here’s a function that diverges in some cases: \\[ x \\mbox{tan}^{-1}(x)-0.5*\\log(\\left|1+x^2\\right|), \\] because the first derivative is \\(\\mbox{tan}^{-1}(x)\\) (atan() in Julia), which flattens out as \\(\\left|x\\right|\\) goes to infinity. However, there are lots of others you could come up, so feel free to work with others that may be better than these.\nWrite a function constructor that creates a version of a function that reports how many times it has been called. Ideally this would work regardless of how many (if any) positional or keyword arguments are used by the function. Additionally, try to manipulate kwargs so that the wrapper only reports the number of times run when the wrapped function is called as myfun(&lt;regular_arguments&gt;, _report = true).\nIf you want a real challenge, try to do this using a macro. I was able to do this using some meta programming (code manipulation) tools in Julia but it took some Googling and experimentation, and it was helpful that I have experience with meta programming in R.\nConsider the overdispersed binomial likelihood calculation below.\nThe following is the probability mass function for an overdispersed binomial random variable: \\[\nP(Y =y;n,p,\\phi)  =  \\frac{f(y;n,p,\\phi)}{\\sum_{k=0}^{n}f(k;n,p,\\phi)}\n\\]\n\\[\nf(k;n,p,\\phi)  =  {n \\choose k}\\frac{k^{k}(n-k)^{n-k}}{n^{n}}\\left(\\frac{n^{n}}{k^{k}(n-k)^{n-k}}\\right)^{\\phi}p^{k\\phi}(1-p)^{(n-k)\\phi}\n\\]\nwhere the denominator of \\(P(Y =y;n,p,\\phi)\\) serves as a normalizing constant to ensure this is a valid probability mass function.\n\nWrite two functions, (a) one using looping and (b) one using vectorization, to implement the calculation of the denominator above and compare speed, taking care that the time for JIT compilation is not included. (Some reasonable values to use are \\(n=10000\\), \\(p=0.3\\) and \\(\\phi=0.5\\).) In your implementations, avoid unnecessary calculations (e.g., simplify/combine terms where possible). Also assess the time for the JIT compilation.\nTips: Remember that \\(0^0 = 1\\) and that for numerical stability the sorts of calculations done in calculating \\(f()\\) should be done on the log scale before exponentiating at the end.\nWhat happens in terms of speed if you do the calculation outside of a function? In this case the JIT compilation is presumably not possible.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "ps/ps1.html",
    "href": "ps/ps1.html",
    "title": "Problem Set 1",
    "section": "",
    "text": "I haven’t fully worked these problems myself, so if you run into any strange issues, please post on Ed as there could be mistakes/oversights on my part.\nPlease submit as a PDF to Gradescope.\nPlease generate the PDF using Quarto. Feel free to work in a Jupyter notebook and then convert to Quarto before rendering to PDF. Other formats that look professional and are designed for working with code and math notation may also be fine - just check with me via a public post on Ed first.\nRemember to note at the start of your document the names of any other students that you worked with on the problem set (or indicating you didn’t work with anyone if that was the case) and then indicate in the text or in code comments any specific ideas or code you borrowed from another student or any online reference (including ChatGPT or the like).\nFor problems 1 and 5, your solution should not just be code - you should have text describing how you approached the problem and what decisions/conclusions you made, though for simple problems, this can be quite short. Your code should have comments indicating what each function or block of code does, and for any lines of code or code constructs that may be hard to understand, a comment indicating what that code does.\nYou do not need to (and should not) show exhaustive output, but in general you should show short examples of what your code does to demonstrate its functionality. The output should be produced as a result of the code chunks being run during the rendering process, not by copy-pasting of output from running the code separately (and definitely not as screenshots).\nI do not recommend writing initial answers using a ChatBot, as I think you are likely to fool yourself in terms of how much you are learning about Julia and programming concepts/skills more generally. But it’s up to you to decide how heavily to rely on a ChatBot. And refining your initial answers using a ChatBot seems like a good strategy. Using your own knowledge and information online to check the results of a ChatBot and using a ChatBot to check your own coding can both be important/useful.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "ps/ps1.html#comments",
    "href": "ps/ps1.html#comments",
    "title": "Problem Set 1",
    "section": "",
    "text": "I haven’t fully worked these problems myself, so if you run into any strange issues, please post on Ed as there could be mistakes/oversights on my part.\nPlease submit as a PDF to Gradescope.\nPlease generate the PDF using Quarto. Feel free to work in a Jupyter notebook and then convert to Quarto before rendering to PDF. Other formats that look professional and are designed for working with code and math notation may also be fine - just check with me via a public post on Ed first.\nRemember to note at the start of your document the names of any other students that you worked with on the problem set (or indicating you didn’t work with anyone if that was the case) and then indicate in the text or in code comments any specific ideas or code you borrowed from another student or any online reference (including ChatGPT or the like).\nFor problems 1 and 5, your solution should not just be code - you should have text describing how you approached the problem and what decisions/conclusions you made, though for simple problems, this can be quite short. Your code should have comments indicating what each function or block of code does, and for any lines of code or code constructs that may be hard to understand, a comment indicating what that code does.\nYou do not need to (and should not) show exhaustive output, but in general you should show short examples of what your code does to demonstrate its functionality. The output should be produced as a result of the code chunks being run during the rendering process, not by copy-pasting of output from running the code separately (and definitely not as screenshots).\nI do not recommend writing initial answers using a ChatBot, as I think you are likely to fool yourself in terms of how much you are learning about Julia and programming concepts/skills more generally. But it’s up to you to decide how heavily to rely on a ChatBot. And refining your initial answers using a ChatBot seems like a good strategy. Using your own knowledge and information online to check the results of a ChatBot and using a ChatBot to check your own coding can both be important/useful.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "ps/ps1.html#problems",
    "href": "ps/ps1.html#problems",
    "title": "Problem Set 1",
    "section": "Problems",
    "text": "Problems\n\nWrite a function that implements a basic version of Newton’s method for minimizing a function of one variable.\n\nStart with a simple implementation. Use an existing Julia package to implement the finite difference estimates for the gradient and Hessian. Think about the arguments and any defaults, as well as the type of the output. For the moment don’t set the types of the input arguments. You can start by assuming the function takes only one argument and simply returns the value at which the function is minimized.\nNow consider returning richer output. Consider using a named tuple, a dictionary, or a struct. What seem like the advantages/disadvantages? Choose one and implement it.\nSet up an array and save the progression of values along the optimization path.\n(We won’t cover this in class until Tuesday Jan. 28.) Add argument typing to your Newton function. Allow the initial value to be of any numeric type. See what the the type of the return value is for various numeric input types.\nUse a ChatBot (lmarena.ai provides free access if you don’t want to sign up for the free tier of one of the commercial services) to write the code. Compare it to your code and indicate strengths/weaknesses.\n\nDefine the matrix A = [1:4  5:8  ones(Int64,4)]. Predict the result of performing the following operations in Julia (before checking your answers by running them). Note that the lines are meant to be run one-by-one in the same workspace, so when you change an array, this will affect the subsequent statements.\n\nx = A[3,:]\nB = A[2:2,1:3]\nA[1,1] = 9 + A[2,3]\nA[1:3,2:3] = [0 0; 0 0; 0 0]\nA[1:2,2:3] = [1 3; 1 3]\ny = A[1:4, 3]\nA = [A [2 1 7; 7 4 5; ones(Int64,2,3)]]\nC = A[[1,3],2]\nD = A[[1,2,4],[1,3,4]]\n\nExperiment with some ways to extract the elements of a vector that correspond to the even indices, i.e., x[2], x[4],…\nUse array functions and vectorization to solve the problems below using only a single line of code for each problem, where A = reshape((-22:22) .% 11, 9, 5).\n\nCount the number of elements for which \\(A_{i,j}^2&lt;10\\).\nCreate a matrix containing only the columns of A where the first element \\(A_{1,j} &gt;= 0\\).\nModify A such that all elements that are even are multiplied by 3.\n\nConsider dictionaries, named tuples, and structs. Experiment with sizeof and pointer_from_objref to try to understand memory use (including any pointers) of these data structures. Next consider arrays that have homogeneous types and those with heterogeneous types.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Student Presentations",
    "section": "",
    "text": "Testing (link to repository)\nPlotting (link to repository)\nJIT compilation",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 244: Computing for Statistics and Data Science with Julia",
    "section": "",
    "text": "Ed\n\n  Gradescope\n\n  Tutorials\n\n  PollEV\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Statistics 244: Computing for Statistics and Data Science with Julia",
    "section": "Course description",
    "text": "Course description\nProgramming and computation for applications in statistics, data science and related fields, focusing on the use of Julia, a modern language that offers interactivity with high performance based on just-in-time compilation. The course will also cover the use of co-processors, in particular GPUs, through Julia and Python packages such as Jax and PyTorch. Topics will include data types, functional programming, multiple argument dispatch, memory use, efficiency, parallelization, robustness and testing.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Statistics 244: Computing for Statistics and Data Science with Julia",
    "section": "Course materials",
    "text": "Course materials\nSee the links above for the key resources for the course.\nMost course content (in particular notes and problem sets) are available through this website via the links in the left sidebar.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#questions-about-taking-the-class",
    "href": "index.html#questions-about-taking-the-class",
    "title": "Statistics 244: Computing for Statistics and Data Science with Julia",
    "section": "Questions about taking the class",
    "text": "Questions about taking the class\nIf you would like to audit the class, enroll as a UC Berkeley undergraduate, or enroll as a concurrent enrollment student (i.e., for visiting students), or for some other reason are not enrolled, please fill out this survey as soon as possible and, ideally, chat with me before/after the first class. All those enrolled or wishing to take the class should have filled it out by Wednesday January 24 at noon.\nPlease see the syllabus for the computing background I expect.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "ps/ps3.html",
    "href": "ps/ps3.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "I haven’t fully worked these problems myself, so if you run into any strange issues, please post on Ed as there could be mistakes/oversights on my part.\nYou’ll need to use the SCF for the GPU work (and possibly the CPU-based parallelization). The how-to on accessing the SCF and the SCF quick-start guide and the page on submitting GPU jobs give guidance. Please ask questions on Ed or in office hours or in class.\nPlease submit as a PDF to Gradescope.\nPlease generate the PDF using Quarto. Feel free to work in a Jupyter notebook and then convert to Quarto before rendering to PDF. Other formats that look professional and are designed for working with code and math notation may also be fine - just check with me via a public post on Ed first.\n\nGiven that this PS involves parallelization, it’s fine to set your code chunks not to execute and to copy-paste output from running your code separately from the rendering of your solution.\n\nRemember to note at the start of your document the names of any other students that you worked with on the problem set (or indicating you didn’t work with anyone if that was the case) and then indicate in the text or in code comments any specific ideas or code you borrowed from another student or any online reference (including ChatGPT or the like).\nIn general, your solution should not just be code - you should have text describing how you approached the problem and what decisions/conclusions you made, though for simple problems, this can be quite short. Your code should have comments indicating what each function or block of code does, and for any lines of code or code constructs that may be hard to understand, a comment indicating what that code does.\nYou do not need to (and should not) show exhaustive output, but in general you should show short examples of what your code does to demonstrate its functionality. The output should be produced as a result of the code chunks being run during the rendering process, not by copy-pasting of output from running the code separately (and definitely not as screenshots).\nI do not recommend writing initial answers using a ChatBot, as I think you are likely to fool yourself in terms of how much you are learning about Julia and programming concepts/skills more generally. But it’s up to you to decide how heavily to rely on a ChatBot. And refining your initial answers using a ChatBot seems like a good strategy. Using your own knowledge and information online to check the results of a ChatBot and using a ChatBot to check your own coding can both be important/useful.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "ps/ps3.html#comments",
    "href": "ps/ps3.html#comments",
    "title": "Problem Set 3",
    "section": "",
    "text": "I haven’t fully worked these problems myself, so if you run into any strange issues, please post on Ed as there could be mistakes/oversights on my part.\nYou’ll need to use the SCF for the GPU work (and possibly the CPU-based parallelization). The how-to on accessing the SCF and the SCF quick-start guide and the page on submitting GPU jobs give guidance. Please ask questions on Ed or in office hours or in class.\nPlease submit as a PDF to Gradescope.\nPlease generate the PDF using Quarto. Feel free to work in a Jupyter notebook and then convert to Quarto before rendering to PDF. Other formats that look professional and are designed for working with code and math notation may also be fine - just check with me via a public post on Ed first.\n\nGiven that this PS involves parallelization, it’s fine to set your code chunks not to execute and to copy-paste output from running your code separately from the rendering of your solution.\n\nRemember to note at the start of your document the names of any other students that you worked with on the problem set (or indicating you didn’t work with anyone if that was the case) and then indicate in the text or in code comments any specific ideas or code you borrowed from another student or any online reference (including ChatGPT or the like).\nIn general, your solution should not just be code - you should have text describing how you approached the problem and what decisions/conclusions you made, though for simple problems, this can be quite short. Your code should have comments indicating what each function or block of code does, and for any lines of code or code constructs that may be hard to understand, a comment indicating what that code does.\nYou do not need to (and should not) show exhaustive output, but in general you should show short examples of what your code does to demonstrate its functionality. The output should be produced as a result of the code chunks being run during the rendering process, not by copy-pasting of output from running the code separately (and definitely not as screenshots).\nI do not recommend writing initial answers using a ChatBot, as I think you are likely to fool yourself in terms of how much you are learning about Julia and programming concepts/skills more generally. But it’s up to you to decide how heavily to rely on a ChatBot. And refining your initial answers using a ChatBot seems like a good strategy. Using your own knowledge and information online to check the results of a ChatBot and using a ChatBot to check your own coding can both be important/useful.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "ps/ps3.html#problems",
    "href": "ps/ps3.html#problems",
    "title": "Problem Set 3",
    "section": "Problems",
    "text": "Problems\n\nGaussian processes provide a distribution over functions, allowing us to simulate random time series and images, amongst other things. We’ll focus on using Gaussian processes in one dimension.\nConsider a set of values \\(x = {{x_1,\\ldots,x_n}} \\in \\Re\\). We will simulate from a multivariate normal, \\(y \\sim \\mbox{MVN}(\\mu 1, \\Sigma)\\), where \\(\\mu\\) is (for simplicity) a scalar mean and \\(1\\) is a vector of ones. The covariance matrix, \\(\\Sigma\\) is constructed as \\(\\Sigma_{i,j} = f(x_i, x_j)\\) for some positive definite covariance function, \\(f\\). Some examples are the exponential covariance, \\(f(x_i,x_j) = \\sigma^2 \\exp(-d_{i,j}/\\rho)\\) for \\(d_{i,j}=|x_i-x_j|\\) and the squared exponential \\(f(x_i,x_j) = \\sigma^2 \\exp(-d_{i,j}^2/\\rho^2)\\). To simulate a random function (in the one-d case, you can think of this as a random time series), we compute\n\\[ \\mu 1 + L z \\] where \\(L\\) is the lower-triangular Cholesky decomposition of \\(\\Sigma\\) and \\(z\\) is a vector of \\(n\\) numbers simulated independently from a standard normal. Note that for both of the \\(f\\) functions above, these are simple stationary covariance functions that are functions of the distance between the two points and a range/length scale parameter \\(\\rho\\) that determines how “wiggly” the generated time series are (more complicated covariance functions are possible).\n\nUse struct(s) and functions/methods with type annotation to implement this in Julia. Make sure to cache information effectively for cases where a user wants to simulate multiple time series with the same \\(x\\), \\(f\\), \\(\\sigma\\), and \\(\\rho\\). Include logging and exception handling as you did in PS2. An object-oriented implementation is available in Python here. Tip: Distances.jl has a Euclidean() distance function.\nConsider a coarse grid, \\(x = 0:0.1:1\\), and a fine grid, \\(x = 0:0.01:1\\). Use Float64 and Float16 representations to compute \\(\\Sigma\\) and its Cholesky. You’ll find numerical issues in some cases. Plot a random time series with the finer grid and notice the difference in smoothness between the exponential and squared exponential cases. Note that one (slightly awkward) workaround to add a small epsilon to the diagonal of \\(\\Sigma\\) for the situations where the matrix is not (numerically) positive definite.\nAllow your code to use GPUs if available on the system (but fall back to the CPU if no GPU is available). Benchmark your code on an SCF GPU (or your own if you happen to have one or have access to one), making sure to report which type of GPU you used for your work. In your time comparisons, also compare to using multiple CPU cores via the threaded BLAS.\nSuppose your users want to generate many time series at once (e.g., for a simulation study). So the output should be an \\(n \\times m\\) matrix, where \\(m\\) is the number of simulated time series. Use threaded or multi-process parallelization (your choice) to generate the time series in parallel, using the same underlying Cholesky decomposition.\n\nThis problem asks you to implement kernel density estimation by writing your own GPU kernel in Julia. (Unfortunately, “kernel” will mean both the statistical kernel used for the averaging and the GPU kernel used to implement the calculations.)\n\nWrite a GPU kernel using Julia that computes a kernel density estimate (KDE) in two dimensions using a normal density with bandwidth \\(h\\) equal to the standard deviation of the normal density. The estimate should be based on an input dataset with \\(n\\) data points. Your code should compute the KDE on a grid of points, for \\(m\\) points, with each evaluation point handled in a separate thread. This means that each thread will access the full dataset (more on this in part (c)). Check that your code works by applying it to a moderately-sized simulated dataset - e.g., you could simulate \\(n\\) points using a mixture of 2-d normal densities.\nApply your code to data on the location of wildfires in the US, which contains information on the location, year, and size of fires in the US from the federal government’s wildfire database, which I obtained from Kaggle and then modified to remove various columns and project to an Albers equal area projection (in meters), so that we can directly work with Euclidean distances. You’ll presumably want a value of \\(h\\) somewhere between, say, 10 km and 300 km. Feel free to subset to a portion of the US of interest to you. And feel free to choose a square region and implement your code assuming a regular square grid of evaluation points to keep things simpler. There are 2.3 million fires in the dataset, most of them quite small.\nNow consider using 10-fold cross-validation to find the best value of \\(h\\) from a (small-ish) grid of potential values (possibly equally-spaced on the log scale rather than the original scale), evaluating the log of the kernel density estimate on the held-out points. First, time this by calling your kernel from part (a) for the 10 folds for each value of \\(h\\). Next, include the calculation of the density for each value of \\(h\\) in a loop within the kernel. Call your kernel on the 10 folds. Compare the timing when you avoid having to re-access the data for each value of $h. (Note that one could go further by embedding the cross-validation in the kernel too…)\nNow consider using shared memory amongst the threads in a block via CuStaticSharedArray or CuDynamicSharedArray. Load the data in chunks of size equal to the number of threads in a block into the shared array in parallel using the threads in the block. Then use your code from earlier to calculate the density for the chunk of data, with each thread doing an evaluation point (as before). Finally wrap the chunk-specific code by looping over all the chunks needed to process the full dataset. Compare the speed when using shared memory with the original implementation in part (a) (i.e., for this comparison you don’t need to do cross-validation and can use a single \\(h\\) value).\n\nSide note: rather than having each thread handle a single evaluation point on the grid of \\(m\\) points, one could have each thread handle a single data point and loop over all \\(m\\) points. This would reduce memory access, but there would then need to be a shared reduction operation, which involves atomic calls to avoid conflicts between the threads accessing the same output. Ideally one would first do the partial reduction across the data points within each block in shared memory.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "License"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "UNDER CONSTRUCTION - PROVIDED FOR INFORMATION BUT NOT FINAL/DEFINITIVE",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#objectives-of-the-course",
    "href": "syllabus.html#objectives-of-the-course",
    "title": "Syllabus",
    "section": "Objectives of the course",
    "text": "Objectives of the course\nThe goals of the course are that, by the end of the course, students be able to:\n\nhave a solid understanding of general programming concepts and principles, including parallelization for CPUs and GPUs; and\nbe able to program effectively using Julia.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#topics",
    "href": "syllabus.html#topics",
    "title": "Syllabus",
    "section": "Topics",
    "text": "Topics\nThese are tentative (particularly in terms of exact timing and the later weeks), as the course has never been taught before.\n\nWeek 1: Julia syntax, data structures and basic types, functional programming, memory and copying, variable scope\nWeek 2: Methods and multiple dispatch, types, managing Julia (packaging, debugging, testing, exceptions, etc.)\nWeek 3: Efficiency and memory use, just-in-time (JIT) compilation\nWeek 4: Parallel programming concepts and parallelization in Julia\nWeek 5: Parallel programming in Python and additional topics in parallelization\nWeek 6: Numerical computation in Julia (floating point concepts, linear algebra, optimization\nWeek 7: Student project presentations and other topics",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-content",
    "href": "syllabus.html#course-content",
    "title": "Syllabus",
    "section": "Course content",
    "text": "Course content\nKey websites for the course are:\n\nThis course website, which is hosted on GitHub Pages, and the GitHub repository containing the source materials: https://github.com/berkeley-stat244/spring-2025. The main course content will be notes prepared by me under the Notes dropdown and student presentations on various topics.\nSCF tutorials for additional content: https://statistics.berkeley.edu/computing/training/tutorials\nEd Discussion site for discussions/Q&A: https://edstem.org/us/courses/73174/discussion\nGradescope for problem sets: https://www.gradescope.com/courses/949978\n\nThere will be no course capture for this course, in part because of the room we are in and in part because of the seminar/special topics nature of the course. I expect you to attend all classes, except for illness or with advance notice of well-justified travel.\nWhile there is a bCourses site for the course, we won’t use it at all, as far as I anticipate at the moment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-culture",
    "href": "syllabus.html#course-culture",
    "title": "Syllabus",
    "section": "Course culture",
    "text": "Course culture\nThis is a small class, a graduate class, and essentially a special topics/seminar class. That has the following implications.\nI expect a corresponding degree of maturity from all students, including the small number of undergraduates who might take the class. Among other things, this means trying to not stress about grading, being comfortable with ambiguity (in terms of content, expectations, and grading), being curious, and fully participating in class, even if you feel a bit uncomfortable because you think it might reveal some lack of knowledge or understanding on your part.\nEvaluation/grading will be somewhat informal and based in part on your contributions to the class community (either to the in-class discussion or the Ed discussion board), as well as completion of problem sets, and my assessment of student project/presentations. See more below.\nFurthermore, I am not an expert in Julia! Part of my motivation for teaching the class was to learn Julia myself (as well as more about working with GPUs), which I’ve done some of in preparation for the class and to some degree will make up for remaining gaps based on my larger expertise in this sort of programming (in particular experience with R and Python). But there’s a lot I don’t know, and there will be lots of questions I’m not sure how to answer in the moment in class. Contributing your thoughts in class or on Ed is part of your responsibility in the class.\nPlease do not use phones during class and limit laptop use to the material being covered.\nFinally, if you have suggestions for how to make the course better, including how we use time in class, please let me know.\n\nEd culture\nPlease follow these guidelines for choosing how to post on Ed:\n\nPublic (non-anonymous) posts: I highly encourage public posts about course topics/concepts and topics discussed in class as well as questions about problem set problems, as that is the only way to contribute to the course discussion.\n\nPlease DO NOT post public messages anonymously.\n\nPrivate (but non-anonymous) posts:\n\nIf you feel very uncomfortable asking a question about the items mentioned above, you can make a private (but non-anomymous) post. But it won’t “count” in terms of your class participation.\nIn some cases, I may ask/encourage you to make your post public, so I can respond such that all students see my thoughts.\nQuestions specific to your project or presentation are also appropriate for private posts, but in some cases you might want to elicit feedback from other students, in which case a public post is entirely welcome.\nI welcome feedback about the course, particularly given it is the first time offered and there will be lots of things that could be improved.\n\nAnonymous private posts:\n\nIf you’d like to leave anonymous feedback via a private post to me, you can. I prefer to know about it rather than for you to keep it to yourself, but I encourage you not to make it anonymous.\nI won’t answer general topic/course questions that are posted anonymously, whether public or private.\n\nAnonymous public posts: Please DO NOT make such posts. I don’t think it is conducive to productive discussion in a community.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-material",
    "href": "syllabus.html#course-material",
    "title": "Syllabus",
    "section": "Course material",
    "text": "Course material\n\nPrimary materials: Course notes on the website and student presentation materials, plus SCF tutorials.\nAdditional materials:\n\nThink Julia book\nJulia manual\nPer-Olof Persson’s Berkeley Math 124 materials\n\nSee also the Statistics 243 course material list for references on numerical computing and software tools (bash, Quarto).",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#computing-resources",
    "href": "syllabus.html#computing-resources",
    "title": "Syllabus",
    "section": "Computing Resources",
    "text": "Computing Resources\nMost work for the course can be done on your laptop. Later in the course (in a cfew weeks!), we’ll also use the Statistics Department Linux cluster for access to GPUs. Please sign up for an SCF account now if you don’t have one.\nThe software needed for the course is as follows:\n\nJulia\nQuarto\nPython (the Miniforge installation of Conda is recommended but by no means required)\nGit\n\nSee the “how tos” in the left sidebar for tips on software installation and usage.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-requirements-and-grading",
    "href": "syllabus.html#course-requirements-and-grading",
    "title": "Syllabus",
    "section": "Course requirements and grading",
    "text": "Course requirements and grading\n\nCourse grades\nGrading will be based on a combination of your general participation (in class and on the discussion forum), completion of several problem sets, and either an in-class presentation on a topic or a project. I have a number of topics I’d like to see covered by student presentation but I am open to other ideas as well. Projects and in-class presentations can be on your own or in pairs.\nGrades will generally be As and Bs, unless I assess a lack of engagement on your part.\n\nProblem sets\nPlease prepare your assignments using Quarto (or a Jupyter notebook), output to PDF and submitted to Gradescope. Answers should consist of textual response (and any useful mathematical expressions as appropriate), with key chunks of code embedded within the document.\nYou must include a “Collaboration” section in which you indicate any other students you worked with and describing the nature of any use of ChatBots/LLMs. If you did not collaborate with anyone else (including said AIs), simply state that.\nYou’re welcome to consult with classmates on problem set problems, and I encourage it. You can discuss a problem with another student, brainstorm approaches, and share code syntax (generally not more than one line) on how to do small individual coding tasks within a problem.\n\nYou should not ask another student for complete code or solutions, or look at their code/solution.\nYou should not share complete code or solutions with another student or on Ed Discussion.\n\nRemember this is a small class with one goal that you learn how to effectively program, in Julia in particular. There’s no point in copying code in bulk from another student or a ChatBot, because it won’t help you learn programming/Julia. For that you need to wrestle with the concepts/syntax and develop “muscle memory”.\nI am still considering how I will grade problem sets, given there is no GSI and the nature of the class. They will either be graded complete/incomplete or on a 4 point scale:\n\n0 = no credit,\n1 = partial credit (you did some of the problems but not all),\n2 = generally satisfactory (you tried everything but there were pieces of what you did that didn’t completely/correctly solve or present/explain one or more problems), and\n3 = full credit.\n\n\n\nProjects\nI’m expecting that many of you will want to come up with your own final project. You’ll need to discuss the topic with me in advance. It should involve programming in Julia, though use of Python for GPU computations may be ok too. I may also have a couple project topics that you could choose from if you don’t have a project idea and want to do a project rather than doing class presentation(s).\nIn some cases a final project could overlap/be combined with the class presentation topics, such as a project focused on optimization, use of special matrices, AI/ML, or using GPUs. You would present your project to the class (probably in the last week or two) and in the process of doing so, you can talk about the general context as well as your specific work.\nYou should use Git to manage the code in your project. The result does not have to be, but could be, a Julia package.\n\n\nPresentations\nStudent presentations will be a core part of the content of the course. I have some specific ideas of topics that I think would be good, but I am also open to your ideas of topics you’d be excited to dive into and present.\nI expect you to do a deep dive on the topic, and while I don’t expect you to be an expert in a week or two, I do expect you to have done a some research and experimentation and pulled together a clear, in-depth presentation focused on examples/demonstration/code syntax, and be ready to answer questions from the class. I will discuss the focus/goal of the presentation/topic with the student(s) when the topic is chosen and then we’ll have a quick interim “progress” discussion a few days before the presentation. I’m expecting the presentations would be 15-30 minutes, depending on the topic.\nI highly recommend using Quarto to prepare presentation materials, but I am also happy to discuss other possibilities with you. We’ll need to figure out a good way to make the final materials available through the website, probably through pull requests made to the course GitHub site.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#accomodations-for-students-with-disabilities",
    "href": "syllabus.html#accomodations-for-students-with-disabilities",
    "title": "Syllabus",
    "section": "Accommodations for Students with Disabilities",
    "text": "Accommodations for Students with Disabilities\nPlease see me as soon as possible if you need particular accommodations, and we will work out the necessary arrangements. That said, given the lack of exams, there may be no need for an accommodation even if you have a disability.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#campus-honor-code",
    "href": "syllabus.html#campus-honor-code",
    "title": "Syllabus",
    "section": "Campus Honor Code",
    "text": "Campus Honor Code\nThe following is the Campus Honor Code. With regard to collaboration and independence, please see my rules regarding problem sets above – Chris.\nThe student community at UC Berkeley has adopted the following Honor Code: “As a member of the UC Berkeley community, I act with honesty, integrity, and respect for others.” The hope and expectation is that you will adhere to this code.\nCollaboration and Independence: Reviewing lecture and reading materials and studying for exams can be enjoyable and enriching things to do with fellow students. This is recommended. However, unless otherwise instructed, homework assignments are to be completed independently and materials submitted as homework should be the result of one’s own independent work.\nCheating: A good lifetime strategy is always to act in such a way that no one would ever imagine that you would even consider cheating. Anyone caught cheating on a quiz or exam in this course will receive a failing grade in the course and will also be reported to the University Center for Student Conduct. In order to guarantee that you are not suspected of cheating, please keep your eyes on your own materials and do not converse with others during the quizzes and exams.\nPlagiarism: To copy text or ideas from another source without appropriate reference is plagiarism and will result in a failing grade for your assignment and usually further disciplinary action. For additional information on plagiarism and how to avoid it, see, for example: http://gsi.berkeley.edu/teachingguide/misconduct/prevent-plag.html.\nAcademic Integrity and Ethics: Cheating on exams and plagiarism are two common examples of dishonest, unethical behavior. Honesty and integrity are of great importance in all facets of life. They help to build a sense of self-confidence, and are key to building trust within relationships, whether personal or professional. There is no tolerance for dishonesty in the academic world, for it undermines what we are dedicated to doing – furthering knowledge for the benefit of humanity.\nYour experience as a student at UC Berkeley is hopefully fueled by passion for learning and replete with fulfilling activities. And we also appreciate that being a student may be stressful. There may be times when there is temptation to engage in some kind of cheating in order to improve a grade or otherwise advance your career. This could be as blatant as having someone else sit for you in an exam, or submitting a written assignment that has been copied from another source. And it could be as subtle as glancing at a fellow student’s exam when you are unsure of an answer to a question and are looking for some confirmation. One might do any of these things and potentially not get caught. However, if you cheat, no matter how much you may have learned in this class, you have failed to learn perhaps the most important lesson of all.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "howtos/installGit.html",
    "href": "howtos/installGit.html",
    "title": "Installing Git",
    "section": "",
    "text": "Here are some instructions for installing Git on your computer. You should use Git to manage your work on your final project. You may also want to use it for your problem set and presentation work.\nYou can install Git by downloading and installing the correct binary from here.\nFor macOS, we recommend using the Homebrew option.\nGit comes installed on the SCF, so if you login to an SCF machine and want to use Git there, you don’t need to install Git.",
    "crumbs": [
      "How tos",
      "Installing Git"
    ]
  },
  {
    "objectID": "howtos/presentations.html",
    "href": "howtos/presentations.html",
    "title": "Presentation guidelines",
    "section": "",
    "text": "Here are some guidelines for your presentations.\n\nSome general thoughts on the topics are in the spreadsheet tab on “Scheduled topics”. I may also email some of you with some additional thoughts. Let me know as you have questions as you are developing your materials.\nYou must meet with me 2-3 days before your presentation so we can discuss your plan for the presentation (in particular the content) and I can help with any re-focusing I think might be useful. It’s your responsibility to talk with me after class, find me in office hours, or make an appointment.\nIn order to have this meeting be productive, you should have already done much of the work for the presentation before the meeting, in particular having a detailed outline of what you plan to cover, even if some of the details remain to be filled in.\nPlan on 15-25 minutes for your presentation. It’s always easy to go over time, so please walk through your material in advance to adjust the timing. And don’t be offended if I suggest you skip over some details if I feel it makes sense. Do not feel that you need to make your presentation longer to get a better grade.\nI’d like for there to be a qmd file or Jupyter notebook for your material that I can share with the class as part of the course materials. However, I understand that depending on the topic, different formats might make sense, so there is some leeway here. Let me know if you have questions. Please point me to your material online (or send me a document) by the morning of the presentation day, so I can link to the material from the course website.\nWe will record your presentation as part of the recording for class that day. You’ll need to join the Zoom session and sit where I usually sit to connect to the projector (or you can stand in front of the class if you prefer).\nPlan to have your presentation have a large focus on demonstration. If you wish, you can also have the rest of the class engage in some sort of exercise(s) - e.g., a bit of coding or some brainstorming, etc.\nFor some of the topics, feel free to have the presentation have some flavor of a project. The content can be a combination of general overview of the topic and a deeper look into something you’re specifically interested in, including some code development.",
    "crumbs": [
      "How tos",
      "Presentation guidelines"
    ]
  },
  {
    "objectID": "howtos/accessSCF.html",
    "href": "howtos/accessSCF.html",
    "title": "Accessing the SCF",
    "section": "",
    "text": "The SCF is the Statistical Computing Facility, which provides computational resources for the Department of Statistics.\nFor our purposes, the SCF provides access to all the software and resources we will need for class. For much of what we do, it will serve as an alternative to your laptop if you run into problems. For GPU work and for some of the parallelization work it will be the primary resource.",
    "crumbs": [
      "How tos",
      "Accessing the SCF"
    ]
  },
  {
    "objectID": "howtos/accessSCF.html#get-an-scf-account",
    "href": "howtos/accessSCF.html#get-an-scf-account",
    "title": "Accessing the SCF",
    "section": "Get an SCF account",
    "text": "Get an SCF account\nIf you don’t already have an account, please request one, indicating you are a student in Statistics 244.",
    "crumbs": [
      "How tos",
      "Accessing the SCF"
    ]
  },
  {
    "objectID": "howtos/accessSCF.html#access-to-the-statistical-computing-facility-scf",
    "href": "howtos/accessSCF.html#access-to-the-statistical-computing-facility-scf",
    "title": "Accessing the SCF",
    "section": "Access to the Statistical Computing Facility (SCF)",
    "text": "Access to the Statistical Computing Facility (SCF)\nThere are two main ways to access the SCF machines.\n\nYou can login to our various Linux servers and access a terminal/bash shell that way. Please see http://statistics.berkeley.edu/computing/access.\nThe SCF JupyterHub provides browser-based access to a terminal/bash shell, Jupyter notebooks, and more. From a Jupyter notebook, you can select to use Julia as the kernel (the language used for processing code cells).",
    "crumbs": [
      "How tos",
      "Accessing the SCF"
    ]
  },
  {
    "objectID": "howtos/accessSCF.html#software",
    "href": "howtos/accessSCF.html#software",
    "title": "Accessing the SCF",
    "section": "Software",
    "text": "Software\nJulia (possibly not with all needed packages at the moment), Python (with all needed packages), Git, and Quarto are all available from any SCF machine.",
    "crumbs": [
      "How tos",
      "Accessing the SCF"
    ]
  },
  {
    "objectID": "notes/notes3.html",
    "href": "notes/notes3.html",
    "title": "Notes 3: Types and dispatch",
    "section": "",
    "text": "This document is the third of a set of notes, this document focusing on types and method dispatch based on types. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.",
    "crumbs": [
      "Course Notes",
      "Notes 3: Types and dispatch"
    ]
  },
  {
    "objectID": "notes/notes3.html#introduction",
    "href": "notes/notes3.html#introduction",
    "title": "Notes 3: Types and dispatch",
    "section": "",
    "text": "This document is the third of a set of notes, this document focusing on types and method dispatch based on types. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.",
    "crumbs": [
      "Course Notes",
      "Notes 3: Types and dispatch"
    ]
  },
  {
    "objectID": "notes/notes3.html#composite-types-structs",
    "href": "notes/notes3.html#composite-types-structs",
    "title": "Notes 3: Types and dispatch",
    "section": "Composite types (structs)",
    "text": "Composite types (structs)\nA struct is a collection of named fields, useful for holding information of a particular structure.\nHere’s a struct meant to contain information about a Gaussian process, with each element in the type also having its own declared type (though that is not required).\nObjects of the type are constructed by calling the name of the type with the field values, in order.\n\nstruct GaussianProcess\n  x::Vector\n  covFun::Function\n  params\nend\n\nfunction expCov(dists, params)\n   return params[2]*exp.(-dists / params[1])\nend\n\nmyGP = GaussianProcess([0:0.01:1;], expCov, (1, .5));\nmyGP.params\n\n(1, 0.5)\n\n\nIt appears one has to know the exact order of the inputs to the constructor and give them by position rather than by name.\n\n\n\n\n\n\nNaming of types\n\n\n\nType names (including structs) are generally capitalized. One nice thing is that it helps distinguish constructors (e.g., GaussianProcess) from regular functions.\n\n\n\nStruct constructors\nWe can create an explicit (outer) constructor that gives some flexibility in terms of what inputs the user can provide.\n\nfunction ExpGaussianProcess(x, params)\n   GaussianProcess(x, expCov, params)\nend\n\nmyGP = ExpGaussianProcess([0:0.01:1;], (1, .5));\n\nWe can use an inner constructor to check inputs. This is a bad example because by defining types for the variables (as seen in the original definition of the GaussianProcess struct), this can be handled automatically by Julia.\n\nstruct GaussianProcess2\n  x\n  covFun\n  params\n  ## Define the constructor using function creation shorthand and if-else shorthand:\n  GaussianProcess2(x, covFun, params) = isa(covFun, Function) ?\n                   new(x, covFun, params) : error(\"covFun needs to be a function\")\nend\n\nmyGP = GaussianProcess2([0:0.01:1;], expCov, (1, .5));\n\nmyGP = GaussianProcess2([0:0.01:1;], 3, (1, .5))\n\nLoadError: covFun needs to be a function\ncovFun needs to be a function\n\nStacktrace:\n [1] error(s::String)\n   @ Base ./error.jl:35\n [2] GaussianProcess2(x::Vector{Float64}, covFun::Int64, params::Tuple{Int64, Float64})\n   @ Main ./In[4]:6\n [3] top-level scope\n   @ In[4]:12\n\n\n\n\n\n\n\n\nTypes can’t be redefined\n\n\n\nWe can’t redefine a struct (hence my use of GaussianProcess2 above). I think this has to do with the fact that this would break methods that have been specialized to the type.",
    "crumbs": [
      "Course Notes",
      "Notes 3: Types and dispatch"
    ]
  },
  {
    "objectID": "notes/notes3.html#type-declarations",
    "href": "notes/notes3.html#type-declarations",
    "title": "Notes 3: Types and dispatch",
    "section": "Type declarations",
    "text": "Type declarations\nType declarations are a good and easy way to make our code robust without including a bunch of manual assertions.",
    "crumbs": [
      "Course Notes",
      "Notes 3: Types and dispatch"
    ]
  },
  {
    "objectID": "notes/notes3.html#type-declarations-for-functions",
    "href": "notes/notes3.html#type-declarations-for-functions",
    "title": "Notes 3: Types and dispatch",
    "section": "Type declarations for functions",
    "text": "Type declarations for functions\nWe can declare types for function arguments.\n\nfunction mysum(x::Float64, y::Float64)\n  return x+y\nend\n\nmysum(3.5, 4.8)\n\n8.3\n\n\n\nmysum(3, 4.8)\n\n\nMethodError: no method matching mysum(::Int64, ::Float64)\n\nClosest candidates are:\n  mysum(::Float64, ::Float64)\n   @ Main In[5]:1\n\n\nStacktrace:\n [1] top-level scope\n   @ In[6]:1\n\n\n\n\nmysum(\"hello\", 4.8)\n\n\nMethodError: no method matching mysum(::String, ::Float64)\n\nClosest candidates are:\n  mysum(::Float64, ::Float64)\n   @ Main In[5]:1\n\n\nStacktrace:\n [1] top-level scope\n   @ In[7]:1\n\n\n\n\nType declarations with structs\n\nusing Random, Distributions, LinearAlgebra, Plots\n\nfunction simulate(gp::GaussianProcess)\n  n = length(gp.x)\n  dists = abs.(gp.x .- gp.x')\n  cov = gp.covFun(dists, gp.params)\n  L = cholesky(cov).L\n  y = L * rand(Normal(), n)\n  return y\nend\n\nRandom.seed!(123)\nmyGP = GaussianProcess([0:0.01:1;], expCov, (1, .5));\nf = simulate(myGP);\n\n\nplot(myGP.x, f)   \n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmp = simulate(7) \n\n\nMethodError: no method matching simulate(::Int64)\n\nClosest candidates are:\n  simulate(::GaussianProcess)\n   @ Main In[8]:3\n\n\nStacktrace:\n [1] top-level scope\n   @ In[10]:1\n\n\n\n\n\nType unions and abstract types\n\nIntOrFloat = Union{Float16, Float32, Float64, Int16, Int32, Int64}\nfunction mysum(x::IntOrFloat, y::IntOrFloat)\n  return x+y\nend\n\nmysum(3.5, 4.8)\n\n8.3\n\n\n\nmysum(3, 4.8)\n\n7.8\n\n\n\ntypeof(mysum(3, 4))\n\nInt64\n\n\n\nmysum(\"hello\", 4.8)\n\n\nMethodError: no method matching mysum(::String, ::Float64)\n\nClosest candidates are:\n  mysum(::Float64, ::Float64)\n   @ Main In[5]:1\n  mysum(::Union{Float16, Float32, Float64, Int16, Int32, Int64}, ::Union{Float16, Float32, Float64, Int16, Int32, Int64})\n   @ Main In[11]:2\n\n\nStacktrace:\n [1] top-level scope\n   @ In[14]:1\n\n\n\nAlternatively (and better as it’s provided as part of the language), there is already a abstract type of ints and floats. Abstract types help with having a hierarchy of types (more later).\n\nfunction mysum(x::Real, y::Real)\n  return x+y\nend\n\nmysum(3.5, 4.8)\n\n8.3\n\n\n\nmysum(3, 4.8)\n\n7.8\n\n\n\nmysum(3, 4)\n\n7\n\n\n\ntypeof(mysum(3, 4))\n\nInt64\n\n\n\nmysum(\"hello\", 4.8)\n\n\nMethodError: no method matching mysum(::String, ::Float64)\n\nClosest candidates are:\n  mysum(::Float64, ::Float64)\n   @ Main In[5]:1\n  mysum(::Union{Float16, Float32, Float64, Int16, Int32, Int64}, ::Union{Float16, Float32, Float64, Int16, Int32, Int64})\n   @ Main In[11]:2\n  mysum(::Real, ::Real)\n   @ Main In[15]:1\n\n\nStacktrace:\n [1] top-level scope\n   @ In[19]:1\n\n\n\n\n\nType declarations for function output\n\nfunction mysum2(x::Real, y::Real)::Float64\n  return x+y\nend\n\ntypeof(mysum2(3, 4))\n\nFloat64\n\n\nWhat do you think this will happen if we do this?\n\nfunction mysum3(x::Real, y::Real)::String\n  return x+y\nend\n\nmysum3(3, 4)\n\n\n\nTypes in arrays\nWe’ve already seen that one can create arrays of heterogeneous elements, but one can’t modify a homogeneous array in a way that would make it heterogeneous.\n\n\n\n\n\n\nExercise\n\n\n\n\nExperiment with arrays that contain missing values (via missing). What happens with computations?\nWhat seems to be the difference between missing and nothing?\n\n\n\n\n\nMore on structs and constructors\nHere’s a slightly more involved example of a struct that ties together some of what we’ve seen so far. (For use later on, we’ll make it a mutable struct so we can modify the struct elements.)\n\nmutable struct Person\n  name::String\n  age::Real\n  function Person(name::String, age::Real)\n    @assert(age &lt; 130, \"Age exceeds human lifespan.\")\n    new(name, age)\n  end\nend\n\nbiden = Person(\"Joe Biden\", 82)\n\nPerson(\"Joe Biden\", 82)\n\n\nThese invocations will both fail.\n\nlincoln = Person(\"Abraham Lincoln\", \"hello\")\n\n\nMethodError: no method matching Person(::String, ::String)\n\nClosest candidates are:\n  Person(::String, ::Real)\n   @ Main In[21]:4\n\n\nStacktrace:\n [1] top-level scope\n   @ In[22]:1\n\n\n\n\nlincoln = Person(\"Abraham Lincoln\", 200)\n\nLoadError: AssertionError: Age exceeds human lifespan.\nAssertionError: Age exceeds human lifespan.\n\nStacktrace:\n [1] Person(name::String, age::Int64)\n   @ Main ./In[21]:5\n [2] top-level scope\n   @ In[23]:1",
    "crumbs": [
      "Course Notes",
      "Notes 3: Types and dispatch"
    ]
  },
  {
    "objectID": "notes/notes3.html#multiple-dispatch",
    "href": "notes/notes3.html#multiple-dispatch",
    "title": "Notes 3: Types and dispatch",
    "section": "Multiple dispatch",
    "text": "Multiple dispatch\nWe can define different versions of functions or operators (these are called methods) that are used depending on the input types.\n\nFunctions and methods\nThe first time we define a method, it creates the generic function and initial method. When we define further methods, they are added to the existing function.\n\nfunction test(x)\n  println(\"In test, called with arbitrary input: \", x)\nend\n\nfunction test(x::String)\n  println(\"In test, called with a string: \", x)\nend\n\ntest(7)\n\nIn test, called with arbitrary input: 7\n\n\n\ntest(\"hello\")\n\nIn test, called with a string: hello\n\n\nIt’s called multiple dispatch because the dispatching can depend on multiple arguments and not just the first, which is not generally the case in other languages.\n\n\nOperator overloading is multiple dispatch\nMultiple dispatch is basically what is called “overloading” in other languages.\nLet’s illustrate multiple dispatch with our mutable Person struct.\n\nimport Base.+\n\nfunction +(person::Person, incr::Real)\n  person.age += incr\n  return person\nend\n\nbarack = Person(\"Barack Obama\", 60)\nbarack + 3;\nbarack\n\nPerson(\"Barack Obama\", 63)\n\n\nNow we’ll set up another + method for adding two Persons.\n\nstruct Partnership\n  person1::Person\n  person2::Person\n  year_formed::Int\nend\n\nfunction +(personA::Person, personB::Person)::Partnership\n  return Partnership(personA, personB, 1990)\nend\n\nmichelle = Person(\"Michelle Obama\", 60)\n\nobamas = barack + michelle\n\nPartnership(Person(\"Barack Obama\", 63), Person(\"Michelle Obama\", 60), 1990)\n\n\nThat’s a decent illustration of the power of multiple dispatch but it’s awkward to hard-code in the “1990” in this case. If we were using multiple dispatch with a function, then we wouldn’t be restricted to having two arguments.\nWe’ve now added a couple more + methods to the large number already existing in Julia’s Base module/library.\nNote the similarity to object-oriented programming, but the methods are not part of classes.\n\n\n\n\n\n\nExtensibility\n\n\n\nThis an example of extensibility. Julia’s core functionality will work with user-defined objects.\n\n\n\n\nShow methods\nThe same idea extends to other core Julia functions/functionality, such as printing objects.\nHere we’ll overload the Base show function. Before we do so, let’s see what existing show methods there are:\n\nBase.show\n# methods(Base.show)\n\nprint(myGP)\n\nfunction Base.show(io::IO, gp::GaussianProcess)\n  println(\"A Gaussian process defined on a grid from $(gp.x[1]) to $(gp.x[end]).\")\nend\n\nprint(myGP)\nmyGP  \n\nGaussianProcess([0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0], expCov, (1, 0.5))A Gaussian process defined on a grid from 0.0 to 1.0.\n\n\n\n\n\nA Gaussian process defined on a grid from 0.0 to 1.0.\n\n\nThis is similar to combining the __str__ and __repr__ “dunder” (double underscore) methods for Python classes.",
    "crumbs": [
      "Course Notes",
      "Notes 3: Types and dispatch"
    ]
  },
  {
    "objectID": "notes/notes3.html#subtyping",
    "href": "notes/notes3.html#subtyping",
    "title": "Notes 3: Types and dispatch",
    "section": "Subtyping",
    "text": "Subtyping\nTypes can relate to each other. Generally in Julia one groups types as subtypes of an abstract type whose purpose is to relate the subtypes. This is similar to class inheritance in object-oriented programming.\n\nabstract type GeneralPerson end\n\nmutable struct President &lt;: GeneralPerson\n    name::String\n    age::Real\n    inauguration_year::Int64\nend\n\nmutable struct Employee &lt;: GeneralPerson\n    name::String\n    age::Real\n    employer::String\nend\n\nThe benefit is that we can define a single function/operator that works with multiple subtypes.\n\n# These will work with both Employee and President\nfunction +(person::GeneralPerson, incr::Real)\n  person.age += incr\n  return person\nend\n\nfunction Base.show(io::IO, person::GeneralPerson)\n  println(\"A person of age $(person.age).\")\nend\n\nmacron = President(\"Emmanuel Macron\", 48, 2017)\n\n\n\n\nA person of age 48.\n\n\n\nmacron + 1\n\n\n\n\nA person of age 49.\n\n\nWell that worked as an illustration, but in this case it’s not clear that macron is a special kind of person based on the printout. So we probably would want special show methods for the concrete types.\n\nAbstract types vs. concrete types\nThe purpose of abstract types is to organize types into a hierarchy. In principle, one doesn’t create instances of an abstract type (hence “abstract”), only of a concrete type, and yet, one can do Real(4) and Number(4), even though those are abstract types, so I’m not fully understanding something here.\n\n\nType ordering\nWe can check if a type is a subtype like this:\n\nInt64 &lt;: Real\n\ntrue\n\n\n\nReal &lt;: Number\n\ntrue\n\n\n\nNumber &lt;: Any\n\ntrue\n\n\n\nEmployee &lt;: GeneralPerson\n\ntrue\n\n\n\nEmployee &lt;: Float64\n\nfalse\n\n\n\nGeneralPerson &gt;: Employee\n\ntrue\n\n\n\n\n\n\n\n\nUse of :\n\n\n\nNote that : appears in various ways related to Julia code structure:\n\nx::Real        # type declaration\nInt64 &lt;: Real  # type hierarchy\n:x             # representing a variable as code in meta programming\n\n\n\n\n\nNested subtypes\nOne can’t have subtypes of non-abstract (concrete) types.\nHere’s what the Julia manual has to say: “It turns out that being able to inherit behavior is much more important than being able to inherit structure, and inheriting both causes significant difficulties in traditional object-oriented languages.”\nSo the idea seems to be that there’s not much point to having one type that inherits the fields (structure) of another type. Though I’m not sure what the difficulties are in inheriting both.\n\nmutable struct Electrician &lt;: Employee\n    name::String\n    age::Real\n    employer::String\n    times_electrocuted::Int\nend\n\nLoadError: invalid subtyping in definition of Electrician: can only subtype abstract types.\ninvalid subtyping in definition of Electrician: can only subtype abstract types.\n\nStacktrace:\n [1] top-level scope\n   @ In[38]:1\n\n\n\n\nParametric types\nThere’s even more flexibility in terms of types, if we parameterize types.\nWe can use T as a parametric type that accommodates various types (in this case only numeric ones).\nThis is not a particularly realistic example, but it enforces that both the age and inauguration_year have the same numeric type.\n\nmutable struct USPresident{T &lt;: Real} &lt;: GeneralPerson\n    name::String\n    age::T\n    inauguration_year::T\nend\n\nUSPresident(\"Grant\", 125, 1868)\n\n\n\n\nA person of age 125.\n\n\n\nUSPresident(\"Grant\", 125.3, 1868.0)\n\n\n\n\nA person of age 125.3.\n\n\n\nUSPresident(\"Grant\", 125.3, 1868);\n\n\nMethodError: no method matching USPresident(::String, ::Float64, ::Int64)\n\nClosest candidates are:\n  USPresident(::String, ::T, ::T) where T&lt;:Real\n   @ Main In[39]:2\n\n\nStacktrace:\n [1] top-level scope\n   @ In[41]:1",
    "crumbs": [
      "Course Notes",
      "Notes 3: Types and dispatch"
    ]
  },
  {
    "objectID": "notes/notes5.html",
    "href": "notes/notes5.html",
    "title": "Notes 5: Efficiency",
    "section": "",
    "text": "This document is the fifth of a set of notes, this document focusing on writing efficient Julia code. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.",
    "crumbs": [
      "Course Notes",
      "Notes 5: Efficiency"
    ]
  },
  {
    "objectID": "notes/notes5.html#introduction",
    "href": "notes/notes5.html#introduction",
    "title": "Notes 5: Efficiency",
    "section": "",
    "text": "This document is the fifth of a set of notes, this document focusing on writing efficient Julia code. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.",
    "crumbs": [
      "Course Notes",
      "Notes 5: Efficiency"
    ]
  },
  {
    "objectID": "notes/notes5.html#timing",
    "href": "notes/notes5.html#timing",
    "title": "Notes 5: Efficiency",
    "section": "Timing",
    "text": "Timing\nBeing able to time code is critical for understanding and improving efficiency.\n\n\n\n\n\n\nCompilation time\n\n\n\nWith Julia, we need to pay particular attention to the effect of just-in-time (JIT) compilation on timing. The first time a function is called with specific set of argument types, Julia will compile the method that is invoked. We generally don’t want to time the compilation, only the run time, assuming the function will be run repeatedly with a given set of argument types.\n\n\n@time is a macro that will time some code. However, it’s better to use @btime from BenchmarkTools as that will run the code multiple times and will make sure not to count the compilation time.\n\nfunction myexp!(x)\n  for i in 1:length(x)\n    x[i] = exp(x[i])\n  end\nend\n\nn = Int(1e7)\ny = rand(n);\n@time myexp!(y) ## Compilation time included.\n\n  0.074654 seconds (2.62 k allocations: 178.000 KiB, 10.70% compilation time)\n\n\n\ny = rand(n);\n@time myexp!(y) ## Compilation time not included.\n\n  0.066447 seconds\n\n\n\nusing BenchmarkTools\n\ny = rand(n);\n@btime myexp!(y) \n\n  58.960 ms (0 allocations: 0 bytes)\n\n\n\n\n\n\n\n\nExercise\n\n\n\nHow long does that loop take in R or Python? What about a vectorized solution in R or Python?\n\n\nWe can time a block of code, but I’m not sure what Julia does in terms of JIT for code that is not in functions. You may discover more in working on the fourth problem of PS2.\n\n@btime begin\ny = 3\nz = 7\nend\n\n  1.299 ns (0 allocations: 0 bytes)\n\n\n7",
    "crumbs": [
      "Course Notes",
      "Notes 5: Efficiency"
    ]
  },
  {
    "objectID": "notes/notes5.html#profiling",
    "href": "notes/notes5.html#profiling",
    "title": "Notes 5: Efficiency",
    "section": "Profiling",
    "text": "Profiling\nProfiling involves timing each step in a set of code. One can use the Profile module to do this in Julia.\nOne thing to keep in mind when profiling is whether the timing for nested function calls is included in the timing of the function that makes the nested function calls.\n\nusing Profile\n\nfunction ols_slow(y::Vector{&lt;:Number}, X::Matrix{&lt;:Number})\n    xtx = X'X; \n    xty = X'y;\n    xtxinverse = inv(xtx);  ## This is an inefficient approach.\n    return xtxinverse * xty\nend\n\nn = Int(1e4)\np = 2000\ny = randn(n);\nX = randn((n,p));\n\n## Run once to avoid profiling JIT compilation.\ncoefs = ols_slow(y, X);\n\nDirectly interpreting the Profile output can be difficulty. In this case, if we ran the following code, we’d see very long, hard-to-interpret information.\n\n@profile coefs = ols_slow(y, X)\nProfile.print()  \n\nInstead let’s try a visualization. There are other Julia packages for visualizing profiler output. Some might be better than this. (I tried ProfileView and liked StatProfilerHTML better.)\n\nusing ProfileView\n@profview ols_slow(y, X)\n\nusing StatProfilerHTML\n@profilehtml ols_slow(y, X)\n\n@profilehtml produces [this output](statprof/index.html), which can in some ways be hard to interpret, but the color-coded division betweeninv,and` gives us an idea of where time is being spent. That output might not show up fully in the links - you might need to run the code above yourself.",
    "crumbs": [
      "Course Notes",
      "Notes 5: Efficiency"
    ]
  },
  {
    "objectID": "notes/notes5.html#pre-allocation",
    "href": "notes/notes5.html#pre-allocation",
    "title": "Notes 5: Efficiency",
    "section": "Pre-allocation",
    "text": "Pre-allocation\nIn R (also with numpy arrays in Python), it’s a bad idea to iteratively increase the size of an object, such as doing this:\nn &lt;- 5000\nx &lt;- 1\nfor(i in 2:n)\n  x &lt;- c(x, i)\nPython lists handle this much better by allocating increasingly large additional amounts of memory as the object grows when using .append().\nLet’s consider this in Julia.\n\nfunction fun_prealloc(n) \n  x = zeros(n);\n  for i in 1:n\n    x[i] = i;\n  end\n  return x\nend\n\nfunction fun_grow(n) \n  x = Float64[];\n  for i in 1:n\n    push!(x, i);\n  end\n  return x\nend\n\nusing BenchmarkTools\n\nn = 100000000\n@btime x1 = fun_prealloc(n);\n\n  334.486 ms (2 allocations: 762.94 MiB)\n\n\n\n@btime x2 = fun_grow(n);\n\n  1.718 s (23 allocations: 1019.60 MiB)\n\n\nThat indicates that it’s better to pre-allocate memory in Julia, but the time does not seem to grow as order of \\(n^2\\) as it does in R or with numpy arrays. So that suggests Julia is growing the array in a smart fashion.\nWe can verify that by looking at the memory allocation information returned by @btime.\nFor fun_prealloc, we see an allocation of ~800 MB, consistent with allocating an array of 100 million 8 byte floats. (It turns out the “second” allocation occurs because we are running @btime in the global scope).\nFor fun_grow, we see 23 allocations of ~1 GB, consistent with Julia growing the array in a smart fashion but with some additional memory allocation.\nIf the array were reallocated each time it grew by one, we’d allocate and copy \\(1+2+\\cdots+n = n (n+1)/2\\) numbers in total over the course of the computation (but not all at once), which would take a lot of time.",
    "crumbs": [
      "Course Notes",
      "Notes 5: Efficiency"
    ]
  },
  {
    "objectID": "notes/notes5.html#vectorization",
    "href": "notes/notes5.html#vectorization",
    "title": "Notes 5: Efficiency",
    "section": "Vectorization",
    "text": "Vectorization\nAs we’ve seen, the vectorized versions of functions have a dot after the function name (or before an operator).\n\nx = [\"spam\", 2.0, 5, [10, 20]]\nlength(x)\n\n4\n\n\n\nlength.(x)\n\n4-element Vector{Int64}:\n 4\n 1\n 1\n 2\n\n\n\nmap(length, x)\n\n4-element Vector{Int64}:\n 4\n 1\n 1\n 2\n\n\n\nx = [2.1, 3.1, 5.3, 7.9]\nx .+ 10\n\n4-element Vector{Float64}:\n 12.1\n 13.1\n 15.3\n 17.9\n\n\n\nx + x\n\n4-element Vector{Float64}:\n  4.2\n  6.2\n 10.6\n 15.8\n\n\n\nx .&gt; 5.0\n\n4-element BitVector:\n 0\n 0\n 1\n 1\n\n\n\nx .== 3.1\n\n4-element BitVector:\n 0\n 1\n 0\n 0\n\n\nUnlike in Python or R, it shouldn’t matter for efficiency if you use a vectorized function or write a loop, because with Julia’s just-in-time compilation, the compiled code should be similar. (This assumes your code is inside a function.) So the main appeal of vectorization is code clarity and ease of writing the code.\nWe can automatically use the dot vectorization with functions we write:\n\nfunction plus3(x)\n  return x + 3\nend\n\nplus3.(x)\n\n4-element Vector{Float64}:\n  5.1\n  6.1\n  8.3\n 10.9\n\n\nThis invokes broadcast(plus3, args...).\nBroadcasting will happen over multiple arguments if more than one argument is an array.\nConsider the difference between the following vectorized calls:\n\nx = randn(5)\nσ = 10;\ny1 = x .+ σ .* randn.()\ny2 = x .+ σ .* randn()\nprint((y1 - x) / σ)\nprint((y2 - x) / σ)\n\n[0.14677616340308833, 0.581444839867881, -0.3787768358297307, 1.236223662587814, 0.3476772389317324][-0.2981459541428111, -0.2981459541428111, -0.2981459541428111, -0.2981459541428111, -0.2981459541428111]\n\n\nThat’s perhaps a bit surprising given one might think that because the multiplication is done first, the σ .* randn.() might produce a scalar, as it does if you just run σ .* randn.() on its own.",
    "crumbs": [
      "Course Notes",
      "Notes 5: Efficiency"
    ]
  },
  {
    "objectID": "notes/notes5.html#loop-fusion",
    "href": "notes/notes5.html#loop-fusion",
    "title": "Notes 5: Efficiency",
    "section": "Loop fusion",
    "text": "Loop fusion\nIf one runs a vectorized calculation that involves multiple steps in a language like R or Python, there are some inefficiencies.\nConsider this computation:\nx = tan(x) + 3*sin(x)\nIf run as vectorized code in a language like R or Python, it’s much faster than using a loop, but it does have some downsides.\n\nFirst, it will use additional memory (temporary arrays will be created to store tan(x), sin(x), 3*sin(x)). (We can consider what the abstract syntax tree would be for that calculation.)\nSecond, multiple for loops will have to get executed when the vectorized code is run, looping over the elements of x to calculate tan(x), sin(x), etc. (For example in R or Python/numpy, multiple for loops would get run in the underlying C code.)\n\nIn contrast, running via a for loop (in R or Python or Julia) avoids the temporary arrays and involves a single loop:\n\nfor i in 1:length(x)\n    x[i] = tan(x[i]) + 3*sin(x[i])\nend\n\nThankfully, Julia “fuses” the loops of vectorized code automatically when one uses the dot syntax for vectorization, so one shouldn’t suffer from the downsides of vectorization. One could of course use a loop in Julia, and it should be fast, but it’s more code to write and harder to read.\n\nMemory allocation with loop fusion\nLet’s look at memory allocation when putting the code into a function:\n\nfunction mymath(x)\n   return tan(x) + 3*sin(x)\nend\n\nfunction mymathloop(x)\n  for i in 1:length(x)\n    x[i] = tan(x[i]) + 3*sin(x[i])\n  end\n  return x\nend \n\nn = 100000000;\nx = rand(n);\n\n@btime y = mymath.(x);\n\n  2.407 s (3 allocations: 762.94 MiB)\n\n\n\n@btime y = mymathloop(x);\n\n  3.115 s (0 allocations: 0 bytes)\n\n\nNote that it appears only 800 MB (~760 MiB; ~0.95 MiB = 1 MB) are allocated (for the output) in the (presumably) fused operation, rather than multiples of 800 MB for various temporary arrays that one might expect to be created.\nAnd in the loop, there is no allocation. We might expect some allocation of scalars, but those are probably handled differently than allocating memory for arrays off the heap. I’ve seen some information for how Julia handles allocation of space for immutable objects (including scalars and strings), but I hvaen’t had a chance to absorb that.\n\n\nCases without loop fusion\nWe can do addition or subtraction of two arrays or multiplication/division with array and scalar without the “dot” vectorization. However, as seen with the additional memory allocation here, the loop fusion is not done.\n\nfunction mymath2(x)\n   return 3*x+x/7\nend\n\n@btime y = mymath2(x);\n\n  1.078 s (6 allocations: 2.24 GiB)\n\n\nIn contrast, here we see only the allocation for the output object.\n\n@btime y = mymath2.(x);\n\n  454.045 ms (3 allocations: 762.94 MiB)",
    "crumbs": [
      "Course Notes",
      "Notes 5: Efficiency"
    ]
  },
  {
    "objectID": "notes/notes5.html#cache-aware-programming-and-array-storage",
    "href": "notes/notes5.html#cache-aware-programming-and-array-storage",
    "title": "Notes 5: Efficiency",
    "section": "Cache-aware programming and array storage",
    "text": "Cache-aware programming and array storage\nJulia stores the values in a matrix contiguously column by column (and analogously for higher-dimensional arrays).\nWe should therefore access matrix elements within a column rather than within a row. Why is that?\n\n\n\n\n\n\nMemory access and the cache\n\n\n\nWhen a value is retrieved from main memory into the CPU cache, a block of values will be retrieved, and those will generally include the values in the same column but (for large enough arrays) not all the values in the same row. If subsequent operations work on values from that column, the values won’t need to be moved into the cache. (This is called a “cache hit”).\n\n\nLet’s first see if it makes a difference when using Julia’s built-in sum function, which can do the reduction operation on various dimensions of the array.\n\nusing Random\nusing BenchmarkTools\n\nnr = 800000;\nnc = 100;\nA = randn(nr, nc);    # long matrix\ntA = randn(nc, nr);   # wide matrix\n\nfunction sum_by_column(X)\n    return sum(X, dims=1) \nend\n\nfunction sum_by_row(X)\n    return sum(X, dims=2)  \nend\n\n@btime tmp = sum_by_column(A);\n\n  38.943 ms (1 allocation: 896 bytes)\n\n\n\n@btime tmp = sum_by_row(tA);\n\n  42.923 ms (5 allocations: 976 bytes)\n\n\nThere’s little difference.\nAre we wrong about how the cache works? Probably not; rather it’s probably that Julia’s sum() is set up to take advantage of how the cache works by being careful about the order of operations used to sum the rows or columns.\n\n\n\n\n\n\nExercise\n\n\n\nHow could you program the for loops involved in row-wise summation to be efficient when a matrix is stored column-major given how caching work? If you retrieve the data by column, how do you get the row sums?\n\n\nIn contrast, if we manually loop over rows or columns, we do see a big (almost order-of-magnitude) difference.\n\n@btime tmp = [sum(A[:,col]) for col in 1:size(A,2)];\n\n  133.829 ms (405 allocations: 610.36 MiB)\n\n\n\n@btime tmp = [sum(A[row,:]) for row in 1:size(A,1)];\n\n  779.155 ms (4798474 allocations: 750.71 MiB)\n\n\nSo while one lesson is to code with the cache in mind, another is to use built-in functions that are probably written for efficiency.\n\n\n\n\n\n\nExercise\n\n\n\nIn your own work, can you think of an algorithm and associated data structures where one has to retrieve a lot of data and one would want to think about cache hits and misses? In general the idea is that if you retrieve a value, try to make use of the nearby values at that same time, rather than retrieving the nearby values later on in the computation.\n\n\n\nStore values contiguously in memory\nIf we are storing an array of all the same type of values, these can be stored contiguously. That’s not the case with abstract types.\nFor example, here Real values can vary in size.\n\na = Real[]\nsizeof(a)\npush!(a, 3.5)\nsizeof(a)\npush!(a, Int16(2))\nsizeof(a[2])\nsizeof(a)\n\n16\n\n\nAnd we see that having an array of Reals is bad for performance. As part of this notice the additional allocation.\n\nusing LinearAlgebra\nn = 100;\nA = rand(n, n);\n@btime tmp = A'A;  # Equivalent to A' * A or transpose(A) * A.\n\n  32.629 μs (3 allocations: 78.19 KiB)\n\n\n100×100 Matrix{Float64}:\n 36.9452  24.6027  29.9873  28.0731  …  28.1847  28.5245  24.4188  24.118\n 24.6027  27.4659  26.1276  23.3942     23.4948  22.6057  21.3434  22.1072\n 29.9873  26.1276  40.6052  29.5876     29.6133  31.3339  26.9341  30.7159\n 28.0731  23.3942  29.5876  35.9484     26.1913  28.8578  25.5692  25.8774\n 27.7808  23.7795  31.4558  29.0977     28.4308  30.2666  26.0004  28.154\n 23.5452  19.4437  26.9979  24.5491  …  23.7654  26.7213  22.5085  23.4483\n 24.2351  20.9448  27.1727  24.9626     25.5764  27.1748  22.9908  25.1908\n 24.2352  21.1566  27.9748  26.2707     24.5957  27.2691  21.3511  27.2015\n 28.0796  25.104   30.0434  27.1167     27.8067  28.7391  22.4229  28.0245\n 25.212   21.6682  27.5374  25.7883     24.3316  27.0613  22.3679  25.011\n 24.8338  20.8862  27.0238  24.971   …  24.3629  25.4161  22.7082  27.2442\n 28.1029  24.5713  31.3381  29.325      29.0279  31.3595  25.7613  32.1622\n 27.7427  23.8843  32.116   28.6105     26.8459  30.8597  24.8898  28.3011\n  ⋮                                  ⋱                             \n 28.2413  22.4823  31.8238  28.0718     28.5363  29.8544  25.617   30.8102\n 25.9031  22.3158  30.2684  26.1889     26.8291  29.3049  24.9818  27.6305\n 24.4902  21.7182  27.3646  25.1295  …  24.8507  26.6667  23.323   27.5324\n 26.8904  22.4576  29.1816  26.7035     26.9178  29.1504  23.4181  27.3258\n 25.3568  20.0535  25.8843  24.9667     24.4746  26.4892  22.1566  25.2715\n 23.7338  19.7943  25.0737  23.2836     22.768   24.5572  21.7913  24.1243\n 24.5237  20.9264  28.9895  25.7668     26.1901  27.8939  23.8366  26.1041\n 25.6805  22.8274  27.4589  24.8699  …  25.0053  26.1065  22.3841  25.4258\n 28.1847  23.4948  29.6133  26.1913     33.458   29.1317  24.1902  28.0532\n 28.5245  22.6057  31.3339  28.8578     29.1317  38.251   25.3375  28.9003\n 24.4188  21.3434  26.9341  25.5692     24.1902  25.3375  30.1251  24.9156\n 24.118   22.1072  30.7159  25.8774     28.0532  28.9003  24.9156  38.3942\n\n\n\nrA = convert(Array{Real}, A);\n@btime tmp = rA'rA;\n\n  41.719 ms (2030004 allocations: 31.05 MiB)",
    "crumbs": [
      "Course Notes",
      "Notes 5: Efficiency"
    ]
  },
  {
    "objectID": "notes/notes5.html#lookup-speed",
    "href": "notes/notes5.html#lookup-speed",
    "title": "Notes 5: Efficiency",
    "section": "Lookup speed",
    "text": "Lookup speed\nIf we have code that needs to retrieve a lot of values from a data structure, it’s worth knowing the situations in which we can expect that lookup to be fast.\nLookup in arrays is fast (\\(O(1)\\); i.e., not varying with the size of the array) because of the “random access” aspect of RAM (random access memory).\n\nn=Int(1e7);\n\nx = randn(n);\nind = Int(n/2);\n@btime x[ind];\n\n  19.324 ns (1 allocation: 16 bytes)\n\n\n\ny = rand(10);\n@btime y[5];\n\n  19.481 ns (1 allocation: 16 bytes)\n\n\nNext, lookup in a Julia dictionary is fast \\(O(1)\\) because dictionaries using hashing (like Python dictionaries and R environments).\n\nfunction makedict(n)\n  d=Dict{String,Int}()\n  for i in 1:n\n    push!(d, string(i) =&gt; i)\n  end\n  return d\nend\n\n## Make a large dictionary, with keys equal to strings representing integers.\nd = makedict(n);\nindstring = string(ind);\n@btime d[indstring]; \n\n  40.196 ns (1 allocation: 16 bytes)\n\n\nFinally, let’s consider tuples. Lookup by index is quite slow, which is surprising as I was expecting it to be similar to lookup in the array, as I think the tuple in this case has values stored contiguously.\n\nxt = Tuple(x);\n@btime xt[ind];  \n\n  49.652 ms (1 allocation: 16 bytes)\n\n\nFor named tuples, I’m not sure how realistic this is, since it would probably be a pain to create a large named tuple. But we see that lookup by name is slow, even though we are using a smaller tuple than the array and dictionary above.\n\n## Set up a named tuple (this is very slow for large array, so use a subset).\ndsub = makedict(100000);\nxsub = x[1:100000];\nnames = Symbol.('x' .* keys(dsub));  # For this construction of tuple, the keys need to be symbols.\nxtnamed = (;zip(names, xsub)...); \n@btime xtnamed.x50000;\n\n  60.522 μs (1 allocation: 16 bytes)\n\n\n\n\n\n\n\n\nDeveloping a perspective on speed\n\n\n\nNote that while all the individual operations above are fast in absolute terms for a single lookup, for simple operations, we generally want them to be really fast (e.g., order of nanoseconds) because we’ll generally be doing a lot of such operations for any sizeable overall computation.",
    "crumbs": [
      "Course Notes",
      "Notes 5: Efficiency"
    ]
  },
  {
    "objectID": "notes/notes5.html#performance-tips",
    "href": "notes/notes5.html#performance-tips",
    "title": "Notes 5: Efficiency",
    "section": "Performance tips",
    "text": "Performance tips\nThe Julia manual has an extensive section on performance.\nWe won’t dive too deeply into all the complexity, but here are a few key tips, which mainly relate to writing in a way that is aware of the JIT compilation that will happen:\n\nCode for which performance is important should be inside a function, as this allows for JIT compilation.\nAvoid use of global variables that don’t have a type, as that is hard to optimize since the type could change.\nThe use of immutable objects can improve performance.\nHave functions always return the same type and avoid changing (or unknown) variable types within a function.",
    "crumbs": [
      "Course Notes",
      "Notes 5: Efficiency"
    ]
  },
  {
    "objectID": "notes/notes1.html",
    "href": "notes/notes1.html",
    "title": "Notes 1: Introduction",
    "section": "",
    "text": "This document is the first of a set of notes. It gives an overview of key syntax, tools, and concepts for using Julia. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.\nThis document covers basic syntax, basic types, data structures, and functions. For more information, Think Julia Chapters 1-12 is a good reference.",
    "crumbs": [
      "Course Notes",
      "Notes 1: Introduction"
    ]
  },
  {
    "objectID": "notes/notes1.html#introduction",
    "href": "notes/notes1.html#introduction",
    "title": "Notes 1: Introduction",
    "section": "",
    "text": "This document is the first of a set of notes. It gives an overview of key syntax, tools, and concepts for using Julia. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.\nThis document covers basic syntax, basic types, data structures, and functions. For more information, Think Julia Chapters 1-12 is a good reference.",
    "crumbs": [
      "Course Notes",
      "Notes 1: Introduction"
    ]
  },
  {
    "objectID": "notes/notes1.html#variables-and-types",
    "href": "notes/notes1.html#variables-and-types",
    "title": "Notes 1: Introduction",
    "section": "Variables and types",
    "text": "Variables and types\n\nBasic types\nLet’s start by defining some variables and seeing what their types are.\n\ntypeof(2)\n\nInt64\n\nx = 2.0\n\n2.0\n\ntypeof(x)\n\nFloat64\n\ns = \"hello\"\n\n\"hello\"\n\ntypeof(s)\n\nString\n\ntypeof(s[1])\n\nChar\n\ntypeof('\\n')\n\nChar\n\n## Unicode characters\n'h'\n\n'h': ASCII/Unicode U+0068 (category Ll: Letter, lowercase)\n\n'i'\n\n'i': ASCII/Unicode U+0069 (category Ll: Letter, lowercase)\n\n'\\n'\n\n'\\n': ASCII/Unicode U+000A (category Cc: Other, control)\n\n'θ'\n\n'θ': Unicode U+03B8 (category Ll: Letter, lowercase)\n\n\ny = (3, 7.5)\n\n(3, 7.5)\n\ntypeof(y)\n\nTuple{Int64, Float64}\n\n\nAs we’ll be discussing more, knowing what type a variable is (particularly for large objects such as large arrays) is important for thinking about memory use, what methods work with what types of variables, and when variables need to be cast/coerced to a different type.\n\n\n\n\n\n\nWarning\n\n\n\nThe Unicode/LaTeX characters may not show up in the PDF version of this document.\n\n\nWe can enter LaTeX characters/formatting by typing in LaTeX syntax (starting with a \\) and then TAB.\n\nθ = 3.57  # \\theta TAB\n\n3.57\n\n\n#=\nNote the use of a comment\nin the initial line.\n\nAnd this here is a multi-line comment.\n=#\n\nx₁ = 7  # x\\_1 TAB\n\n7\n\n\n# Try \\theta TAB \\bar TAB = 7 (it works in some contexts).\n\n水=5\n\n5\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDo you like the idea of using non-ASCII characters for variable names?\n\n\n\n\nA bit about strings\n\nx = 'hello'\n\nx = \"hello\"\nx[1] = \"a\"\n\n\n\nCasting/coercing between types\n\nstring(32)\n\n\"32\"\n\nparse(Float64, \"32.5\")\n\n32.5\n\n\nSome languages (such as R) will often cast between types behind the scenes. With Julia, one is often more deliberate about types as we’ll see.\n\n\nMore on types and comparisons\n\nx = 3\n\n3\n\ny = 3.0\n\n3.0\n\nx == y\n\ntrue\n\nx ≠ y\n\nfalse\n\nx &gt; y\n\nfalse\n\nx &gt; y || x &lt;= y\n\ntrue\n\n\nisa(x, Int)\n\ntrue\n\ny isa Int\n\nfalse\n\ny isa Number\n\ntrue\n\n\n'a' ∈ \"banana\"  # \\in TAB\n\ntrue\n\n'a' ∉ \"banana\"  # \\notin TAB\n\nfalse\n\n\naString = \"a\"\n\n\"a\"\n\n'a' == aString\n\nfalse\n\n'a' == aString[1]\n\ntrue\n\n\n\n\nConditional (if-else) statements\n\nif x &lt; y\n    println(\"x is less than y\")\nelseif x &gt; y\n    println(\"x is greater than y\")\nelse\n    println(\"x and y are equal\")\nend\n\nx and y are equal\n\n\n\n\nPrinting and string interpolation\nWe can use variables in print statements in various ways.\n\nperson = \"Alice\"\n\n\"Alice\"\n\n\nperson = \"Alice\";\n\n\"Hello, $(person) with name of length $(length(person)).\"\n\n\"Hello, Alice with name of length 5.\"\n\n\nprintln(\"Hello, \", person, \" with name of length \", length(person), \".\")\n\nHello, Alice with name of length 5.\n\nprintln(\"Hello, $(person) with name of length $(length(person)).\")\n\nHello, Alice with name of length 5.\n\nprintln(\"Hello, \" * person * \" with name of length \" * string(length(person)) * \".\")\n\nHello, Alice with name of length 5.",
    "crumbs": [
      "Course Notes",
      "Notes 1: Introduction"
    ]
  },
  {
    "objectID": "notes/notes1.html#functions-and-operators",
    "href": "notes/notes1.html#functions-and-operators",
    "title": "Notes 1: Introduction",
    "section": "Functions and operators",
    "text": "Functions and operators\n\nOperators (and not just for math/booleans)\n\nvalue = 7;\nvalue *= 3;\nvalue\n\n21\n\n\n\nValue\n\nERROR: UndefVarError: `Value` not defined\n\nx = 3\n\n3\n\ntmp = 7x   # Unlike any other language I know!\n\n21\n\n\ns * \" there\"\n\n\"hello there\"\n\n\ns^4\n\n\"hellohellohellohello\"\n\n\n\n\nGetting help on functions\nType ? to get into help mode, then the name of the function you want help on.\nTo see all the functions/operators available in base Julia, type “Base.” and hit tab.\n\n\nFunction definitions\n\nfunction plus3(x=0)\n  return 3+x\nend\n\nplus3 (generic function with 2 methods)\n\n\nplus3(5)\n\n8\n\n\nWhy are there two methods?\n\nmethods(plus3)\nmethods(+)\n\n\n\nVectorized use\nTo use a function (or operator) in a vectorized way, we (with exceptions) need to use the dot notation.\n\ny = [5.3, 2.5];\n\n\ny + 3\nplus3(y)\n\nERROR: MethodError: no method matching +(::Vector{Float64}, ::Int64)\nFor element-wise addition, use broadcasting with dot syntax: array .+ scalar\n\ny .+ 3\n\n2-element Vector{Float64}:\n 8.3\n 5.5\n\nplus3.(y)\n\n2-element Vector{Float64}:\n 8.3\n 5.5\n\n\n\n## Apparently no general \"recycling\"/broadcasting.\nx = [2.1, 3.1, 5.3, 7.9]\nx .+ [0., 100.]\n\nERROR: DimensionMismatch: arrays could not be broadcast to a common size; got a dimension with lengths 4 and 2\n\n\nPositional and keyword arguments\nPositional arguments (which are matched based on the order they are given) are specified before keyword arguments.\n\nfunction norm(x, p; verbose, extra)\n  if verbose  # We'll see that \"logging\" is a better way to do this.\n     println(\"Executing $(p)-norm.\")\n  end\n  if !isfinite(p) && p &gt; 0\n    return maximum(abs.(x))\n  end \n  return sum(x .^ p)^(1/p)\nend\n\nz = [3.3, 4.7, -2.2]\n\nnorm(z, 2, verbose=false, extra=0)\nnorm(z, 2; verbose=false, extra=0)\nnorm(z, 2, false, 0)\nnorm(z, p=1; verbose=false, extra=0)\nnorm(z, 1, extra=0, verbose=false)\n\nArguments can have defaults:\n\nfunction norm(x, p=2; verbose=false)\n  if verbose  # We'll see that \"logging\" is a better way to do this.\n     println(\"Executing $(p)-norm.\")\n  end\n  return sum(x .^ p)^(1/p)\nend\n\nnorm (generic function with 2 methods)\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry out various argument orders and giving or not giving names or values to the arguments and try to figure out the syntax rules of how Julia behaves. Think about how they are similar/different to your primary language and whether you like the syntax rules.\n\n\nKeyword arguments are generally used for controlling function behavior rather than as core inputs. They are not involved in multiple dispatch (more later).\nLet’s try asking a ChatBot to write a norm function in Julia.\n\n\n\n\n\n\nExercise\n\n\n\nWrite a function that implements the gamma density, \\[ f(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x), \\] for shape \\(\\alpha\\) and rate \\(\\beta\\) or scale \\(1/\\beta\\), with \\(x&gt;0, \\alpha&gt;0, \\beta&gt;0\\). Allow it to handle either the rate or scale parameterization and to return either the density or log density. Check that it works in a vectorized way for the random variable value and the parameters. Compare what you wrote to what a ChatBot gives.\n\n\n\n\nShorthand function definitions\nThese can be handy, but as a newcomer to Julia, I find them a bit hard to read.\n\nplus3a(x=1) = 3+x\n\nplus3b = (x=1) -&gt; 3+x\n\n# An anonymous function (useful for maps, functional programming).\n((x) -&gt; 3+x)\n\n((x) -&gt; 3+x)(7)",
    "crumbs": [
      "Course Notes",
      "Notes 1: Introduction"
    ]
  },
  {
    "objectID": "notes/notes1.html#arrays-and-subsetting",
    "href": "notes/notes1.html#arrays-and-subsetting",
    "title": "Notes 1: Introduction",
    "section": "Arrays and subsetting",
    "text": "Arrays and subsetting\n\nSequences (and slicing)\n\nsome_text = \"This is the Greek θ\"\n\n\"This is the Greek θ\"\n\nsome_text[1]\n\n'T': ASCII/Unicode U+0054 (category Lu: Letter, uppercase)\n\nsome_text[19]\n\n'θ': Unicode U+03B8 (category Ll: Letter, lowercase)\n\nsome_text[1:4]\n\n\"This\"\n\nsome_text[17:end]\n\n\"k θ\"\n\n\n\ny = [1.1, 2.1, 3.2, 4.3, 5.7]\n\n5-element Vector{Float64}:\n 1.1\n 2.1\n 3.2\n 4.3\n 5.7\n\nprintln(y)               # Original vector\n\n[1.1, 2.1, 3.2, 4.3, 5.7]\n\n\n# Slicing by index sequence:\n\nprintln(y[1:3])          # First 3 elements\n\n[1.1, 2.1, 3.2]\n\nprintln(y[1:2:4])        # All odd-numbered elements\n\n[1.1, 3.2]\n\nprintln(y[end:-1:2])     # From end back to second element in reverse\n\n[5.7, 4.3, 3.2, 2.1]\n\nprintln(y[4:3])          # Empty subset\n\nFloat64[]\n\nz = y[:]                 # All elements (copy (not alias) of original vector)\n\n5-element Vector{Float64}:\n 1.1\n 2.1\n 3.2\n 4.3\n 5.7\n\n\n# Slicing by arbitrary index vector\nprintln(y[[4,2,4,3,3,4,4]])  # Slice by index\n\n[4.3, 2.1, 4.3, 3.2, 3.2, 4.3, 4.3]\n\n\n# Slicing by boolean array\ny[[true,false,true,false,true]]   # Slice by boolean array\n\n3-element Vector{Float64}:\n 1.1\n 3.2\n 5.7\n\n\n\n\n\n\n\n\nExercise\n\n\n\nExperiment more with slicing/indexing to make sure you get it, and what errors can occur. (As an example what happens if you index beyond the extent of the object?) See also Problems 2 and 3 on PS1.\n\n\nNote that the discussion of fruits[len] in Section 7 of Think Julia is incorrect.\n\n\nArrays (i.e., lists)\n\nx = [\"spam\", 2.0, 5, Missing, [10, 20], NaN]\n\n6-element Vector{Any}:\n    \"spam\"\n   2.0\n   5\n    Missing\n    [10, 20]\n NaN\n\nlength(x)\n\n6\n\n\ntypeof(x)\n\nVector{Any} (alias for Array{Any, 1})\n\ny = [10, 20, 30, 40]\n\n4-element Vector{Int64}:\n 10\n 20\n 30\n 40\n\ntypeof(y)\n\nVector{Int64} (alias for Array{Int64, 1})\n\nx[1] = 3.3\n\n3.3\n\nx[4] = 2.7\n\n2.7\n\ntypeof(x)   # Mutable, but type doesn't change.\n\nVector{Any} (alias for Array{Any, 1})\n\n\n\n\n\n\n\n\nMath with arrays\n\n\n\nFor computational efficiency, we’d want the array to contain elements all of the same type.\nNote that languages like R and Python distinguish types intended for math (e.g., numpy arrays, R matrices) from more general types (e.g., lists). This is not the case for Julia, where the key thing is the type(s) involved.\n\n\n\n\nMulti-dimensional arrays\n\nA = [1 2 3; 4 5 6; 7 8 9]\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\nA\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\nA[2,2]\n\n5\n\nA[2,:]\n\n3-element Vector{Int64}:\n 4\n 5\n 6\n\n\nsize(A)\n\n(3, 3)\n\nsize(A, 2)\n\n3\n\n\n## Defined column-wise:\nA = [1:4  5:8  ones(Int64,4)]\n\n4×3 Matrix{Int64}:\n 1  5  1\n 2  6  1\n 3  7  1\n 4  8  1\n\n\n\n\nArrays vs. vectors\n\nones(5)\n\n5-element Vector{Float64}:\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n\nones(5, 1)\n\n5×1 Matrix{Float64}:\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n\nones(1, 5)\n\n1×5 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  1.0\n\nones(5, 5)\n\n5×5 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n\n\n## Outer product:\nones(5, 1) * ones(1, 5)\n\n5×5 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n\nones(5, 1) .* ones(1, 5)\n\n5×5 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n\n\n\n\nA bit of linear algebra\nWe do linear algebra directly on the core Array type.\n\nA = [1 2 3; 4 1 6; 7 8 1]\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  1  6\n 7  8  1\n\nA * A\n\n3×3 Matrix{Int64}:\n 30  28  18\n 50  57  24\n 46  30  70\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat do you expect to happen if you try to do matrix multiplication with a matrix with a mix of reals and integers? What would you expect if an element is a string?\n\n\nMuch more on linear algebra in a few weeks.\n\n\nMore on vectorization\n\nx = [\"spam\", 2.0, 5, [10, 20]]\n\n4-element Vector{Any}:\n  \"spam\"\n 2.0\n 5\n  [10, 20]\n\nlength(x)\n\n4\n\nlength.(x)\n\n4-element Vector{Int64}:\n 4\n 1\n 1\n 2\n\nmap(length, x)\n\n4-element Vector{Int64}:\n 4\n 1\n 1\n 2\n\n\nx = [2.1, 3.1, 5.3, 7.9]\n\n4-element Vector{Float64}:\n 2.1\n 3.1\n 5.3\n 7.9\n\nx .+ 10\n\n4-element Vector{Float64}:\n 12.1\n 13.1\n 15.3\n 17.9\n\n\nx + x\n\n4-element Vector{Float64}:\n  4.2\n  6.2\n 10.6\n 15.8\n\n\nx .&gt; 5.0\n\n4-element BitVector:\n 0\n 0\n 1\n 1\n\nx .== 3.1\n\n4-element BitVector:\n 0\n 1\n 0\n 0\n\n\n\n\nReduction\n\nA = rand(4, 5)\n\n4×5 Matrix{Float64}:\n 0.142875  0.55564   0.643497  0.699022  0.213742\n 0.692119  0.455461  0.367236  0.353614  0.18481\n 0.896094  0.100057  0.444038  0.972838  0.198488\n 0.64145   0.502393  0.96593   0.794334  0.956481\n\nsum(A)\n\n10.780119774540436\n\nsum(A, dims = 1)  # 2D array result\n\n1×5 Matrix{Float64}:\n 2.37254  1.61355  2.4207  2.81981  1.55352\n\nsum(A, dims = 1)[:]  # 1D array result\n\n5-element Vector{Float64}:\n 2.3725376951964825\n 1.6135508215925356\n 2.420701808002711\n 2.8198074074732937\n 1.5535220422754121\n\n\nsum(A, dims = 2)\n\n4×1 Matrix{Float64}:\n 2.2547760824787435\n 2.053240230668491\n 2.611514770564903\n 3.8605886908282976\n\n\n\n\nList comprehension (comprehension syntax)\nSimilar to Python.\n\ny = [1.0, 2.0, 2.5]\n\n3-element Vector{Float64}:\n 1.0\n 2.0\n 2.5\n\nysq = [ w^2 for w in y ]\n\n3-element Vector{Float64}:\n 1.0\n 4.0\n 6.25\n\nxsqu = [ x^2 for x = 1:5 ]\n\n5-element Vector{Int64}:\n  1\n  4\n  9\n 16\n 25\n\n\nxsqu_even = [ x^2 for x = 1:5 if iseven(x)]\n\n2-element Vector{Int64}:\n  4\n 16\n\n\nnorm2 = [ x^2 + y^2 for x = 1:5, y = 1:5 ]\n\n5×5 Matrix{Int64}:\n  2   5  10  17  26\n  5   8  13  20  29\n 10  13  18  25  34\n 17  20  25  32  41\n 26  29  34  41  50\n\n\nA nice terse shorthand but can be hard to read.\n(Some people love it and some people hate it.)",
    "crumbs": [
      "Course Notes",
      "Notes 1: Introduction"
    ]
  },
  {
    "objectID": "notes/notes1.html#dictionaries-tuples-and-structs",
    "href": "notes/notes1.html#dictionaries-tuples-and-structs",
    "title": "Notes 1: Introduction",
    "section": "Dictionaries, tuples, and structs",
    "text": "Dictionaries, tuples, and structs\n\nDictionaries\nKey-value pairs like Python dictionaries (and somewhat like named R lists).\n\nx = Dict(\"test\" =&gt; 3, \"tmp\" =&gt; [2.1, 3.5], 7 =&gt; \"weird\")\n\nDict{Any, Any} with 3 entries:\n  7      =&gt; \"weird\"\n  \"test\" =&gt; 3\n  \"tmp\"  =&gt; [2.1, 3.5]\n\nx[\"tmp\"][2]\n\n3.5\n\nx[7]\n\n\"weird\"\n\nx[\"newkey\"] = 'a'\n\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\n\nkeys(x)\n\nKeySet for a Dict{Any, Any} with 4 entries. Keys:\n  7\n  \"test\"\n  \"tmp\"\n  \"newkey\"\n\n\n\nx[\"hello\"]\n\nERROR: KeyError: key \"hello\" not found\n\nget(x, \"hello\", 0)\n\n0\n\n\nNote that the keys don’t have to be strings! This could be good for caching/memoizing/lookup:\n\nx = Dict([\"foo\", \"bar\"] =&gt; 3, \"tmp\" =&gt; [2.1, 3.5], 7 =&gt; \"weird\")\n\nDict{Any, Any} with 3 entries:\n  7              =&gt; \"weird\"\n  [\"foo\", \"bar\"] =&gt; 3\n  \"tmp\"          =&gt; [2.1, 3.5]\n\nx[[\"foo\", \"bar\"]]\n\n3\n\nind = 7\n\n7\n\nx[ind]\n\n\"weird\"\n\ntypeof(ind)\n\nInt64\n\n\nIt would be interesting to know how it’s implemented that arbitrary objects can be keys. Perhaps using hashing?\nWhat do you think will happen here?\n\nind = Int32(7)  # What do you expect?\nx[ind]\nind = 7.0       # What do you expect?\nx[ind]\n\n\n\nTuples\nTuples are are similar to 1-dimensional arrays but they are immutable (they can’t be modified) and can have named elements.\n\nx = (3, 5, \"hello\")\n\n(3, 5, \"hello\")\n\nx[2]\n\n5\n\n\n\nx[2] = 7\n\nERROR: MethodError: no method matching setindex!(::Tuple{Int64, Int64, String}, ::Int64, ::Int64)\n\nx = 3\n\n3\n\ny = 9\n\n9\n\ny,x = x,y\n\n(3, 9)\n\n\n# Named tuple:\nx = (a=3, b=5, other=\"hello\")\n\n(a = 3, b = 5, other = \"hello\")\n\nx.b\n\n5\n\n\n\n\n\n\n\n\nFunctions with ! in their names\n\n\n\nWhat’s the deal with the “!” in setindex? We’ll see this more shortly, but functions that modify their inputs should have their name end in “!”.\n\n\nWhat do you think will happen here?\n\nx = (a=3, b=5, other=\"hello\", b=\"foo\")\nx.b\n\nTuples come in handy for providing flexibility in function inputs and outputs, as seen next.\n\n\nTuples and functions\nHere we create a function that can take an arbitrary number of inputs.\n\nfunction flexsum(args...)\n   println(\"The first value is $(args[1]).\")\n   x = args[1]*2\n   return sum(args)\nend\n\nflexsum (generic function with 1 method)\n\n\nflexsum(5, 7, 9)\n\nThe first value is 5.\n\n\n21\n\n\nHere’s how to call a function that takes multiple inputs, but pass as a tuple:\n\nfunction mydiv(x, y)\n   return x / y\nend\n\nmydiv (generic function with 1 method)\n\n\nvals = [3,5]\n\n2-element Vector{Int64}:\n 3\n 5\n\nmydiv(vals...)\n\n0.6\n\n\nWe use tuples to have a function return multiple values.\n\nfunction flexsum(args...)\n   println(\"The first value is $(args[1]).\")\n   x = args[1]*2\n   return args, x, sum(args)\nend\n\nflexsum (generic function with 1 method)\n\n\nflexsum(5, 7, 9)\n\nThe first value is 5.\n\n\n((5, 7, 9), 10, 21)\n\n\n\n\nStructs\nA struct is a “composite type”, a collection of named fields, useful for holding information with a particular structure.\n\nstruct Person\n  name\n  age\n  occupation\nend\n\nlincoln = Person(\"Abraham Lincoln\", 203, \"politician\")\n\nPerson(\"Abraham Lincoln\", 203, \"politician\")\n\nlincoln.age\n\n203\n\n\nWe’ll see much more on structs next week when we talk more about using types for robust code.\nLet’s discuss how these are similar to and different from objects in object-oriented languages like Python, Java, and C++.",
    "crumbs": [
      "Course Notes",
      "Notes 1: Introduction"
    ]
  },
  {
    "objectID": "notes/notes1.html#loops",
    "href": "notes/notes1.html#loops",
    "title": "Notes 1: Introduction",
    "section": "Loops",
    "text": "Loops\nWe’ll illustrate for loop syntax by using Monte Carlo simulation to estimate \\(\\pi\\) by generating points in a square and then finding the number that are inside the unit circle.\n\nnumThrows = 1000;\nin_circle = 0;\n\n# Run Monte Carlo simulation\nfor _ in 1:numThrows\n  # Generate random points on 2x2 square.\n  xPos = rand() * 2 - 1.0  # Equivalent to random.uniform(-1.0, 1.0)\n  yPos = rand() * 2 - 1.0\n\n  # Is point inside unit circle?\n  if sqrt(xPos^2 + yPos^2) &lt;= 1.0  # Equivalent to math.hypot()\n    in_circle += 1\n  end\nend\n\n# Estimate PI\npi_estimate = 4 * in_circle / numThrows\n\nIf you were using R or Python, what would the value of xPos be at the end of the loop execution?\n\nxPos\n\nERROR: UndefVarError: `xPos` not defined\nIn Julia, variables defined in the loop are local variables accessible only in the scope of the loop (more on scoping soon). This avoids clutter in the global scope.\n\n\n\n\n\n\nExercise\n\n\n\nI used different naming conventions for my variables (numThrows and in_circle). Look online to see what the recommended style is.\n\n\nWe can iterate over elements of an object like this:\n\nfor i in eachindex(x)\n  println(i)\nend\n\na\nb\nother",
    "crumbs": [
      "Course Notes",
      "Notes 1: Introduction"
    ]
  },
  {
    "objectID": "notes/notes1.html#string-processing-and-regular-expressions",
    "href": "notes/notes1.html#string-processing-and-regular-expressions",
    "title": "Notes 1: Introduction",
    "section": "String processing and regular expressions",
    "text": "String processing and regular expressions\n\nx = \"The cat in the hat.\"\n\n\"The cat in the hat.\"\n\nreplace(x, \"at\"=&gt;\"\")\n\n\"The c in the h.\"\n\n\nx = \"We found 999 red balloons.\"\n\n\"We found 999 red balloons.\"\n\nreplace(x, r\"[0-9]+\"=&gt;\"some\")  # Regular expression.\n\n\"We found some red balloons.\"\n\n\n'a' ∈ \"banana\"\n\ntrue\n\n\n\nx = \"We found 99 red balloons.\"\n\n\"We found 99 red balloons.\"\n\nm = match(r\"[0-9]+ ([a-z]+)\", x)\n\nRegexMatch(\"99 red\", 1=\"red\")\n\nm.match\n\n\"99 red\"\n\nm.captures\n\n1-element Vector{Union{Nothing, SubString{String}}}:\n \"red\"\n\nm.offset\n\n10",
    "crumbs": [
      "Course Notes",
      "Notes 1: Introduction"
    ]
  },
  {
    "objectID": "notes/notes7.html",
    "href": "notes/notes7.html",
    "title": "Notes 7: Parallelization",
    "section": "",
    "text": "This document is the seventh of a set of notes, this document focusing on the parallelization. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.\nIn some cases, I have set the code chunks to not execute when rendering this document, since the parallelization complicates execution and display of results.",
    "crumbs": [
      "Course Notes",
      "Notes 7: Parallelization"
    ]
  },
  {
    "objectID": "notes/notes7.html#introduction",
    "href": "notes/notes7.html#introduction",
    "title": "Notes 7: Parallelization",
    "section": "",
    "text": "This document is the seventh of a set of notes, this document focusing on the parallelization. The notes are not meant to be particularly complete in terms of useful functions (Google and LLMs can now provide that quite well), but rather to introduce the language and consider key programming concepts in the context of Julia.\nGiven that, the document heavily relies on demos, with interpretation in some cases left to the reader.\nIn some cases, I have set the code chunks to not execute when rendering this document, since the parallelization complicates execution and display of results.",
    "crumbs": [
      "Course Notes",
      "Notes 7: Parallelization"
    ]
  },
  {
    "objectID": "notes/notes7.html#parallelization-overview",
    "href": "notes/notes7.html#parallelization-overview",
    "title": "Notes 7: Parallelization",
    "section": "Parallelization overview",
    "text": "Parallelization overview\nLet’s first discuss some general concepts related to parallelization, based on the overview in this SCF tutorial.\n\nGlossary of terms\n\ncores: We’ll use this term to mean the different processing units available on a single machine or node.\nnodes: We’ll use this term to mean the different computers, each with their own distinct memory, that make up a cluster or supercomputer.\nprocesses: instances of a program executing on a machine; multiple processes may be executing at once. A given executable (e.g., Julia or Python) may start up multiple processes at once. Ideally we have no more user processes than cores on a node.\nworkers: the individual processes that are carrying out the (parallelized) computation. We’ll use worker and process interchangeably.\ntasks: This term gets used in various ways (including having the same meaning as ‘processes’ in the context of Slurm and MPI), but we’ll use it to refer to the individual computational items you want to complete - e.g., one task per cross-validation fold or one task per simulation replicate/iteration.\nthreads: multiple paths of execution within a single process; the OS sees the threads as a single process, but one can think of them as ‘lightweight’ processes. Ideally when considering the processes and their threads, we would have the number of total threads across all processes not exceed the number of cores on a node. Note: this is a separate term/concept from hardware hyperthreading, in which a single core can have two hyperthreads and operate almost as if it had two cores.\nforking: child processes are spawned that are identical to the parent, but with different process IDs and their own memory. In some cases if objects are not changed, the objects in the child process may refer back to the original objects in the original process, avoiding making copies.\nsockets: some of R’s parallel functionality involves creating new R processes (e.g., starting processes via Rscript) and communicating with them via a communication technology called sockets.\nscheduler: a program that manages users’ jobs on a cluster. Slurm is the most popular.\nload-balanced: when all the workers that are part of a computation are busy for the entire period of time the computation is running.\nlatency: the time delay or lag in starting an operation (e.g., copying a chunk of data or starting a process or sending a message to a worker). Note this is a fixed cost and does not scale with the size of the operation.\n\n\n\nParallelization strategies\nSome of the considerations that apply when thinking about how effective a given parallelization approach will be include:\n\nthe amount of memory that will be used by the various processes (the main concern is the maximum memory used at a given time during a computation),\nthe amount of communication that needs to happen – how much data will need to be passed between processes,\nthe latency of any communication - how much delay/lag is there in sending data between processes or starting up a worker process, and\nto what extent do processes have to wait for other processes to finish before they can do their next step.\n\nThe following are some basic principles/suggestions for how to parallelize your computation.\n\nShould I use one machine/node or many machines/nodes?\n\nIf you can do your computation on the cores of a single node using shared memory, that will be faster than using the same number of cores (or even somewhat more cores) across multiple nodes. Similarly, jobs with a lot of data/high memory requirements that one might think of as requiring multiple nodes may in some cases be much faster if you can find a single machine with a lot of memory.\nThat said, if you would run out of memory on a single node, then you’ll need to use distributed memory.\n\nWhat level or dimension should I parallelize over?\n\nIf you have nested loops, you generally only want to parallelize at one level of the code. Often you will want to parallelize over a loop and not use threaded linear algebra within the iterations of the loop.\nOften it makes sense to parallelize the outer loop when you have nested loops.\nYou generally want to parallelize in such a way that your code is load-balanced and does not involve too much communication.\n\nHow do I balance communication overhead with keeping my cores busy?\n\nIf you have very few tasks, particularly if the tasks take different amounts of time, often some processors will be idle and your code poorly load-balanced.\nIf you have very many tasks and each one takes little time, the overhead of starting and stopping the tasks (caused by latency) will reduce efficiency.\n\nShould multiple tasks be pre-assigned (statically assigned) to a process (i.e., a worker) (sometimes called prescheduling) or should tasks be assigned dynamically as previous tasks finish?\n\nTo illustrate the difference, suppose you have 6 tasks and 3 workers. If the tasks are pre-assigned, worker 1 might be assigned tasks 1 and 4 at the start, worker 2 assigned tasks 2 and 5, and worker 3 assigned tasks 3 and 6. If the tasks are dynamically assigned, worker 1 would be assigned task 1, worker 2 task 2, and worker 3 task 3. Then whichever worker finishes their task first (it woudn’t necessarily be worker 1) would be assigned task 4 and so on.\nBasically if you have many tasks that each take similar time, you want to preschedule the tasks to reduce communication. If you have few tasks or tasks with highly variable completion times, you don’t want to preschedule, to improve load-balancing.\n\n\n\n\nTypes of parallelization\n\nThreading\n\nMulti-threaded computation involves a single process that executes multiple threads (paths of execution on (ideally) as many cores as there are threads.\nAll the threads have access to the same memory (which is efficient but can potentially cause conflicts (aka “race conditions”).\nOne will see &gt;100% CPU usage because all usage is associated with a single process.\n\nMulti-process\n\nMultiple processes collaborate to execute a computation.\nProcesses may be controlled by a single main process and/or may execute distinct code based on their ID.\nOne will see multiple processes running.\n\nGPUs\n\nMany processing units (thousands) that are slow individually compared to the CPU but provide massive parallelism.\nThey have somewhat more limited memory (though modern GPUs like the A100 can have 80 GB of GPU memory).\nThey can only use data in their own memory, not in the CPU’s memory, so one must transfer data back and forth between the CPU (the host) and the GPU (the device). This copying can, in some computations, constitute a very large fraction of the overall computation. So it is best to create the data and/or leave the data (for subsequent calculations) on the GPU when possible and to limit transfers.\n\n\nMore on GPUs in the next set of notes.\nOne of the nice things about Julia is that much of the parallelization functionality is available directly in the Base package/module or in a limited number of additional packages. In Python and R, there is a confusing explosion of different parallelization packages (though for R, I highly recommend the future package and related packages for all your non-GPU parallelization work). E.g., in Python there is ipyparallel, ray, dask, multiprocessing in addition to other numerical computing packages that can run code in parallel (and on the GPU), such as JAX and PyTorch.",
    "crumbs": [
      "Course Notes",
      "Notes 7: Parallelization"
    ]
  },
  {
    "objectID": "notes/notes7.html#threading-in-julia",
    "href": "notes/notes7.html#threading-in-julia",
    "title": "Notes 7: Parallelization",
    "section": "Threading in Julia",
    "text": "Threading in Julia\nThis material is drawn from this SCF tutorial.\n\nThreaded linear algebra\nAs with Python and R, Julia uses BLAS, a standard library of basic linear algebra operations (written in Fortran or C), for linear algebra operations. A fast BLAS can greatly speed up linear algebra relative to the default BLAS on a machine. Julia uses a fast, open source, free BLAS library called OpenBLAS. In addition to being fast when used on a single core, the openBLAS library is threaded - if your computer has multiple cores and there are free resources, your linear algebra will use multiple cores.\nHere’s an example.\n\nusing LinearAlgebra\nusing Distributions\nusing BenchmarkTools\n\nn = 6000;\nx = rand(Uniform(0,1), n,n);\n\nprintln(BLAS.get_num_threads())\n\n4\n\n\n\nfunction chol_xtx(x)\n    z = x'x;   ## `z` is positive definite.\n    C = cholesky(z);\n    return C;\nend\n\nBLAS.set_num_threads(4);\n@btime chol = chol_xtx(x);  \n\n  5.734 s (5 allocations: 549.32 MiB)\n\n\n\nBLAS.set_num_threads(1);\n@btime chol = chol_xtx(x);  \n\n  6.881 s (5 allocations: 549.32 MiB)\n\n\nWe see that using four threads is faster than one, but in this case the speedup was (surprisingly) minimal. (In Python or R, I would expect a much bigger speedup and since all are using BLAS/LAPACK, I don’t understand why there is so little difference in Julia.)\nBy default, Julia will set the number of threads for linear algebra equal to the number of processors on your machine.\nAs seen above, you can check the number of threads being used with:\n\nBLAS.get_num_threads()\n\n1\n\n\nYou can also control the number of threads used for linear algebra via the OMP_NUM_THREADS (or OPENBLAS_NUM_THREADS) environment variable in the shell:\n## Option 1\nexport OMP_NUM_THREADS=4\njulia\n\n## Option 2\nOMP_NUM_THREADS=4 julia\n\n\nThreaded for loops and map-like operations\nIn Julia, you can directly set up software threads to use for parallel processing.\nHere we’ll see some examples of running a for loop in parallel, both acting on a single object and used as a parallel map operation.\nHere we can operate on a vector in parallel:\n\nusing Base.Threads\n\nn = 50000000;\nx = rand(n);\n\n@threads for i in 1:length(x)\n    x[i] = tan(x[i]) + 3*sin(x[i]);\nend\n\nThe iterations are grouped into chunks; otherwise the latency from communicating about each individual task (iteration) would probably slow things down a lot. One has some control over this with the option schedule argument, like this: @threads :static .....\nIf anyone is familiar with OpenMP, the use of the macro above to tag the loop for parallelization will look familiar.\nWe could also threads to carry out a parallel map operation, implemented as a for loop.\n\nn = 1000;\n\nfunction test(n)\n    x = rand(Uniform(0,1), n,n);\n    z = x'x;\n    C = cholesky(z);\n    return(C.U[1,1])  ## Arbitrary output to assure that the computation was done.\nend\n\na = zeros(12)\n@threads for i in 1:12\n    a[i] = test(n);\nend\n\n\n\nSpawning tasks on threads\nYou can also create (aka spawn) individual tasks on threads, with the tasks running in parallel. Tasks will be allocated to the available threads as threads become available (i.e., you can have many more tasks than threads, with the threadpool working its way through the tasks).\nLet’s see an example (taken from here of sorting a vector in parallel, by sorting subsets of the vector in separate threads.\n\nimport Base.Threads.@spawn\n\n\"\"\"\nSort the elements of `v` in place, from indices `lo` to `hi` inclusive.\n\"\"\"\nfunction psort!(v, lo::Int=1, hi::Int=length(v))\n    println(current_task(), ' ', lo, ' ', hi)\n    if lo &gt;= hi                       # 1 or 0 elements; nothing to do.\n        return v\n    end\n    if hi - lo &lt; 100000               # Below some cutoff, run in serial.\n        sort!(view(v, lo:hi), alg = MergeSort);\n        return v\n    end\n\n    mid = (lo+hi)&gt;&gt;&gt;1                 # Find the midpoint.\n\n    ### Set up parallelization. ###\n\n    ## Sort two halves in parallel, one in current call and one in a new task\n    ## in a separate thread.\n    \n    half = @spawn psort!(v, lo, mid)  # Sort the lower half in new thread.\n    psort!(v, mid+1, hi)              # Sort the upper half in the current call.\n    \n    wait(half)                        # Wait for the lower half to finish.\n\n    temp = v[lo:mid];                 # Create workspace for merging.\n\n    i, k, j = 1, lo, mid+1            # Merge the two sorted sub-arrays.\n    @inbounds while k &lt; j &lt;= hi       #  `@inbounds` skips array bound checking for efficiency.\n        if v[j] &lt; temp[i]\n            v[k] = v[j]\n            j += 1\n        else\n            v[k] = temp[i]\n            i += 1\n        end\n        k += 1\n    end\n    @inbounds while k &lt; j\n        v[k] = temp[i]\n        k += 1\n        i += 1\n    end\n\n    return v\nend\n\npsort!\n\n\nHow does this work? Let’s consider an example where we sort a vector of length 250000.\nThe vector gets split into elements 1:125000 (run in task #1) and 125001:250000 (run in the main call). Then the elements 1:125000 are split into 1:62500 (run in task #2) and 62501:125000 (run in task #1), while the elements 125001:250000 are split into 125001:187500 (run in task #3) and 187501:250000 (run in the main call). No more splitting occurs because vectors of length less than 100000 are run in serial.\nAssuming we have at least four threads (including the main process), each of the tasks will run in a separate thread, and all four sorts on the vector subsets will run in parallel. With fewer threads than tasks, some tasks will have to wait.\n\nx = rand(250000);\n## Run once to invoke JIT.\npsort!(x);\nsort!(x);\n\nTask (runnable) @0x00007fa897db7080 1 250000\nTask (runnable) @0x00007fa897db7080 125001 250000\nTask (runnable) @0x00007fa897db7080 187501 250000\nTask (runnable) @0x00007fa89694c010 1 125000\nTask (runnable) @0x00007fa89694c1a0 125001 187500\nTask (runnable) @0x00007fa89694c010 62501 125000\nTask (runnable) @0x00007fa8998841a0 1 62500\n\n\nWe see that the output from current_task() shows that the task labels correspond with what I stated above.\nI’ll use @time since using the repeated runs in @btime would end up sorting an already sorted vector.\n\nx = rand(250000);\ny = x[:];\n@time psort!(x);\n@time sort!(y; alg = MergeSort);\n\nTask (runnable) @0x00007fa897db7080 1 250000\nTask (runnable) @0x00007fa897db7080 125001 250000\nTask (runnable) @0x00007fa89694f080 1 125000\nTask (runnable) @0x00007fa89694f210 125001 187500\nTask (runnable) @0x00007fa89694f080 62501 125000\nTask (runnable) @0x00007fa8998801a0 1 62500\nTask (runnable) @0x00007fa897db7080 187501 250000\n  0.006713 seconds (549 allocations: 2.877 MiB)\n  0.258231 seconds (174.99 k allocations: 16.453 MiB, 27.29% gc time, 93.61% compilation time)\n\n\nAssuming that we started Julia such that there are multiple threads available (and also needing to account for the compilation time), we see that the parallel sort is faster than the non-parallel sort, though I was getting somewhat varied timing results on repeated runs.\nThe number of tasks running in parallel will be at most the number of threads set in the Julia session.\n\nControlling the number of threads\nYou can see the number of threads available:\n\nThreads.nthreads()\n\n4\n\n\nYou can control the number of threads used for threading in Julia (apart from linear algebra) either by:\n\nsetting the JULIA_NUM_THREADS environment variable in the shell before starting Julia, or\nstarting Julia with the -t (or --threads) flag, e.g.: julia -t 4.\n\nNote that we can’t use OMP_NUM_THREADS as the Julia threading is not based on OpenMP.\nSurprisingly, I’m not seeing a way to change the number of threads after Julia starts.",
    "crumbs": [
      "Course Notes",
      "Notes 7: Parallelization"
    ]
  },
  {
    "objectID": "notes/notes7.html#multi-process-parallelization-in-julia",
    "href": "notes/notes7.html#multi-process-parallelization-in-julia",
    "title": "Notes 7: Parallelization",
    "section": "Multi-process parallelization in Julia",
    "text": "Multi-process parallelization in Julia\nThis material is drawn from this SCF tutorial.\nWe’d want to think about how JIT compilation fits into the multi-process parallelization. I have not considered that in these materials.\n\nParallel map operations\nWe can use pmap to run a parallel map operation across multiple Julia processes (on one or more machines). pmap is good for cases where each task takes a non-negligible amount of time, as there is overhead (latency) in starting the tasks.\nHere we’ll carry out multiple computationally expensive calculations in the map.\n\nusing Distributed\n\nif nprocs() == 1\n    addprocs(4)\nend\n\nnprocs()\n\nWARNING: using Distributed.@spawn in module Main conflicts with an existing identifier.\n\n\n5\n\n\nWe need to import packages and create the function on each of the worker processes using @everywhere.\n\n@everywhere begin\n    using Distributions\n    using LinearAlgebra\n    function test(n)\n        x = rand(Uniform(0,1), n,n)\n        z = x'x\n        C = cholesky(z)\n        return C.U[2,3]  ## Arbitrary output to assure that the computation was done.\n    end\nend\n\nresult = pmap(test, repeat([5000],12))\n\n12-element Vector{Float64}:\n 11.937251860428423\n 11.883408190400335\n 11.95987057332551\n 11.711543109565259\n 11.739308301710718\n 11.253047211559021\n 10.928092406221712\n 11.493726016418318\n 11.235452679899693\n 11.56918006302922\n 11.580243164873602\n 11.924951129390438\n\n\nOne can use static allocation (prescheduling) with the batch_size argument, thereby assigning that many tasks to each worker to reduce latency.\n\n\nParallel for loops\nOne can execute for loops in parallel across multiple worker processes as follows. This is particularly handy for cases where one uses a reduction operator (e.g., the + here) so that little data needs to be copied back to the main process. (And in this case we don’t copy any data to the workers either.)\nHere we’ll sum over a large number of random numbers with chunks done on each of the workers, comparing the time to a basic for loop.\n\nusing BenchmarkTools\n\nfunction forfun(n)\n    sum = 0.0\n    for i in 1:n\n        sum += rand(1)[1]\n    end\n    return(sum)\nend\n\nfunction pforfun(n)\n   out = @sync @distributed (+) for i = 1:n\n       rand(1)[1]\n   end\n   return(out)\nend\n\nn=50000000\n@time forfun(n);\n@btime forfun(n);\n\n  2.587218 seconds (50.01 M allocations: 2.981 GiB, 14.32% gc time, 0.43% compilation time)\n  1.899 s (50000001 allocations: 2.98 GiB)\n\n\n\n@time pforfun(n);\n@btime pforfun(n); \n\n  1.736884 seconds (498.52 k allocations: 33.223 MiB, 24.03% compilation time)\n  484.000 ms (317 allocations: 13.36 KiB)\n\n\nThe use of @sync causes the operation to block until the result is available so we can get the correct timing.\nWithout a reduction operation, depending on what one is doing, one could end up passing a lot of data back to the main process, and this could take a lot of time. For such calculations, one would generally be better off using threaded for loops in order to take advantage of shared memory.\n\n\n\n\n\n\nRandom number generation in parallel\n\n\n\nWe’d have to look into how the random number seed is set on each worker to better understand any issues that might arise from parallel random number generation, but I believe that each worker has a different seed (but note that this does not explicitly ensure that the random number streams on the workers are distinct, as is the case if one uses the L’Ecuyer algorithm).\n\n\n\n\nPassing data to the workers\nWith multiple workers, particularly on more than one machine, one generally wants to be careful about having to copy large data objects to each worker, as that could make up a substantial portion of the time involved in the computation.\nOne can explicitly copy a variable to the workers in an @everywhere block by using Julia’s interpolation syntax:\n\nx = randn(5);\nprintln(x[1])\n\n@everywhere begin\n    x = $x  # copy to workers using interpolation syntax\n    println(pointer_from_objref(x), ' ', x[1])  \nend\n\nsleep(2)  # There is probably a better way to wait until all worker printing is done.\n\n-0.020421923133491972\nPtr{Nothing} @0x00007fa810ce2e90 -0.020421923133491972\n      From worker 4:    Ptr{Nothing} @0x00007f3776e4c010 -0.020421923133491972\n      From worker 5:    Ptr{Nothing} @0x00007fd32dc50010 -0.020421923133491972\n      From worker 2:    Ptr{Nothing} @0x00007fb700c4c010 -0.020421923133491972\n      From worker 3:    Ptr{Nothing} @0x00007f2b9ed50010 -0.020421923133491972\n\n\nWe see based on pointer_from_objref that each copy of x is stored at a distinct location in memory, even when processes are on the same machine.\nAlso note that if one creates a variable within an @everywhere block, that variable is available to all tasks run on the worker, so it is global’ with respect to those tasks. Note the repeated values in the result here.\n\n@everywhere begin\n    x = rand(5)\n    function test(i)\n        return sum(x)\n    end\nend\n\nresult = pmap(test, 1:12, batch_size = 3)\n\n12-element Vector{Float64}:\n 2.856845670431737\n 2.9430623763370254\n 2.8345321006917334\n 2.599262869484746\n 2.856845670431737\n 2.9430623763370254\n 2.8345321006917334\n 2.599262869484746\n 2.856845670431737\n 2.9430623763370254\n 2.8345321006917334\n 2.599262869484746\n\n\nIf one wants to have multiple processes all work on the same object, without copying it, one can consider using Julia’s SharedArray (one machine) or DArray from the DistributedArrays package (multiple machines) types, which break up arrays into pieces, with different pieces stored locally on different processes.\n\n\nSpawning tasks\nOne can use the Distributed.@spawnat macro to run tasks on separate workers/processes, in a fashion similar to using Threads.@spawn to run tasks on separate threads. More details can be found here.\n\n\nUsing multiple machines\nIn addition to using processes on one machine, one can use processes across multiple machines. One can either start the processes when you start the main Julia session or you can start them from within the Julia session. In both cases you’ll need to have the ability to ssh to the other machines without entering your password. In some cases you as the user might need to set up SSH keys.\nTo start the processes when starting Julia, create a “machinefile” that lists the names of the machines and the number of worker processes to start on each machine.\nHere’s an example machinefile:\nradagast.berkeley.edu\nradagast.berkeley.edu\ngandalf.berkeley.edu\ngandalf.berkeley.edu\nNote that if you’re using Slurm on a Linux cluster, you could generate that file in the shell from within your Slurm allocation like this:\nsrun hostname &gt; machines\nThen start Julia like this:\njulia --machine-file machines\nIf anyone has used MPI for programming parallel code, the idea of a “machine file” or “host file” will look familiar.\nFrom within Julia, you can add processes like this (first we’ll remove the existing worker processes started using addprocs() previously):\n\nrmprocs(workers())\n\naddprocs([(\"radagast\", 2), (\"gandalf\", 2)])\n\n4-element Vector{Int64}:\n 6\n 7\n 8\n 9\n\n\nTo check on the number of processes:\n\nnprocs()\n\n5\n\n\nWhen we demo this in class, we’ll try our parallel map example (the linear algebra calculation) and check that we have workers running on the various machines using top or ps after sshing to the machines.\n\nresult = pmap(test, repeat([5000],12))",
    "crumbs": [
      "Course Notes",
      "Notes 7: Parallelization"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Scheduling information",
    "section": "",
    "text": "For the moment the schedule only lists the problem sets. Time permitting I may post info on class content.\n\n\n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           \n           \n           \n           Jan 28:\n           \n           \n              Problem_Set 1 Problem Set 1\n           \n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           \n           \n           \n           Feb 13:\n           \n           \n              Problem_Set 2 Problem Set 2\n           \n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           \n           \n           \n           Mar 6:\n           \n           \n              Problem_Set 3 Problem Set 3\n           \n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n\nNo matching items",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#schedule",
    "href": "schedule.html#schedule",
    "title": "Scheduling information",
    "section": "",
    "text": "For the moment the schedule only lists the problem sets. Time permitting I may post info on class content.\n\n\n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           \n           \n           \n           Jan 28:\n           \n           \n              Problem_Set 1 Problem Set 1\n           \n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           \n           \n           \n           Feb 13:\n           \n           \n              Problem_Set 2 Problem Set 2\n           \n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           \n           \n           \n           Mar 6:\n           \n           \n              Problem_Set 3 Problem Set 3\n           \n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n\nNo matching items",
    "crumbs": [
      "Schedule"
    ]
  }
]